{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/BLT_models\n"
     ]
    }
   ],
   "source": [
    "data_path = '/share/data/imagenet-pytorch'\n",
    "\n",
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/BLT_models/')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 0): env://\n",
      "| distributed init (rank 1): env://\n",
      "cuda\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3890/3890 [00:05<00:00, 737.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3890/3890 [00:05<00:00, 748.17it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3890/3890 [00:01<00:00, 2202.38it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3890/3890 [00:01<00:00, 2203.92it/s]\n",
      "\n",
      "train_params ['conv_input.weight', 'conv_input.bias', 'norm_input.weight', 'norm_input.bias', 'norm_0.weight', 'norm_0.bias', 'norm_0_0.weight', 'norm_0_0.bias', 'conv_0_0.weight', 'conv_0_0.bias', 'norm_0_1.weight', 'norm_0_1.bias', 'conv_0_1.weight', 'conv_0_1.bias', 'norm_1.weight', 'norm_1.bias', 'norm_1_1.weight', 'norm_1_1.bias', 'conv_1_1.weight', 'conv_1_1.bias', 'norm_1_2.weight', 'norm_1_2.bias', 'conv_1_2.weight', 'conv_1_2.bias', 'norm_2.weight', 'norm_2.bias', 'norm_2_2.weight', 'norm_2_2.bias', 'conv_2_2.weight', 'conv_2_2.bias', 'norm_2_3.weight', 'norm_2_3.bias', 'conv_2_3.weight', 'conv_2_3.bias', 'norm_3.weight', 'norm_3.bias', 'norm_3_3.weight', 'norm_3_3.bias', 'conv_3_3.weight', 'conv_3_3.bias', 'read_out.linear.weight', 'read_out.linear.bias']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhosseinadeli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/engram/nklab/hossein/recurrent_models/BLT_models/wandb/run-20240529_130043-0jxwfps8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mblt_bl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hosseinadeli/vggface2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/hosseinadeli/vggface2/runs/0jxwfps8\u001b[0m\n",
      "Start training\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch: [0]  [   0/5008]  eta: 1 day, 11:14:11  lr: 0.000500  loss_labels: 8.2926 (8.2926)  loss: 8.2926 (8.2926)  time: 25.3297\n",
      "Epoch: [0]  [ 100/5008]  eta: 0:36:18  lr: 0.000500  loss_labels: 8.2975 (8.2994)  loss: 8.2738 (8.2994)  time: 0.1716\n",
      "Epoch: [0]  [ 200/5008]  eta: 0:24:34  lr: 0.000500  loss_labels: 8.2597 (8.2805)  loss: 8.2561 (8.2805)  time: 0.1706\n",
      "Epoch: [0]  [ 300/5008]  eta: 0:20:31  lr: 0.000500  loss_labels: 8.2478 (8.2706)  loss: 8.2402 (8.2706)  time: 0.1704\n",
      "Epoch: [0]  [ 400/5008]  eta: 0:18:20  lr: 0.000500  loss_labels: 8.2371 (8.2625)  loss: 8.2345 (8.2625)  time: 0.1709\n",
      "Epoch: [0]  [ 500/5008]  eta: 0:16:54  lr: 0.000500  loss_labels: 8.2308 (8.2557)  loss: 8.2280 (8.2557)  time: 0.1690\n",
      "Epoch: [0]  [ 600/5008]  eta: 0:15:52  lr: 0.000500  loss_labels: 8.2147 (8.2492)  loss: 8.1969 (8.2492)  time: 0.1707\n",
      "Epoch: [0]  [ 700/5008]  eta: 0:15:03  lr: 0.000500  loss_labels: 8.1961 (8.2413)  loss: 8.1740 (8.2413)  time: 0.1705\n",
      "Epoch: [0]  [ 800/5008]  eta: 0:14:22  lr: 0.000500  loss_labels: 8.1496 (8.2299)  loss: 8.1220 (8.2299)  time: 0.1736\n",
      "Epoch: [0]  [ 900/5008]  eta: 0:13:48  lr: 0.000500  loss_labels: 8.0829 (8.2124)  loss: 8.0143 (8.2124)  time: 0.1729\n",
      "Epoch: [0]  [1000/5008]  eta: 0:13:15  lr: 0.000500  loss_labels: 7.9614 (8.1873)  loss: 7.9123 (8.1873)  time: 0.1708\n",
      "Epoch: [0]  [1100/5008]  eta: 0:12:45  lr: 0.000500  loss_labels: 7.8268 (8.1543)  loss: 7.7488 (8.1543)  time: 0.1701\n",
      "Epoch: [0]  [1200/5008]  eta: 0:12:18  lr: 0.000500  loss_labels: 7.6515 (8.1120)  loss: 7.5919 (8.1120)  time: 0.1716\n",
      "Epoch: [0]  [1300/5008]  eta: 0:11:52  lr: 0.000500  loss_labels: 7.5058 (8.0656)  loss: 7.4767 (8.0656)  time: 0.1722\n",
      "Epoch: [0]  [1400/5008]  eta: 0:11:28  lr: 0.000500  loss_labels: 7.3937 (8.0173)  loss: 7.3376 (8.0173)  time: 0.1736\n",
      "Epoch: [0]  [1500/5008]  eta: 0:11:05  lr: 0.000500  loss_labels: 7.2584 (7.9667)  loss: 7.1886 (7.9667)  time: 0.1731\n",
      "Epoch: [0]  [1600/5008]  eta: 0:10:42  lr: 0.000500  loss_labels: 7.1771 (7.9175)  loss: 7.1414 (7.9175)  time: 0.1708\n",
      "Epoch: [0]  [1700/5008]  eta: 0:10:20  lr: 0.000500  loss_labels: 7.0541 (7.8666)  loss: 7.0704 (7.8666)  time: 0.1718\n",
      "Epoch: [0]  [1800/5008]  eta: 0:09:58  lr: 0.000500  loss_labels: 6.9436 (7.8162)  loss: 6.9066 (7.8162)  time: 0.1720\n",
      "Epoch: [0]  [1900/5008]  eta: 0:09:37  lr: 0.000500  loss_labels: 6.8470 (7.7657)  loss: 6.8419 (7.7657)  time: 0.1709\n",
      "Epoch: [0]  [2000/5008]  eta: 0:09:17  lr: 0.000500  loss_labels: 6.7761 (7.7159)  loss: 6.6981 (7.7159)  time: 0.1738\n",
      "Epoch: [0]  [2100/5008]  eta: 0:08:56  lr: 0.000500  loss_labels: 6.6732 (7.6658)  loss: 6.5709 (7.6658)  time: 0.1716\n",
      "Epoch: [0]  [2200/5008]  eta: 0:08:37  lr: 0.000500  loss_labels: 6.5644 (7.6162)  loss: 6.5303 (7.6162)  time: 0.1739\n",
      "Epoch: [0]  [2300/5008]  eta: 0:08:17  lr: 0.000500  loss_labels: 6.4857 (7.5671)  loss: 6.4419 (7.5671)  time: 0.1760\n",
      "Epoch: [0]  [2400/5008]  eta: 0:07:58  lr: 0.000500  loss_labels: 6.3863 (7.5179)  loss: 6.3278 (7.5179)  time: 0.1748\n",
      "Epoch: [0]  [2500/5008]  eta: 0:07:38  lr: 0.000500  loss_labels: 6.3298 (7.4701)  loss: 6.2726 (7.4701)  time: 0.1730\n",
      "Epoch: [0]  [2600/5008]  eta: 0:07:19  lr: 0.000500  loss_labels: 6.2283 (7.4220)  loss: 6.1578 (7.4220)  time: 0.1737\n",
      "Epoch: [0]  [2700/5008]  eta: 0:07:00  lr: 0.000500  loss_labels: 6.1300 (7.3745)  loss: 6.0761 (7.3745)  time: 0.1735\n",
      "Epoch: [0]  [2800/5008]  eta: 0:06:41  lr: 0.000500  loss_labels: 6.0606 (7.3276)  loss: 6.0433 (7.3276)  time: 0.1717\n",
      "Epoch: [0]  [2900/5008]  eta: 0:06:22  lr: 0.000500  loss_labels: 5.9612 (7.2809)  loss: 5.9029 (7.2809)  time: 0.1754\n",
      "Epoch: [0]  [3000/5008]  eta: 0:06:04  lr: 0.000500  loss_labels: 5.9288 (7.2358)  loss: 5.9455 (7.2358)  time: 0.1750\n",
      "Epoch: [0]  [3100/5008]  eta: 0:05:45  lr: 0.000500  loss_labels: 5.8414 (7.1909)  loss: 5.7620 (7.1909)  time: 0.1753\n",
      "Epoch: [0]  [3200/5008]  eta: 0:05:27  lr: 0.000500  loss_labels: 5.7467 (7.1459)  loss: 5.7514 (7.1459)  time: 0.1721\n",
      "Epoch: [0]  [3300/5008]  eta: 0:05:08  lr: 0.000500  loss_labels: 5.6814 (7.1016)  loss: 5.6316 (7.1016)  time: 0.1707\n",
      "Epoch: [0]  [3400/5008]  eta: 0:04:50  lr: 0.000500  loss_labels: 5.6279 (7.0581)  loss: 5.5806 (7.0581)  time: 0.1722\n",
      "Epoch: [0]  [3500/5008]  eta: 0:04:31  lr: 0.000500  loss_labels: 5.5522 (7.0151)  loss: 5.5572 (7.0151)  time: 0.1733\n",
      "Epoch: [0]  [3600/5008]  eta: 0:04:13  lr: 0.000500  loss_labels: 5.4701 (6.9724)  loss: 5.4199 (6.9724)  time: 0.1719\n",
      "Epoch: [0]  [3700/5008]  eta: 0:03:55  lr: 0.000500  loss_labels: 5.4084 (6.9296)  loss: 5.3078 (6.9296)  time: 0.1687\n",
      "Epoch: [0]  [3800/5008]  eta: 0:03:36  lr: 0.000500  loss_labels: 5.3330 (6.8877)  loss: 5.2293 (6.8877)  time: 0.1715\n",
      "Epoch: [0]  [3900/5008]  eta: 0:03:18  lr: 0.000500  loss_labels: 5.2884 (6.8466)  loss: 5.2956 (6.8466)  time: 0.1711\n",
      "Epoch: [0]  [4000/5008]  eta: 0:03:00  lr: 0.000500  loss_labels: 5.1906 (6.8055)  loss: 5.1509 (6.8055)  time: 0.1703\n",
      "Epoch: [0]  [4100/5008]  eta: 0:02:42  lr: 0.000500  loss_labels: 5.1642 (6.7651)  loss: 5.1195 (6.7651)  time: 0.1695\n",
      "Epoch: [0]  [4200/5008]  eta: 0:02:24  lr: 0.000500  loss_labels: 5.0493 (6.7245)  loss: 5.0086 (6.7245)  time: 0.1700\n",
      "Epoch: [0]  [4300/5008]  eta: 0:02:06  lr: 0.000500  loss_labels: 5.0124 (6.6846)  loss: 4.9492 (6.6846)  time: 0.1698\n",
      "Epoch: [0]  [4400/5008]  eta: 0:01:48  lr: 0.000500  loss_labels: 4.9565 (6.6453)  loss: 4.8949 (6.6453)  time: 0.1700\n",
      "Epoch: [0]  [4500/5008]  eta: 0:01:30  lr: 0.000500  loss_labels: 4.8921 (6.6064)  loss: 4.8713 (6.6064)  time: 0.1711\n",
      "Epoch: [0]  [4600/5008]  eta: 0:01:12  lr: 0.000500  loss_labels: 4.7667 (6.5666)  loss: 4.7142 (6.5666)  time: 0.1701\n",
      "Epoch: [0]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 4.7312 (6.5275)  loss: 4.7312 (6.5275)  time: 0.1703\n",
      "Epoch: [0]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 4.6820 (6.4892)  loss: 4.6529 (6.4892)  time: 0.1735\n",
      "Epoch: [0]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 4.6482 (6.4515)  loss: 4.5799 (6.4515)  time: 0.1724\n",
      "Epoch: [0]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 4.5778 (6.4140)  loss: 4.5164 (6.4140)  time: 0.1731\n",
      "Epoch: [0]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 4.5720 (6.4115)  loss: 4.5720 (6.4115)  time: 0.1726\n",
      "Epoch: [0] Total time: 0:14:49 (0.1776 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 4.5720 (6.4115)  loss: 4.5720 (6.4115)\n",
      "Test:  [  0/565]  eta: 2:33:58  loss_labels: 4.9641 (4.9641)  loss: 4.9641 (4.9641)  time: 16.3520\n",
      "Test:  [100/565]  eta: 0:02:09  loss_labels: 4.9457 (5.0023)  loss: 5.1697 (5.0023)  time: 0.1173\n",
      "Test:  [200/565]  eta: 0:01:12  loss_labels: 4.9740 (4.9937)  loss: 5.2140 (4.9937)  time: 0.1208\n",
      "Test:  [300/565]  eta: 0:00:45  loss_labels: 4.9575 (4.9684)  loss: 4.8065 (4.9684)  time: 0.1214\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 5.0342 (4.9884)  loss: 4.9165 (4.9884)  time: 0.1099\n",
      "Test:  [500/565]  eta: 0:00:09  loss_labels: 4.9368 (4.9616)  loss: 5.1655 (4.9616)  time: 0.1172\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 4.9821 (4.9650)  loss: 4.9650 (4.9650)  time: 0.1263\n",
      "Test: Total time: 0:01:24 (0.1500 s / it)\n",
      "Averaged stats: loss_labels: 4.9821 (4.9650)  loss: 4.9650 (4.9650)\n",
      "acc: 0.13097545504570007\n",
      "top 1 and top 5 accuracies {'top1': 0.13097546114219244, 'top5': 0.2996870326261563, 'loss': tensor(0.0388, device='cuda:0')}\n",
      "Epoch: [1]  [   0/5008]  eta: 1 day, 4:59:08  lr: 0.000500  loss_labels: 4.7231 (4.7231)  loss: 4.7231 (4.7231)  time: 20.8364\n",
      "Epoch: [1]  [ 100/5008]  eta: 0:31:05  lr: 0.000500  loss_labels: 4.5140 (4.5348)  loss: 4.4241 (4.5348)  time: 0.1758\n",
      "Epoch: [1]  [ 200/5008]  eta: 0:22:17  lr: 0.000500  loss_labels: 4.4501 (4.4898)  loss: 4.4720 (4.4898)  time: 0.1754\n",
      "Epoch: [1]  [ 300/5008]  eta: 0:19:12  lr: 0.000500  loss_labels: 4.4104 (4.4657)  loss: 4.3524 (4.4657)  time: 0.1773\n",
      "Epoch: [1]  [ 400/5008]  eta: 0:17:30  lr: 0.000500  loss_labels: 4.3120 (4.4310)  loss: 4.2748 (4.4310)  time: 0.1770\n",
      "Epoch: [1]  [ 500/5008]  eta: 0:16:20  lr: 0.000500  loss_labels: 4.2928 (4.4072)  loss: 4.2585 (4.4072)  time: 0.1752\n",
      "Epoch: [1]  [ 600/5008]  eta: 0:15:28  lr: 0.000500  loss_labels: 4.2711 (4.3815)  loss: 4.1926 (4.3815)  time: 0.1749\n",
      "Epoch: [1]  [ 700/5008]  eta: 0:14:45  lr: 0.000500  loss_labels: 4.1635 (4.3506)  loss: 4.1035 (4.3506)  time: 0.1752\n",
      "Epoch: [1]  [ 800/5008]  eta: 0:14:09  lr: 0.000500  loss_labels: 4.1102 (4.3218)  loss: 4.1308 (4.3218)  time: 0.1749\n",
      "Epoch: [1]  [ 900/5008]  eta: 0:13:36  lr: 0.000500  loss_labels: 4.1060 (4.2979)  loss: 4.0918 (4.2979)  time: 0.1747\n",
      "Epoch: [1]  [1000/5008]  eta: 0:13:08  lr: 0.000500  loss_labels: 4.0663 (4.2742)  loss: 4.0502 (4.2742)  time: 0.1773\n",
      "Epoch: [1]  [1100/5008]  eta: 0:12:41  lr: 0.000500  loss_labels: 4.0129 (4.2507)  loss: 3.9520 (4.2507)  time: 0.1738\n",
      "Epoch: [1]  [1200/5008]  eta: 0:12:14  lr: 0.000500  loss_labels: 4.0037 (4.2289)  loss: 3.9321 (4.2289)  time: 0.1741\n",
      "Epoch: [1]  [1300/5008]  eta: 0:11:50  lr: 0.000500  loss_labels: 3.9216 (4.2055)  loss: 3.9220 (4.2055)  time: 0.1746\n",
      "Epoch: [1]  [1400/5008]  eta: 0:11:27  lr: 0.000500  loss_labels: 3.8646 (4.1816)  loss: 3.7998 (4.1816)  time: 0.1749\n",
      "Epoch: [1]  [1500/5008]  eta: 0:11:04  lr: 0.000500  loss_labels: 3.8481 (4.1598)  loss: 3.7734 (4.1598)  time: 0.1747\n",
      "Epoch: [1]  [1600/5008]  eta: 0:10:42  lr: 0.000500  loss_labels: 3.7772 (4.1365)  loss: 3.7468 (4.1365)  time: 0.1737\n",
      "Epoch: [1]  [1700/5008]  eta: 0:10:21  lr: 0.000500  loss_labels: 3.7803 (4.1148)  loss: 3.8054 (4.1148)  time: 0.1760\n",
      "Epoch: [1]  [1800/5008]  eta: 0:10:00  lr: 0.000500  loss_labels: 3.7080 (4.0920)  loss: 3.7201 (4.0920)  time: 0.1750\n",
      "Epoch: [1]  [1900/5008]  eta: 0:09:39  lr: 0.000500  loss_labels: 3.6867 (4.0701)  loss: 3.7030 (4.0701)  time: 0.1735\n",
      "Epoch: [1]  [2000/5008]  eta: 0:09:18  lr: 0.000500  loss_labels: 3.6371 (4.0479)  loss: 3.4988 (4.0479)  time: 0.1825\n",
      "Epoch: [1]  [2100/5008]  eta: 0:08:58  lr: 0.000500  loss_labels: 3.5800 (4.0260)  loss: 3.6118 (4.0260)  time: 0.1734\n",
      "Epoch: [1]  [2200/5008]  eta: 0:08:38  lr: 0.000500  loss_labels: 3.5031 (4.0034)  loss: 3.4937 (4.0034)  time: 0.1736\n",
      "Epoch: [1]  [2300/5008]  eta: 0:08:18  lr: 0.000500  loss_labels: 3.5114 (3.9827)  loss: 3.5022 (3.9827)  time: 0.1739\n",
      "Epoch: [1]  [2400/5008]  eta: 0:07:59  lr: 0.000500  loss_labels: 3.4376 (3.9609)  loss: 3.4362 (3.9609)  time: 0.1721\n",
      "Epoch: [1]  [2500/5008]  eta: 0:07:39  lr: 0.000500  loss_labels: 3.4635 (3.9413)  loss: 3.4408 (3.9413)  time: 0.1717\n",
      "Epoch: [1]  [2600/5008]  eta: 0:07:20  lr: 0.000500  loss_labels: 3.4171 (3.9207)  loss: 3.3440 (3.9207)  time: 0.1735\n",
      "Epoch: [1]  [2700/5008]  eta: 0:07:01  lr: 0.000500  loss_labels: 3.3695 (3.9004)  loss: 3.3523 (3.9004)  time: 0.1737\n",
      "Epoch: [1]  [2800/5008]  eta: 0:06:42  lr: 0.000500  loss_labels: 3.3557 (3.8809)  loss: 3.3125 (3.8809)  time: 0.1752\n",
      "Epoch: [1]  [2900/5008]  eta: 0:06:23  lr: 0.000500  loss_labels: 3.2758 (3.8602)  loss: 3.2538 (3.8602)  time: 0.1723\n",
      "Epoch: [1]  [3000/5008]  eta: 0:06:04  lr: 0.000500  loss_labels: 3.3092 (3.8417)  loss: 3.3085 (3.8417)  time: 0.1734\n",
      "Epoch: [1]  [3100/5008]  eta: 0:05:45  lr: 0.000500  loss_labels: 3.2133 (3.8221)  loss: 3.1951 (3.8221)  time: 0.1741\n",
      "Epoch: [1]  [3200/5008]  eta: 0:05:27  lr: 0.000500  loss_labels: 3.1792 (3.8020)  loss: 3.1928 (3.8020)  time: 0.1744\n",
      "Epoch: [1]  [3300/5008]  eta: 0:05:08  lr: 0.000500  loss_labels: 3.1697 (3.7829)  loss: 3.1632 (3.7829)  time: 0.1741\n",
      "Epoch: [1]  [3400/5008]  eta: 0:04:50  lr: 0.000500  loss_labels: 3.1431 (3.7640)  loss: 3.1431 (3.7640)  time: 0.1727\n",
      "Epoch: [1]  [3500/5008]  eta: 0:04:32  lr: 0.000500  loss_labels: 3.0691 (3.7449)  loss: 3.0545 (3.7449)  time: 0.1721\n",
      "Epoch: [1]  [3600/5008]  eta: 0:04:13  lr: 0.000500  loss_labels: 3.1063 (3.7267)  loss: 3.1072 (3.7267)  time: 0.1730\n",
      "Epoch: [1]  [3700/5008]  eta: 0:03:55  lr: 0.000500  loss_labels: 3.0280 (3.7076)  loss: 3.0257 (3.7076)  time: 0.1755\n",
      "Epoch: [1]  [3800/5008]  eta: 0:03:37  lr: 0.000500  loss_labels: 3.0264 (3.6895)  loss: 2.9310 (3.6895)  time: 0.1747\n",
      "Epoch: [1]  [3900/5008]  eta: 0:03:19  lr: 0.000500  loss_labels: 2.9816 (3.6717)  loss: 2.8998 (3.6717)  time: 0.1751\n",
      "Epoch: [1]  [4000/5008]  eta: 0:03:01  lr: 0.000500  loss_labels: 2.9268 (3.6535)  loss: 2.8808 (3.6535)  time: 0.1766\n",
      "Epoch: [1]  [4100/5008]  eta: 0:02:43  lr: 0.000500  loss_labels: 2.9020 (3.6351)  loss: 2.8928 (3.6351)  time: 0.1737\n",
      "Epoch: [1]  [4200/5008]  eta: 0:02:25  lr: 0.000500  loss_labels: 2.8596 (3.6169)  loss: 2.8095 (3.6169)  time: 0.1713\n",
      "Epoch: [1]  [4300/5008]  eta: 0:02:06  lr: 0.000500  loss_labels: 2.8381 (3.5988)  loss: 2.7787 (3.5988)  time: 0.1704\n",
      "Epoch: [1]  [4400/5008]  eta: 0:01:48  lr: 0.000500  loss_labels: 2.8663 (3.5817)  loss: 2.7528 (3.5817)  time: 0.1740\n",
      "Epoch: [1]  [4500/5008]  eta: 0:01:30  lr: 0.000500  loss_labels: 2.7654 (3.5639)  loss: 2.7804 (3.5639)  time: 0.1705\n",
      "Epoch: [1]  [4600/5008]  eta: 0:01:12  lr: 0.000500  loss_labels: 2.7340 (3.5456)  loss: 2.7495 (3.5456)  time: 0.1697\n",
      "Epoch: [1]  [4700/5008]  eta: 0:00:55  lr: 0.000500  loss_labels: 2.6793 (3.5272)  loss: 2.6837 (3.5272)  time: 0.1705\n",
      "Epoch: [1]  [4800/5008]  eta: 0:00:37  lr: 0.000500  loss_labels: 2.6426 (3.5092)  loss: 2.6673 (3.5092)  time: 0.1696\n",
      "Epoch: [1]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 2.6903 (3.4923)  loss: 2.5938 (3.4923)  time: 0.1684\n",
      "Epoch: [1]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 2.6259 (3.4750)  loss: 2.6159 (3.4750)  time: 0.1692\n",
      "Epoch: [1]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 2.6245 (3.4738)  loss: 2.6159 (3.4738)  time: 0.1684\n",
      "Epoch: [1] Total time: 0:14:52 (0.1782 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 2.6245 (3.4738)  loss: 2.6159 (3.4738)\n",
      "Test:  [  0/565]  eta: 2:42:48  loss_labels: 2.9910 (2.9910)  loss: 2.9910 (2.9910)  time: 17.2895\n",
      "Test:  [100/565]  eta: 0:02:15  loss_labels: 3.3000 (3.3583)  loss: 3.3926 (3.3583)  time: 0.1210\n",
      "Test:  [200/565]  eta: 0:01:15  loss_labels: 3.2389 (3.3356)  loss: 3.4154 (3.3356)  time: 0.1171\n",
      "Test:  [300/565]  eta: 0:00:46  loss_labels: 3.2351 (3.2966)  loss: 3.0229 (3.2966)  time: 0.1270\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 3.2970 (3.3130)  loss: 3.1505 (3.3130)  time: 0.1108\n",
      "Test:  [500/565]  eta: 0:00:09  loss_labels: 3.0816 (3.2748)  loss: 3.5016 (3.2748)  time: 0.1189\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 3.0678 (3.2661)  loss: 3.3313 (3.2661)  time: 0.1155\n",
      "Test: Total time: 0:01:25 (0.1509 s / it)\n",
      "Averaged stats: loss_labels: 3.0678 (3.2661)  loss: 3.3313 (3.2661)\n",
      "acc: 0.3737882971763611\n",
      "top 1 and top 5 accuracies {'top1': 0.3737882900348972, 'top5': 0.5870630920068687, 'loss': tensor(0.0256, device='cuda:0')}\n",
      "Epoch: [2]  [   0/5008]  eta: 1 day, 5:06:35  lr: 0.000500  loss_labels: 2.7492 (2.7492)  loss: 2.7492 (2.7492)  time: 20.9257\n",
      "Epoch: [2]  [ 100/5008]  eta: 0:30:23  lr: 0.000500  loss_labels: 2.5991 (2.6131)  loss: 2.5213 (2.6131)  time: 0.1670\n",
      "Epoch: [2]  [ 200/5008]  eta: 0:21:35  lr: 0.000500  loss_labels: 2.6074 (2.6052)  loss: 2.6374 (2.6052)  time: 0.1668\n",
      "Epoch: [2]  [ 300/5008]  eta: 0:18:29  lr: 0.000500  loss_labels: 2.5481 (2.5903)  loss: 2.6096 (2.5903)  time: 0.1675\n",
      "Epoch: [2]  [ 400/5008]  eta: 0:16:47  lr: 0.000500  loss_labels: 2.4977 (2.5702)  loss: 2.4611 (2.5702)  time: 0.1663\n",
      "Epoch: [2]  [ 500/5008]  eta: 0:15:39  lr: 0.000500  loss_labels: 2.4732 (2.5551)  loss: 2.4254 (2.5551)  time: 0.1669\n",
      "Epoch: [2]  [ 600/5008]  eta: 0:14:47  lr: 0.000500  loss_labels: 2.4755 (2.5415)  loss: 2.3730 (2.5415)  time: 0.1667\n",
      "Epoch: [2]  [ 700/5008]  eta: 0:14:06  lr: 0.000500  loss_labels: 2.4328 (2.5233)  loss: 2.3791 (2.5233)  time: 0.1673\n",
      "Epoch: [2]  [ 800/5008]  eta: 0:13:31  lr: 0.000500  loss_labels: 2.3968 (2.5075)  loss: 2.4016 (2.5075)  time: 0.1671\n",
      "Epoch: [2]  [ 900/5008]  eta: 0:13:00  lr: 0.000500  loss_labels: 2.3589 (2.4948)  loss: 2.3381 (2.4948)  time: 0.1662\n",
      "Epoch: [2]  [1000/5008]  eta: 0:12:32  lr: 0.000500  loss_labels: 2.3596 (2.4814)  loss: 2.3756 (2.4814)  time: 0.1672\n",
      "Epoch: [2]  [1100/5008]  eta: 0:12:05  lr: 0.000500  loss_labels: 2.3525 (2.4700)  loss: 2.3000 (2.4700)  time: 0.1666\n",
      "Epoch: [2]  [1200/5008]  eta: 0:11:40  lr: 0.000500  loss_labels: 2.3295 (2.4596)  loss: 2.2591 (2.4596)  time: 0.1652\n",
      "Epoch: [2]  [1300/5008]  eta: 0:11:55  lr: 0.000500  loss_labels: 2.2829 (2.4462)  loss: 2.3034 (2.4462)  time: 0.1653\n",
      "Epoch: [2]  [1400/5008]  eta: 0:11:29  lr: 0.000500  loss_labels: 2.2728 (2.4337)  loss: 2.2548 (2.4337)  time: 0.1647\n",
      "Epoch: [2]  [1500/5008]  eta: 0:11:04  lr: 0.000500  loss_labels: 2.2362 (2.4216)  loss: 2.2537 (2.4216)  time: 0.1654\n",
      "Epoch: [2]  [1600/5008]  eta: 0:10:40  lr: 0.000500  loss_labels: 2.2424 (2.4103)  loss: 2.2512 (2.4103)  time: 0.1689\n",
      "Epoch: [2]  [1700/5008]  eta: 0:10:17  lr: 0.000500  loss_labels: 2.2338 (2.3994)  loss: 2.2627 (2.3994)  time: 0.1669\n",
      "Epoch: [2]  [1800/5008]  eta: 0:09:55  lr: 0.000500  loss_labels: 2.1662 (2.3868)  loss: 2.1216 (2.3868)  time: 0.1673\n",
      "Epoch: [2]  [1900/5008]  eta: 0:09:34  lr: 0.000500  loss_labels: 2.1534 (2.3750)  loss: 2.1543 (2.3750)  time: 0.1688\n",
      "Epoch: [2]  [2000/5008]  eta: 0:09:13  lr: 0.000500  loss_labels: 2.1505 (2.3631)  loss: 2.1017 (2.3631)  time: 0.1685\n",
      "Epoch: [2]  [2100/5008]  eta: 0:08:52  lr: 0.000500  loss_labels: 2.1180 (2.3514)  loss: 2.1320 (2.3514)  time: 0.1676\n",
      "Epoch: [2]  [2200/5008]  eta: 0:08:32  lr: 0.000500  loss_labels: 2.0518 (2.3387)  loss: 1.9949 (2.3387)  time: 0.1676\n",
      "Epoch: [2]  [2300/5008]  eta: 0:08:12  lr: 0.000500  loss_labels: 2.0750 (2.3278)  loss: 2.0759 (2.3278)  time: 0.1672\n",
      "Epoch: [2]  [2400/5008]  eta: 0:07:52  lr: 0.000500  loss_labels: 2.0522 (2.3167)  loss: 2.0216 (2.3167)  time: 0.1676\n",
      "Epoch: [2]  [2500/5008]  eta: 0:07:33  lr: 0.000500  loss_labels: 2.0320 (2.3058)  loss: 2.0231 (2.3058)  time: 0.1675\n",
      "Epoch: [2]  [2600/5008]  eta: 0:07:13  lr: 0.000500  loss_labels: 2.0648 (2.2957)  loss: 1.9355 (2.2957)  time: 0.1671\n",
      "Epoch: [2]  [2700/5008]  eta: 0:06:54  lr: 0.000500  loss_labels: 1.9870 (2.2849)  loss: 1.9592 (2.2849)  time: 0.1676\n",
      "Epoch: [2]  [2800/5008]  eta: 0:06:35  lr: 0.000500  loss_labels: 1.9729 (2.2742)  loss: 1.9892 (2.2742)  time: 0.1678\n",
      "Epoch: [2]  [2900/5008]  eta: 0:06:17  lr: 0.000500  loss_labels: 1.9391 (2.2631)  loss: 1.9325 (2.2631)  time: 0.1706\n",
      "Epoch: [2]  [3000/5008]  eta: 0:05:58  lr: 0.000500  loss_labels: 1.9403 (2.2532)  loss: 1.9197 (2.2532)  time: 0.1699\n",
      "Epoch: [2]  [3100/5008]  eta: 0:05:40  lr: 0.000500  loss_labels: 1.9197 (2.2426)  loss: 1.8029 (2.2426)  time: 0.1703\n",
      "Epoch: [2]  [3200/5008]  eta: 0:05:22  lr: 0.000500  loss_labels: 1.8922 (2.2315)  loss: 1.8985 (2.2315)  time: 0.1702\n",
      "Epoch: [2]  [3300/5008]  eta: 0:05:03  lr: 0.000500  loss_labels: 1.8808 (2.2209)  loss: 1.8557 (2.2209)  time: 0.1702\n",
      "Epoch: [2]  [3400/5008]  eta: 0:04:45  lr: 0.000500  loss_labels: 1.8781 (2.2106)  loss: 1.8571 (2.2106)  time: 0.1690\n",
      "Epoch: [2]  [3500/5008]  eta: 0:04:27  lr: 0.000500  loss_labels: 1.8545 (2.2007)  loss: 1.9477 (2.2007)  time: 0.1697\n",
      "Epoch: [2]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 1.8366 (2.1906)  loss: 1.8366 (2.1906)  time: 0.1702\n",
      "Epoch: [2]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 1.7828 (2.1799)  loss: 1.7867 (2.1799)  time: 0.1664\n",
      "Epoch: [2]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 1.7992 (2.1696)  loss: 1.7240 (2.1696)  time: 0.1675\n",
      "Epoch: [2]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 1.8061 (2.1603)  loss: 1.7401 (2.1603)  time: 0.1677\n",
      "Epoch: [2]  [4000/5008]  eta: 0:02:57  lr: 0.000500  loss_labels: 1.7633 (2.1504)  loss: 1.7151 (2.1504)  time: 0.1667\n",
      "Epoch: [2]  [4100/5008]  eta: 0:02:39  lr: 0.000500  loss_labels: 1.7314 (2.1406)  loss: 1.7157 (2.1406)  time: 0.1685\n",
      "Epoch: [2]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 1.7236 (2.1307)  loss: 1.6551 (2.1307)  time: 0.1679\n",
      "Epoch: [2]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 1.7459 (2.1219)  loss: 1.6715 (2.1219)  time: 0.1684\n",
      "Epoch: [2]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 1.7352 (2.1134)  loss: 1.6983 (2.1134)  time: 0.1677\n",
      "Epoch: [2]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 1.6917 (2.1042)  loss: 1.6944 (2.1042)  time: 0.1681\n",
      "Epoch: [2]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 1.6682 (2.0948)  loss: 1.6366 (2.0948)  time: 0.2266\n",
      "Epoch: [2]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 1.6571 (2.0854)  loss: 1.7114 (2.0854)  time: 0.2161\n",
      "Epoch: [2]  [4800/5008]  eta: 0:00:37  lr: 0.000500  loss_labels: 1.6521 (2.0764)  loss: 1.7073 (2.0764)  time: 0.1710\n",
      "Epoch: [2]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 1.6517 (2.0676)  loss: 1.5885 (2.0676)  time: 0.1707\n",
      "Epoch: [2]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 1.6365 (2.0591)  loss: 1.6588 (2.0591)  time: 0.1745\n",
      "Epoch: [2]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 1.6296 (2.0584)  loss: 1.5692 (2.0584)  time: 0.1741\n",
      "Epoch: [2] Total time: 0:14:53 (0.1784 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 1.6296 (2.0584)  loss: 1.5692 (2.0584)\n",
      "Test:  [  0/565]  eta: 2:35:05  loss_labels: 2.1391 (2.1391)  loss: 2.1391 (2.1391)  time: 16.4701\n",
      "Test:  [100/565]  eta: 0:02:21  loss_labels: 2.3186 (2.3981)  loss: 2.4748 (2.3981)  time: 0.1305\n",
      "Test:  [200/565]  eta: 0:01:21  loss_labels: 2.2363 (2.3564)  loss: 2.3703 (2.3564)  time: 0.1322\n",
      "Test:  [300/565]  eta: 0:00:51  loss_labels: 2.1592 (2.3046)  loss: 2.0040 (2.3046)  time: 0.1353\n",
      "Test:  [400/565]  eta: 0:00:29  loss_labels: 2.3477 (2.3222)  loss: 2.2531 (2.3222)  time: 0.1139\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 2.1171 (2.2877)  loss: 2.4329 (2.2877)  time: 0.1427\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 2.1726 (2.2742)  loss: 2.3283 (2.2742)  time: 0.1750\n",
      "Test: Total time: 0:01:35 (0.1690 s / it)\n",
      "Averaged stats: loss_labels: 2.1726 (2.2742)  loss: 2.3283 (2.2742)\n",
      "acc: 0.5468204617500305\n",
      "top 1 and top 5 accuracies {'top1': 0.5468204730515703, 'top5': 0.7329391236913533, 'loss': tensor(0.0178, device='cuda:0')}\n",
      "Epoch: [3]  [   0/5008]  eta: 1 day, 11:13:35  lr: 0.000500  loss_labels: 1.6502 (1.6502)  loss: 1.6502 (1.6502)  time: 25.3227\n",
      "Epoch: [3]  [ 100/5008]  eta: 0:35:26  lr: 0.000500  loss_labels: 1.6146 (1.6164)  loss: 1.6146 (1.6164)  time: 0.1655\n",
      "Epoch: [3]  [ 200/5008]  eta: 0:24:07  lr: 0.000500  loss_labels: 1.6181 (1.6149)  loss: 1.6322 (1.6149)  time: 0.1673\n",
      "Epoch: [3]  [ 300/5008]  eta: 0:20:09  lr: 0.000500  loss_labels: 1.5698 (1.6037)  loss: 1.5954 (1.6037)  time: 0.1678\n",
      "Epoch: [3]  [ 400/5008]  eta: 0:18:01  lr: 0.000500  loss_labels: 1.5740 (1.5966)  loss: 1.5740 (1.5966)  time: 0.1673\n",
      "Epoch: [3]  [ 500/5008]  eta: 0:16:37  lr: 0.000500  loss_labels: 1.5467 (1.5874)  loss: 1.4962 (1.5874)  time: 0.1668\n",
      "Epoch: [3]  [ 600/5008]  eta: 0:15:36  lr: 0.000500  loss_labels: 1.5350 (1.5779)  loss: 1.4991 (1.5779)  time: 0.1693\n",
      "Epoch: [3]  [ 700/5008]  eta: 0:14:47  lr: 0.000500  loss_labels: 1.5234 (1.5690)  loss: 1.5252 (1.5690)  time: 0.1683\n",
      "Epoch: [3]  [ 800/5008]  eta: 0:14:07  lr: 0.000500  loss_labels: 1.5055 (1.5631)  loss: 1.6175 (1.5631)  time: 0.1684\n",
      "Epoch: [3]  [ 900/5008]  eta: 0:13:32  lr: 0.000500  loss_labels: 1.4796 (1.5565)  loss: 1.4901 (1.5565)  time: 0.1685\n",
      "Epoch: [3]  [1000/5008]  eta: 0:13:00  lr: 0.000500  loss_labels: 1.4760 (1.5502)  loss: 1.5030 (1.5502)  time: 0.1673\n",
      "Epoch: [3]  [1100/5008]  eta: 0:12:31  lr: 0.000500  loss_labels: 1.4644 (1.5438)  loss: 1.4324 (1.5438)  time: 0.1687\n",
      "Epoch: [3]  [1200/5008]  eta: 0:12:04  lr: 0.000500  loss_labels: 1.4865 (1.5384)  loss: 1.4721 (1.5384)  time: 0.1676\n",
      "Epoch: [3]  [1300/5008]  eta: 0:11:39  lr: 0.000500  loss_labels: 1.4653 (1.5325)  loss: 1.5067 (1.5325)  time: 0.1689\n",
      "Epoch: [3]  [1400/5008]  eta: 0:11:15  lr: 0.000500  loss_labels: 1.4643 (1.5269)  loss: 1.4610 (1.5269)  time: 0.1690\n",
      "Epoch: [3]  [1500/5008]  eta: 0:10:52  lr: 0.000500  loss_labels: 1.4462 (1.5228)  loss: 1.3897 (1.5228)  time: 0.1692\n",
      "Epoch: [3]  [1600/5008]  eta: 0:10:30  lr: 0.000500  loss_labels: 1.4337 (1.5181)  loss: 1.4292 (1.5181)  time: 0.1685\n",
      "Epoch: [3]  [1700/5008]  eta: 0:10:08  lr: 0.000500  loss_labels: 1.4598 (1.5138)  loss: 1.4694 (1.5138)  time: 0.1679\n",
      "Epoch: [3]  [1800/5008]  eta: 0:09:47  lr: 0.000500  loss_labels: 1.4184 (1.5085)  loss: 1.4113 (1.5085)  time: 0.1805\n",
      "Epoch: [3]  [1900/5008]  eta: 0:09:27  lr: 0.000500  loss_labels: 1.3896 (1.5023)  loss: 1.3790 (1.5023)  time: 0.1709\n",
      "Epoch: [3]  [2000/5008]  eta: 0:09:07  lr: 0.000500  loss_labels: 1.4047 (1.4969)  loss: 1.3056 (1.4969)  time: 0.1699\n",
      "Epoch: [3]  [2100/5008]  eta: 0:08:47  lr: 0.000500  loss_labels: 1.3597 (1.4909)  loss: 1.3974 (1.4909)  time: 0.1712\n",
      "Epoch: [3]  [2200/5008]  eta: 0:08:28  lr: 0.000500  loss_labels: 1.3663 (1.4854)  loss: 1.3545 (1.4854)  time: 0.1727\n",
      "Epoch: [3]  [2300/5008]  eta: 0:08:08  lr: 0.000500  loss_labels: 1.3711 (1.4806)  loss: 1.3695 (1.4806)  time: 0.1717\n",
      "Epoch: [3]  [2400/5008]  eta: 0:07:49  lr: 0.000500  loss_labels: 1.3488 (1.4753)  loss: 1.3448 (1.4753)  time: 0.1702\n",
      "Epoch: [3]  [2500/5008]  eta: 0:07:30  lr: 0.000500  loss_labels: 1.3384 (1.4697)  loss: 1.3634 (1.4697)  time: 0.1698\n",
      "Epoch: [3]  [2600/5008]  eta: 0:07:12  lr: 0.000500  loss_labels: 1.3550 (1.4650)  loss: 1.2838 (1.4650)  time: 0.1722\n",
      "Epoch: [3]  [2700/5008]  eta: 0:06:53  lr: 0.000500  loss_labels: 1.3173 (1.4599)  loss: 1.2785 (1.4599)  time: 0.1753\n",
      "Epoch: [3]  [2800/5008]  eta: 0:06:34  lr: 0.000500  loss_labels: 1.2848 (1.4548)  loss: 1.2784 (1.4548)  time: 0.1697\n",
      "Epoch: [3]  [2900/5008]  eta: 0:06:16  lr: 0.000500  loss_labels: 1.3025 (1.4495)  loss: 1.3294 (1.4495)  time: 0.1708\n",
      "Epoch: [3]  [3000/5008]  eta: 0:05:57  lr: 0.000500  loss_labels: 1.3261 (1.4454)  loss: 1.2739 (1.4454)  time: 0.1692\n",
      "Epoch: [3]  [3100/5008]  eta: 0:05:39  lr: 0.000500  loss_labels: 1.2907 (1.4406)  loss: 1.2128 (1.4406)  time: 0.1690\n",
      "Epoch: [3]  [3200/5008]  eta: 0:05:21  lr: 0.000500  loss_labels: 1.2461 (1.4351)  loss: 1.2833 (1.4351)  time: 0.1693\n",
      "Epoch: [3]  [3300/5008]  eta: 0:05:03  lr: 0.000500  loss_labels: 1.2802 (1.4301)  loss: 1.1986 (1.4301)  time: 0.1700\n",
      "Epoch: [3]  [3400/5008]  eta: 0:04:45  lr: 0.000500  loss_labels: 1.2690 (1.4249)  loss: 1.2143 (1.4249)  time: 0.2010\n",
      "Epoch: [3]  [3500/5008]  eta: 0:04:27  lr: 0.000500  loss_labels: 1.2976 (1.4208)  loss: 1.3364 (1.4208)  time: 0.1705\n",
      "Epoch: [3]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 1.2665 (1.4163)  loss: 1.2671 (1.4163)  time: 0.1716\n",
      "Epoch: [3]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 1.2017 (1.4111)  loss: 1.1875 (1.4111)  time: 0.1712\n",
      "Epoch: [3]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 1.2222 (1.4061)  loss: 1.2241 (1.4061)  time: 0.1705\n",
      "Epoch: [3]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 1.2420 (1.4020)  loss: 1.2354 (1.4020)  time: 0.1693\n",
      "Epoch: [3]  [4000/5008]  eta: 0:02:57  lr: 0.000500  loss_labels: 1.2232 (1.3976)  loss: 1.1819 (1.3976)  time: 0.1708\n",
      "Epoch: [3]  [4100/5008]  eta: 0:02:40  lr: 0.000500  loss_labels: 1.2149 (1.3927)  loss: 1.1849 (1.3927)  time: 0.1715\n",
      "Epoch: [3]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 1.2131 (1.3884)  loss: 1.1674 (1.3884)  time: 0.1712\n",
      "Epoch: [3]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 1.1872 (1.3841)  loss: 1.1295 (1.3841)  time: 0.1696\n",
      "Epoch: [3]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 1.2261 (1.3806)  loss: 1.2261 (1.3806)  time: 0.1712\n",
      "Epoch: [3]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 1.1810 (1.3761)  loss: 1.1953 (1.3761)  time: 0.1717\n",
      "Epoch: [3]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 1.1670 (1.3717)  loss: 1.1756 (1.3717)  time: 0.1711\n",
      "Epoch: [3]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 1.1657 (1.3672)  loss: 1.1732 (1.3672)  time: 0.1735\n",
      "Epoch: [3]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 1.1621 (1.3629)  loss: 1.2078 (1.3629)  time: 0.1716\n",
      "Epoch: [3]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 1.1512 (1.3584)  loss: 1.0665 (1.3584)  time: 0.1707\n",
      "Epoch: [3]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 1.1741 (1.3546)  loss: 1.1177 (1.3546)  time: 0.1716\n",
      "Epoch: [3]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 1.1658 (1.3543)  loss: 1.0918 (1.3543)  time: 0.1718\n",
      "Epoch: [3] Total time: 0:14:39 (0.1756 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 1.1658 (1.3543)  loss: 1.0918 (1.3543)\n",
      "Test:  [  0/565]  eta: 3:02:01  loss_labels: 1.6218 (1.6218)  loss: 1.6218 (1.6218)  time: 19.3308\n",
      "Test:  [100/565]  eta: 0:02:27  loss_labels: 1.8416 (1.9716)  loss: 1.9473 (1.9716)  time: 0.1266\n",
      "Test:  [200/565]  eta: 0:01:22  loss_labels: 1.7724 (1.9205)  loss: 1.9187 (1.9205)  time: 0.1206\n",
      "Test:  [300/565]  eta: 0:00:51  loss_labels: 1.6444 (1.8656)  loss: 1.5983 (1.8656)  time: 0.1233\n",
      "Test:  [400/565]  eta: 0:00:29  loss_labels: 1.9416 (1.8907)  loss: 1.8099 (1.8907)  time: 0.1194\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.6765 (1.8639)  loss: 2.0476 (1.8639)  time: 0.1312\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.6848 (1.8469)  loss: 1.8239 (1.8469)  time: 0.1253\n",
      "Test: Total time: 0:01:33 (0.1648 s / it)\n",
      "Averaged stats: loss_labels: 1.6848 (1.8469)  loss: 1.8239 (1.8469)\n",
      "acc: 0.6249099969863892\n",
      "top 1 and top 5 accuracies {'top1': 0.6249099872597352, 'top5': 0.7933584445798483, 'loss': tensor(0.0145, device='cuda:0')}\n",
      "Epoch: [4]  [   0/5008]  eta: 1 day, 8:17:15  lr: 0.000500  loss_labels: 1.2333 (1.2333)  loss: 1.2333 (1.2333)  time: 23.2099\n",
      "Epoch: [4]  [ 100/5008]  eta: 0:32:30  lr: 0.000500  loss_labels: 1.1470 (1.1445)  loss: 1.1391 (1.1445)  time: 0.1689\n",
      "Epoch: [4]  [ 200/5008]  eta: 0:22:44  lr: 0.000500  loss_labels: 1.1379 (1.1339)  loss: 1.1241 (1.1339)  time: 0.1696\n",
      "Epoch: [4]  [ 300/5008]  eta: 0:19:17  lr: 0.000500  loss_labels: 1.1291 (1.1332)  loss: 1.1803 (1.1332)  time: 0.1697\n",
      "Epoch: [4]  [ 400/5008]  eta: 0:17:25  lr: 0.000500  loss_labels: 1.1061 (1.1301)  loss: 1.0998 (1.1301)  time: 0.1691\n",
      "Epoch: [4]  [ 500/5008]  eta: 0:16:10  lr: 0.000500  loss_labels: 1.0823 (1.1237)  loss: 1.0513 (1.1237)  time: 0.1694\n",
      "Epoch: [4]  [ 600/5008]  eta: 0:15:17  lr: 0.000500  loss_labels: 1.0900 (1.1166)  loss: 1.0261 (1.1166)  time: 0.1716\n",
      "Epoch: [4]  [ 700/5008]  eta: 0:14:33  lr: 0.000500  loss_labels: 1.0859 (1.1113)  loss: 1.0964 (1.1113)  time: 0.1717\n",
      "Epoch: [4]  [ 800/5008]  eta: 0:13:57  lr: 0.000500  loss_labels: 1.0737 (1.1084)  loss: 1.1253 (1.1084)  time: 0.1712\n",
      "Epoch: [4]  [ 900/5008]  eta: 0:13:24  lr: 0.000500  loss_labels: 1.0567 (1.1039)  loss: 1.0274 (1.1039)  time: 0.1720\n",
      "Epoch: [4]  [1000/5008]  eta: 0:12:55  lr: 0.000500  loss_labels: 1.0666 (1.1018)  loss: 1.0644 (1.1018)  time: 0.1712\n",
      "Epoch: [4]  [1100/5008]  eta: 0:12:27  lr: 0.000500  loss_labels: 1.0505 (1.0978)  loss: 1.0505 (1.0978)  time: 0.1682\n",
      "Epoch: [4]  [1200/5008]  eta: 0:12:01  lr: 0.000500  loss_labels: 1.0653 (1.0958)  loss: 1.0160 (1.0958)  time: 0.1698\n",
      "Epoch: [4]  [1300/5008]  eta: 0:11:37  lr: 0.000500  loss_labels: 1.0521 (1.0922)  loss: 1.0841 (1.0922)  time: 0.1707\n",
      "Epoch: [4]  [1400/5008]  eta: 0:11:24  lr: 0.000500  loss_labels: 1.0313 (1.0885)  loss: 1.0488 (1.0885)  time: 0.1701\n",
      "Epoch: [4]  [1500/5008]  eta: 0:11:00  lr: 0.000500  loss_labels: 1.0485 (1.0869)  loss: 1.0022 (1.0869)  time: 0.1703\n",
      "Epoch: [4]  [1600/5008]  eta: 0:10:37  lr: 0.000500  loss_labels: 1.0432 (1.0840)  loss: 1.0484 (1.0840)  time: 0.1701\n",
      "Epoch: [4]  [1700/5008]  eta: 0:10:15  lr: 0.000500  loss_labels: 1.0503 (1.0822)  loss: 1.0436 (1.0822)  time: 0.1707\n",
      "Epoch: [4]  [1800/5008]  eta: 0:09:54  lr: 0.000500  loss_labels: 1.0174 (1.0789)  loss: 0.9748 (1.0789)  time: 0.1710\n",
      "Epoch: [4]  [1900/5008]  eta: 0:09:33  lr: 0.000500  loss_labels: 1.0060 (1.0756)  loss: 1.0377 (1.0756)  time: 0.1702\n",
      "Epoch: [4]  [2000/5008]  eta: 0:09:12  lr: 0.000500  loss_labels: 1.0391 (1.0734)  loss: 0.9503 (1.0734)  time: 0.1680\n",
      "Epoch: [4]  [2100/5008]  eta: 0:08:52  lr: 0.000500  loss_labels: 0.9896 (1.0693)  loss: 1.0001 (1.0693)  time: 0.1697\n",
      "Epoch: [4]  [2200/5008]  eta: 0:08:32  lr: 0.000500  loss_labels: 1.0088 (1.0659)  loss: 0.9747 (1.0659)  time: 0.1688\n",
      "Epoch: [4]  [2300/5008]  eta: 0:08:12  lr: 0.000500  loss_labels: 1.0139 (1.0636)  loss: 1.0119 (1.0636)  time: 0.1693\n",
      "Epoch: [4]  [2400/5008]  eta: 0:07:52  lr: 0.000500  loss_labels: 0.9899 (1.0606)  loss: 0.9852 (1.0606)  time: 0.1683\n",
      "Epoch: [4]  [2500/5008]  eta: 0:07:33  lr: 0.000500  loss_labels: 0.9681 (1.0576)  loss: 0.9949 (1.0576)  time: 0.1707\n",
      "Epoch: [4]  [2600/5008]  eta: 0:07:14  lr: 0.000500  loss_labels: 1.0068 (1.0552)  loss: 0.9437 (1.0552)  time: 0.1719\n",
      "Epoch: [4]  [2700/5008]  eta: 0:06:55  lr: 0.000500  loss_labels: 0.9792 (1.0524)  loss: 0.9454 (1.0524)  time: 0.1689\n",
      "Epoch: [4]  [2800/5008]  eta: 0:06:36  lr: 0.000500  loss_labels: 0.9597 (1.0494)  loss: 0.9413 (1.0494)  time: 0.1707\n",
      "Epoch: [4]  [2900/5008]  eta: 0:06:18  lr: 0.000500  loss_labels: 0.9329 (1.0460)  loss: 0.9917 (1.0460)  time: 0.1695\n",
      "Epoch: [4]  [3000/5008]  eta: 0:05:59  lr: 0.000500  loss_labels: 0.9626 (1.0433)  loss: 0.9332 (1.0433)  time: 0.1688\n",
      "Epoch: [4]  [3100/5008]  eta: 0:05:41  lr: 0.000500  loss_labels: 0.9402 (1.0406)  loss: 0.8927 (1.0406)  time: 0.1682\n",
      "Epoch: [4]  [3200/5008]  eta: 0:05:22  lr: 0.000500  loss_labels: 0.9370 (1.0379)  loss: 0.9753 (1.0379)  time: 0.1683\n",
      "Epoch: [4]  [3300/5008]  eta: 0:05:04  lr: 0.000500  loss_labels: 0.9540 (1.0353)  loss: 0.8819 (1.0353)  time: 0.1691\n",
      "Epoch: [4]  [3400/5008]  eta: 0:04:46  lr: 0.000500  loss_labels: 0.9288 (1.0321)  loss: 0.9078 (1.0321)  time: 0.1697\n",
      "Epoch: [4]  [3500/5008]  eta: 0:04:27  lr: 0.000500  loss_labels: 0.9441 (1.0297)  loss: 0.9927 (1.0297)  time: 0.1691\n",
      "Epoch: [4]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.9151 (1.0269)  loss: 0.8815 (1.0269)  time: 0.1697\n",
      "Epoch: [4]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 0.9041 (1.0238)  loss: 0.8846 (1.0238)  time: 0.1692\n",
      "Epoch: [4]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 0.9103 (1.0210)  loss: 0.8532 (1.0210)  time: 0.1696\n",
      "Epoch: [4]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.9459 (1.0192)  loss: 0.9262 (1.0192)  time: 0.1691\n",
      "Epoch: [4]  [4000/5008]  eta: 0:02:58  lr: 0.000500  loss_labels: 0.9143 (1.0168)  loss: 0.8755 (1.0168)  time: 0.1692\n",
      "Epoch: [4]  [4100/5008]  eta: 0:02:40  lr: 0.000500  loss_labels: 0.9126 (1.0141)  loss: 0.9021 (1.0141)  time: 0.1684\n",
      "Epoch: [4]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.9300 (1.0117)  loss: 0.9097 (1.0117)  time: 0.1678\n",
      "Epoch: [4]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.8919 (1.0091)  loss: 0.8098 (1.0091)  time: 0.1689\n",
      "Epoch: [4]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 0.9069 (1.0072)  loss: 0.8903 (1.0072)  time: 0.1707\n",
      "Epoch: [4]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.8767 (1.0046)  loss: 0.9126 (1.0046)  time: 0.1714\n",
      "Epoch: [4]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.8908 (1.0022)  loss: 0.8432 (1.0022)  time: 0.1681\n",
      "Epoch: [4]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.8862 (0.9997)  loss: 0.9120 (0.9997)  time: 0.1690\n",
      "Epoch: [4]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.8567 (0.9970)  loss: 0.8999 (0.9970)  time: 0.1684\n",
      "Epoch: [4]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.8578 (0.9943)  loss: 0.8060 (0.9943)  time: 0.1688\n",
      "Epoch: [4]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.8988 (0.9921)  loss: 0.8545 (0.9921)  time: 0.1700\n",
      "Epoch: [4]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.8937 (0.9919)  loss: 0.8289 (0.9919)  time: 0.1697\n",
      "Epoch: [4] Total time: 0:14:38 (0.1753 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.8937 (0.9919)  loss: 0.8289 (0.9919)\n",
      "Test:  [  0/565]  eta: 2:50:05  loss_labels: 1.4667 (1.4667)  loss: 1.4667 (1.4667)  time: 18.0629\n",
      "Test:  [100/565]  eta: 0:02:24  loss_labels: 1.6357 (1.7373)  loss: 1.7170 (1.7373)  time: 0.1300\n",
      "Test:  [200/565]  eta: 0:01:23  loss_labels: 1.5175 (1.6786)  loss: 1.6951 (1.6786)  time: 0.1464\n",
      "Test:  [300/565]  eta: 0:00:52  loss_labels: 1.3939 (1.6231)  loss: 1.3002 (1.6231)  time: 0.1235\n",
      "Test:  [400/565]  eta: 0:00:29  loss_labels: 1.7027 (1.6483)  loss: 1.5636 (1.6483)  time: 0.1215\n",
      "Test:  [500/565]  eta: 0:00:11  loss_labels: 1.4455 (1.6215)  loss: 1.7521 (1.6215)  time: 0.1294\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.4817 (1.6041)  loss: 1.6347 (1.6041)  time: 0.1343\n",
      "Test: Total time: 0:01:35 (0.1694 s / it)\n",
      "Averaged stats: loss_labels: 1.4817 (1.6041)  loss: 1.6347 (1.6041)\n",
      "acc: 0.6717442870140076\n",
      "top 1 and top 5 accuracies {'top1': 0.6717443084251925, 'top5': 0.8255691574807511, 'loss': tensor(0.0126, device='cuda:0')}\n",
      "Epoch: [5]  [   0/5008]  eta: 1 day, 8:45:25  lr: 0.000500  loss_labels: 0.8954 (0.8954)  loss: 0.8954 (0.8954)  time: 23.5474\n",
      "Epoch: [5]  [ 100/5008]  eta: 0:32:43  lr: 0.000500  loss_labels: 0.8483 (0.8710)  loss: 0.8764 (0.8710)  time: 0.1697\n",
      "Epoch: [5]  [ 200/5008]  eta: 0:22:49  lr: 0.000500  loss_labels: 0.8748 (0.8705)  loss: 0.8624 (0.8705)  time: 0.1681\n",
      "Epoch: [5]  [ 300/5008]  eta: 0:19:21  lr: 0.000500  loss_labels: 0.8672 (0.8701)  loss: 0.8662 (0.8701)  time: 0.1709\n",
      "Epoch: [5]  [ 400/5008]  eta: 0:17:29  lr: 0.000500  loss_labels: 0.8386 (0.8645)  loss: 0.8649 (0.8645)  time: 0.1709\n",
      "Epoch: [5]  [ 500/5008]  eta: 0:16:15  lr: 0.000500  loss_labels: 0.8260 (0.8583)  loss: 0.7562 (0.8583)  time: 0.1706\n",
      "Epoch: [5]  [ 600/5008]  eta: 0:15:19  lr: 0.000500  loss_labels: 0.8381 (0.8542)  loss: 0.7767 (0.8542)  time: 0.1690\n",
      "Epoch: [5]  [ 700/5008]  eta: 0:14:35  lr: 0.000500  loss_labels: 0.8380 (0.8507)  loss: 0.8571 (0.8507)  time: 0.1713\n",
      "Epoch: [5]  [ 800/5008]  eta: 0:13:58  lr: 0.000500  loss_labels: 0.8206 (0.8486)  loss: 0.8695 (0.8486)  time: 0.1708\n",
      "Epoch: [5]  [ 900/5008]  eta: 0:13:26  lr: 0.000500  loss_labels: 0.8096 (0.8458)  loss: 0.7929 (0.8458)  time: 0.1713\n",
      "Epoch: [5]  [1000/5008]  eta: 0:12:56  lr: 0.000500  loss_labels: 0.8216 (0.8441)  loss: 0.8211 (0.8441)  time: 0.1693\n",
      "Epoch: [5]  [1100/5008]  eta: 0:12:28  lr: 0.000500  loss_labels: 0.8029 (0.8415)  loss: 0.7546 (0.8415)  time: 0.1694\n",
      "Epoch: [5]  [1200/5008]  eta: 0:12:02  lr: 0.000500  loss_labels: 0.8114 (0.8399)  loss: 0.8046 (0.8399)  time: 0.1707\n",
      "Epoch: [5]  [1300/5008]  eta: 0:11:38  lr: 0.000500  loss_labels: 0.8054 (0.8376)  loss: 0.8277 (0.8376)  time: 0.1690\n",
      "Epoch: [5]  [1400/5008]  eta: 0:11:14  lr: 0.000500  loss_labels: 0.7994 (0.8355)  loss: 0.8183 (0.8355)  time: 0.1698\n",
      "Epoch: [5]  [1500/5008]  eta: 0:10:51  lr: 0.000500  loss_labels: 0.8023 (0.8344)  loss: 0.7630 (0.8344)  time: 0.1690\n",
      "Epoch: [5]  [1600/5008]  eta: 0:10:29  lr: 0.000500  loss_labels: 0.7992 (0.8326)  loss: 0.7972 (0.8326)  time: 0.1704\n",
      "Epoch: [5]  [1700/5008]  eta: 0:10:08  lr: 0.000500  loss_labels: 0.8013 (0.8314)  loss: 0.8313 (0.8314)  time: 0.1695\n",
      "Epoch: [5]  [1800/5008]  eta: 0:09:47  lr: 0.000500  loss_labels: 0.7849 (0.8292)  loss: 0.7813 (0.8292)  time: 0.1728\n",
      "Epoch: [5]  [1900/5008]  eta: 0:09:27  lr: 0.000500  loss_labels: 0.7732 (0.8271)  loss: 0.7833 (0.8271)  time: 0.1715\n",
      "Epoch: [5]  [2000/5008]  eta: 0:09:07  lr: 0.000500  loss_labels: 0.7964 (0.8256)  loss: 0.7185 (0.8256)  time: 0.1702\n",
      "Epoch: [5]  [2100/5008]  eta: 0:08:48  lr: 0.000500  loss_labels: 0.7636 (0.8231)  loss: 0.7589 (0.8231)  time: 0.1724\n",
      "Epoch: [5]  [2200/5008]  eta: 0:08:28  lr: 0.000500  loss_labels: 0.7630 (0.8203)  loss: 0.7692 (0.8203)  time: 0.1710\n",
      "Epoch: [5]  [2300/5008]  eta: 0:08:09  lr: 0.000500  loss_labels: 0.8015 (0.8192)  loss: 0.7941 (0.8192)  time: 0.1743\n",
      "Epoch: [5]  [2400/5008]  eta: 0:07:50  lr: 0.000500  loss_labels: 0.7757 (0.8175)  loss: 0.7794 (0.8175)  time: 0.1722\n",
      "Epoch: [5]  [2500/5008]  eta: 0:07:31  lr: 0.000500  loss_labels: 0.7594 (0.8152)  loss: 0.7442 (0.8152)  time: 0.1701\n",
      "Epoch: [5]  [2600/5008]  eta: 0:07:12  lr: 0.000500  loss_labels: 0.7801 (0.8136)  loss: 0.7177 (0.8136)  time: 0.1708\n",
      "Epoch: [5]  [2700/5008]  eta: 0:06:53  lr: 0.000500  loss_labels: 0.7641 (0.8122)  loss: 0.7707 (0.8122)  time: 0.1676\n",
      "Epoch: [5]  [2800/5008]  eta: 0:06:34  lr: 0.000500  loss_labels: 0.7450 (0.8104)  loss: 0.7387 (0.8104)  time: 0.1681\n",
      "Epoch: [5]  [2900/5008]  eta: 0:06:16  lr: 0.000500  loss_labels: 0.7407 (0.8084)  loss: 0.7505 (0.8084)  time: 0.1681\n",
      "Epoch: [5]  [3000/5008]  eta: 0:05:57  lr: 0.000500  loss_labels: 0.7650 (0.8069)  loss: 0.7335 (0.8069)  time: 0.1706\n",
      "Epoch: [5]  [3100/5008]  eta: 0:05:39  lr: 0.000500  loss_labels: 0.7556 (0.8052)  loss: 0.6773 (0.8052)  time: 0.1684\n",
      "Epoch: [5]  [3200/5008]  eta: 0:05:20  lr: 0.000500  loss_labels: 0.7317 (0.8035)  loss: 0.7662 (0.8035)  time: 0.1682\n",
      "Epoch: [5]  [3300/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.7532 (0.8019)  loss: 0.7220 (0.8019)  time: 0.1708\n",
      "Epoch: [5]  [3400/5008]  eta: 0:04:44  lr: 0.000500  loss_labels: 0.7363 (0.7998)  loss: 0.7253 (0.7998)  time: 0.1687\n",
      "Epoch: [5]  [3500/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.7334 (0.7983)  loss: 0.7823 (0.7983)  time: 0.1681\n",
      "Epoch: [5]  [3600/5008]  eta: 0:04:08  lr: 0.000500  loss_labels: 0.6991 (0.7962)  loss: 0.6550 (0.7962)  time: 0.1687\n",
      "Epoch: [5]  [3700/5008]  eta: 0:03:50  lr: 0.000500  loss_labels: 0.7206 (0.7944)  loss: 0.6851 (0.7944)  time: 0.1683\n",
      "Epoch: [5]  [3800/5008]  eta: 0:03:32  lr: 0.000500  loss_labels: 0.7247 (0.7926)  loss: 0.6887 (0.7926)  time: 0.1689\n",
      "Epoch: [5]  [3900/5008]  eta: 0:03:14  lr: 0.000500  loss_labels: 0.7545 (0.7915)  loss: 0.7278 (0.7915)  time: 0.1670\n",
      "Epoch: [5]  [4000/5008]  eta: 0:02:57  lr: 0.000500  loss_labels: 0.7252 (0.7901)  loss: 0.6926 (0.7901)  time: 0.1666\n",
      "Epoch: [5]  [4100/5008]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.7150 (0.7882)  loss: 0.7323 (0.7882)  time: 0.1669\n",
      "Epoch: [5]  [4200/5008]  eta: 0:02:21  lr: 0.000500  loss_labels: 0.7227 (0.7868)  loss: 0.7096 (0.7868)  time: 0.1702\n",
      "Epoch: [5]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.6956 (0.7849)  loss: 0.6562 (0.7849)  time: 0.1684\n",
      "Epoch: [5]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 0.7279 (0.7838)  loss: 0.7090 (0.7838)  time: 0.1710\n",
      "Epoch: [5]  [4500/5008]  eta: 0:01:28  lr: 0.000500  loss_labels: 0.6900 (0.7820)  loss: 0.7409 (0.7820)  time: 0.1686\n",
      "Epoch: [5]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.6986 (0.7803)  loss: 0.6926 (0.7803)  time: 0.1693\n",
      "Epoch: [5]  [4700/5008]  eta: 0:00:53  lr: 0.000500  loss_labels: 0.6939 (0.7786)  loss: 0.7123 (0.7786)  time: 0.1699\n",
      "Epoch: [5]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.6853 (0.7769)  loss: 0.6999 (0.7769)  time: 0.1697\n",
      "Epoch: [5]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.6919 (0.7751)  loss: 0.6426 (0.7751)  time: 0.1723\n",
      "Epoch: [5]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.6968 (0.7737)  loss: 0.6745 (0.7737)  time: 0.1696\n",
      "Epoch: [5]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.6913 (0.7735)  loss: 0.6494 (0.7735)  time: 0.1691\n",
      "Epoch: [5] Total time: 0:14:34 (0.1747 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.6913 (0.7735)  loss: 0.6494 (0.7735)\n",
      "Test:  [  0/565]  eta: 2:44:28  loss_labels: 1.3767 (1.3767)  loss: 1.3767 (1.3767)  time: 17.4660\n",
      "Test:  [100/565]  eta: 0:02:17  loss_labels: 1.5191 (1.6472)  loss: 1.5631 (1.6472)  time: 0.1204\n",
      "Test:  [200/565]  eta: 0:01:17  loss_labels: 1.3358 (1.5787)  loss: 1.6626 (1.5787)  time: 0.1186\n",
      "Test:  [300/565]  eta: 0:00:48  loss_labels: 1.3209 (1.5170)  loss: 1.1271 (1.5170)  time: 0.1259\n",
      "Test:  [400/565]  eta: 0:00:27  loss_labels: 1.5612 (1.5365)  loss: 1.4856 (1.5365)  time: 0.1213\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.3014 (1.5079)  loss: 1.5513 (1.5079)  time: 0.1190\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.3364 (1.4886)  loss: 1.5356 (1.4886)  time: 0.1274\n",
      "Test: Total time: 0:01:28 (0.1575 s / it)\n",
      "Averaged stats: loss_labels: 1.3364 (1.4886)  loss: 1.5356 (1.4886)\n",
      "acc: 0.696892499923706\n",
      "top 1 and top 5 accuracies {'top1': 0.6968924832437822, 'top5': 0.8396942336453775, 'loss': tensor(0.0116, device='cuda:0')}\n",
      "Epoch: [6]  [   0/5008]  eta: 1 day, 5:12:10  lr: 0.000500  loss_labels: 0.7831 (0.7831)  loss: 0.7831 (0.7831)  time: 20.9926\n",
      "Epoch: [6]  [ 100/5008]  eta: 0:30:53  lr: 0.000500  loss_labels: 0.6818 (0.6824)  loss: 0.6616 (0.6824)  time: 0.1711\n",
      "Epoch: [6]  [ 200/5008]  eta: 0:22:01  lr: 0.000500  loss_labels: 0.6985 (0.6901)  loss: 0.7176 (0.6901)  time: 0.1708\n",
      "Epoch: [6]  [ 300/5008]  eta: 0:18:49  lr: 0.000500  loss_labels: 0.6822 (0.6920)  loss: 0.7326 (0.6920)  time: 0.1697\n",
      "Epoch: [6]  [ 400/5008]  eta: 0:17:04  lr: 0.000500  loss_labels: 0.6774 (0.6894)  loss: 0.7164 (0.6894)  time: 0.1690\n",
      "Epoch: [6]  [ 500/5008]  eta: 0:15:54  lr: 0.000500  loss_labels: 0.6728 (0.6869)  loss: 0.6728 (0.6869)  time: 0.1685\n",
      "Epoch: [6]  [ 600/5008]  eta: 0:15:01  lr: 0.000500  loss_labels: 0.6586 (0.6822)  loss: 0.6353 (0.6822)  time: 0.1695\n",
      "Epoch: [6]  [ 700/5008]  eta: 0:14:20  lr: 0.000500  loss_labels: 0.6628 (0.6797)  loss: 0.6646 (0.6797)  time: 0.1693\n",
      "Epoch: [6]  [ 800/5008]  eta: 0:13:44  lr: 0.000500  loss_labels: 0.6523 (0.6793)  loss: 0.6300 (0.6793)  time: 0.1702\n",
      "Epoch: [6]  [ 900/5008]  eta: 0:13:12  lr: 0.000500  loss_labels: 0.6488 (0.6775)  loss: 0.6312 (0.6775)  time: 0.1695\n",
      "Epoch: [6]  [1000/5008]  eta: 0:12:44  lr: 0.000500  loss_labels: 0.6529 (0.6761)  loss: 0.6544 (0.6761)  time: 0.1690\n",
      "Epoch: [6]  [1100/5008]  eta: 0:12:17  lr: 0.000500  loss_labels: 0.6304 (0.6732)  loss: 0.6197 (0.6732)  time: 0.1696\n",
      "Epoch: [6]  [1200/5008]  eta: 0:11:52  lr: 0.000500  loss_labels: 0.6454 (0.6724)  loss: 0.6407 (0.6724)  time: 0.1692\n",
      "Epoch: [6]  [1300/5008]  eta: 0:11:29  lr: 0.000500  loss_labels: 0.6488 (0.6711)  loss: 0.6747 (0.6711)  time: 0.1709\n",
      "Epoch: [6]  [1400/5008]  eta: 0:11:06  lr: 0.000500  loss_labels: 0.6516 (0.6698)  loss: 0.6673 (0.6698)  time: 0.1704\n",
      "Epoch: [6]  [1500/5008]  eta: 0:10:44  lr: 0.000500  loss_labels: 0.6515 (0.6699)  loss: 0.6110 (0.6699)  time: 0.1722\n",
      "Epoch: [6]  [1600/5008]  eta: 0:10:23  lr: 0.000500  loss_labels: 0.6655 (0.6697)  loss: 0.6272 (0.6697)  time: 0.1703\n",
      "Epoch: [6]  [1700/5008]  eta: 0:10:03  lr: 0.000500  loss_labels: 0.6595 (0.6691)  loss: 0.6608 (0.6691)  time: 0.1730\n",
      "Epoch: [6]  [1800/5008]  eta: 0:09:42  lr: 0.000500  loss_labels: 0.6339 (0.6676)  loss: 0.6102 (0.6676)  time: 0.1693\n",
      "Epoch: [6]  [1900/5008]  eta: 0:09:22  lr: 0.000500  loss_labels: 0.6232 (0.6659)  loss: 0.6225 (0.6659)  time: 0.1704\n",
      "Epoch: [6]  [2000/5008]  eta: 0:09:03  lr: 0.000500  loss_labels: 0.6354 (0.6649)  loss: 0.5788 (0.6649)  time: 0.1724\n",
      "Epoch: [6]  [2100/5008]  eta: 0:08:43  lr: 0.000500  loss_labels: 0.6151 (0.6631)  loss: 0.6479 (0.6631)  time: 0.1712\n",
      "Epoch: [6]  [2200/5008]  eta: 0:08:24  lr: 0.000500  loss_labels: 0.6278 (0.6612)  loss: 0.6393 (0.6612)  time: 0.1711\n",
      "Epoch: [6]  [2300/5008]  eta: 0:08:05  lr: 0.000500  loss_labels: 0.6482 (0.6605)  loss: 0.6085 (0.6605)  time: 0.1689\n",
      "Epoch: [6]  [2400/5008]  eta: 0:07:46  lr: 0.000500  loss_labels: 0.6077 (0.6588)  loss: 0.6348 (0.6588)  time: 0.1710\n",
      "Epoch: [6]  [2500/5008]  eta: 0:07:28  lr: 0.000500  loss_labels: 0.6221 (0.6574)  loss: 0.6200 (0.6574)  time: 0.1725\n",
      "Epoch: [6]  [2600/5008]  eta: 0:07:09  lr: 0.000500  loss_labels: 0.6437 (0.6566)  loss: 0.5971 (0.6566)  time: 0.1895\n",
      "Epoch: [6]  [2700/5008]  eta: 0:06:51  lr: 0.000500  loss_labels: 0.6211 (0.6558)  loss: 0.6084 (0.6558)  time: 0.1696\n",
      "Epoch: [6]  [2800/5008]  eta: 0:06:32  lr: 0.000500  loss_labels: 0.6197 (0.6547)  loss: 0.5825 (0.6547)  time: 0.1686\n",
      "Epoch: [6]  [2900/5008]  eta: 0:06:14  lr: 0.000500  loss_labels: 0.6072 (0.6534)  loss: 0.6285 (0.6534)  time: 0.1709\n",
      "Epoch: [6]  [3000/5008]  eta: 0:05:56  lr: 0.000500  loss_labels: 0.6299 (0.6524)  loss: 0.5988 (0.6524)  time: 0.1706\n",
      "Epoch: [6]  [3100/5008]  eta: 0:05:38  lr: 0.000500  loss_labels: 0.6107 (0.6512)  loss: 0.5577 (0.6512)  time: 0.1699\n",
      "Epoch: [6]  [3200/5008]  eta: 0:05:20  lr: 0.000500  loss_labels: 0.5945 (0.6497)  loss: 0.6013 (0.6497)  time: 0.1718\n",
      "Epoch: [6]  [3300/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.6066 (0.6484)  loss: 0.5919 (0.6484)  time: 0.1730\n",
      "Epoch: [6]  [3400/5008]  eta: 0:04:44  lr: 0.000500  loss_labels: 0.6119 (0.6470)  loss: 0.5615 (0.6470)  time: 0.1747\n",
      "Epoch: [6]  [3500/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.6191 (0.6463)  loss: 0.6639 (0.6463)  time: 0.1743\n",
      "Epoch: [6]  [3600/5008]  eta: 0:04:08  lr: 0.000500  loss_labels: 0.5788 (0.6448)  loss: 0.5698 (0.6448)  time: 0.1742\n",
      "Epoch: [6]  [3700/5008]  eta: 0:03:50  lr: 0.000500  loss_labels: 0.5965 (0.6435)  loss: 0.6017 (0.6435)  time: 0.1703\n",
      "Epoch: [6]  [3800/5008]  eta: 0:03:32  lr: 0.000500  loss_labels: 0.5978 (0.6423)  loss: 0.5719 (0.6423)  time: 0.1685\n",
      "Epoch: [6]  [3900/5008]  eta: 0:03:18  lr: 0.000500  loss_labels: 0.6054 (0.6414)  loss: 0.6007 (0.6414)  time: 0.7370\n",
      "Epoch: [6]  [4000/5008]  eta: 0:03:00  lr: 0.000500  loss_labels: 0.5881 (0.6404)  loss: 0.5827 (0.6404)  time: 0.1689\n",
      "Epoch: [6]  [4100/5008]  eta: 0:02:42  lr: 0.000500  loss_labels: 0.5812 (0.6390)  loss: 0.5959 (0.6390)  time: 0.1686\n",
      "Epoch: [6]  [4200/5008]  eta: 0:02:24  lr: 0.000500  loss_labels: 0.5945 (0.6381)  loss: 0.5995 (0.6381)  time: 0.1689\n",
      "Epoch: [6]  [4300/5008]  eta: 0:02:06  lr: 0.000500  loss_labels: 0.5746 (0.6369)  loss: 0.5772 (0.6369)  time: 0.1682\n",
      "Epoch: [6]  [4400/5008]  eta: 0:01:48  lr: 0.000500  loss_labels: 0.5912 (0.6362)  loss: 0.6117 (0.6362)  time: 0.1684\n",
      "Epoch: [6]  [4500/5008]  eta: 0:01:30  lr: 0.000500  loss_labels: 0.5774 (0.6351)  loss: 0.5968 (0.6351)  time: 0.1684\n",
      "Epoch: [6]  [4600/5008]  eta: 0:01:12  lr: 0.000500  loss_labels: 0.5912 (0.6342)  loss: 0.5567 (0.6342)  time: 0.1687\n",
      "Epoch: [6]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.5641 (0.6327)  loss: 0.5822 (0.6327)  time: 0.1680\n",
      "Epoch: [6]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.5459 (0.6314)  loss: 0.5838 (0.6314)  time: 0.1687\n",
      "Epoch: [6]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.5674 (0.6300)  loss: 0.5139 (0.6300)  time: 0.1685\n",
      "Epoch: [6]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.5979 (0.6293)  loss: 0.6039 (0.6293)  time: 0.1698\n",
      "Epoch: [6]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.5914 (0.6292)  loss: 0.5759 (0.6292)  time: 0.1701\n",
      "Epoch: [6] Total time: 0:14:47 (0.1771 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.5914 (0.6292)  loss: 0.5759 (0.6292)\n",
      "Test:  [  0/565]  eta: 2:43:44  loss_labels: 1.1416 (1.1416)  loss: 1.1416 (1.1416)  time: 17.3887\n",
      "Test:  [100/565]  eta: 0:02:17  loss_labels: 1.3567 (1.4907)  loss: 1.3567 (1.4907)  time: 0.1261\n",
      "Test:  [200/565]  eta: 0:01:17  loss_labels: 1.2711 (1.4336)  loss: 1.4445 (1.4336)  time: 0.1228\n",
      "Test:  [300/565]  eta: 0:00:48  loss_labels: 1.1775 (1.3799)  loss: 1.0184 (1.3799)  time: 0.1250\n",
      "Test:  [400/565]  eta: 0:00:27  loss_labels: 1.4311 (1.4014)  loss: 1.4029 (1.4014)  time: 0.1096\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.1550 (1.3758)  loss: 1.4714 (1.3758)  time: 0.1218\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.2396 (1.3601)  loss: 1.4473 (1.3601)  time: 0.1401\n",
      "Test: Total time: 0:01:29 (0.1589 s / it)\n",
      "Averaged stats: loss_labels: 1.2396 (1.3601)  loss: 1.4473 (1.3601)\n",
      "acc: 0.7239239811897278\n",
      "top 1 and top 5 accuracies {'top1': 0.7239240015509888, 'top5': 0.8572397939400653, 'loss': tensor(0.0106, device='cuda:0')}\n",
      "Epoch: [7]  [   0/5008]  eta: 1 day, 5:26:34  lr: 0.000500  loss_labels: 0.6992 (0.6992)  loss: 0.6992 (0.6992)  time: 21.1651\n",
      "Epoch: [7]  [ 100/5008]  eta: 0:30:43  lr: 0.000500  loss_labels: 0.5608 (0.5612)  loss: 0.5625 (0.5612)  time: 0.1684\n",
      "Epoch: [7]  [ 200/5008]  eta: 0:21:48  lr: 0.000500  loss_labels: 0.5626 (0.5654)  loss: 0.5442 (0.5654)  time: 0.1677\n",
      "Epoch: [7]  [ 300/5008]  eta: 0:18:37  lr: 0.000500  loss_labels: 0.5878 (0.5700)  loss: 0.6357 (0.5700)  time: 0.1670\n",
      "Epoch: [7]  [ 400/5008]  eta: 0:16:53  lr: 0.000500  loss_labels: 0.5580 (0.5670)  loss: 0.5926 (0.5670)  time: 0.1675\n",
      "Epoch: [7]  [ 500/5008]  eta: 0:15:44  lr: 0.000500  loss_labels: 0.5562 (0.5660)  loss: 0.5565 (0.5660)  time: 0.1689\n",
      "Epoch: [7]  [ 600/5008]  eta: 0:14:52  lr: 0.000500  loss_labels: 0.5482 (0.5619)  loss: 0.5115 (0.5619)  time: 0.1671\n",
      "Epoch: [7]  [ 700/5008]  eta: 0:14:11  lr: 0.000500  loss_labels: 0.5548 (0.5610)  loss: 0.5846 (0.5610)  time: 0.1697\n",
      "Epoch: [7]  [ 800/5008]  eta: 0:13:36  lr: 0.000500  loss_labels: 0.5480 (0.5604)  loss: 0.5224 (0.5604)  time: 0.1685\n",
      "Epoch: [7]  [ 900/5008]  eta: 0:13:05  lr: 0.000500  loss_labels: 0.5324 (0.5588)  loss: 0.5386 (0.5588)  time: 0.1689\n",
      "Epoch: [7]  [1000/5008]  eta: 0:12:37  lr: 0.000500  loss_labels: 0.5395 (0.5577)  loss: 0.5395 (0.5577)  time: 0.1683\n",
      "Epoch: [7]  [1100/5008]  eta: 0:12:11  lr: 0.000500  loss_labels: 0.5229 (0.5562)  loss: 0.5140 (0.5562)  time: 0.1670\n",
      "Epoch: [7]  [1200/5008]  eta: 0:11:46  lr: 0.000500  loss_labels: 0.5335 (0.5550)  loss: 0.5736 (0.5550)  time: 0.1657\n",
      "Epoch: [7]  [1300/5008]  eta: 0:11:26  lr: 0.000500  loss_labels: 0.5410 (0.5545)  loss: 0.5550 (0.5545)  time: 0.1726\n",
      "Epoch: [7]  [1400/5008]  eta: 0:11:05  lr: 0.000500  loss_labels: 0.5075 (0.5528)  loss: 0.5061 (0.5528)  time: 0.1732\n",
      "Epoch: [7]  [1500/5008]  eta: 0:10:44  lr: 0.000500  loss_labels: 0.5381 (0.5525)  loss: 0.4994 (0.5525)  time: 0.1672\n",
      "Epoch: [7]  [1600/5008]  eta: 0:10:22  lr: 0.000500  loss_labels: 0.5322 (0.5524)  loss: 0.5145 (0.5524)  time: 0.1674\n",
      "Epoch: [7]  [1700/5008]  eta: 0:10:01  lr: 0.000500  loss_labels: 0.5466 (0.5523)  loss: 0.5378 (0.5523)  time: 0.1689\n",
      "Epoch: [7]  [1800/5008]  eta: 0:09:40  lr: 0.000500  loss_labels: 0.5395 (0.5513)  loss: 0.5273 (0.5513)  time: 0.1691\n",
      "Epoch: [7]  [1900/5008]  eta: 0:09:20  lr: 0.000500  loss_labels: 0.5424 (0.5508)  loss: 0.5520 (0.5508)  time: 0.1678\n",
      "Epoch: [7]  [2000/5008]  eta: 0:09:00  lr: 0.000500  loss_labels: 0.5269 (0.5501)  loss: 0.4875 (0.5501)  time: 0.1671\n",
      "Epoch: [7]  [2100/5008]  eta: 0:08:40  lr: 0.000500  loss_labels: 0.5152 (0.5486)  loss: 0.5427 (0.5486)  time: 0.1677\n",
      "Epoch: [7]  [2200/5008]  eta: 0:08:24  lr: 0.000500  loss_labels: 0.5217 (0.5470)  loss: 0.5395 (0.5470)  time: 0.1663\n",
      "Epoch: [7]  [2300/5008]  eta: 0:08:05  lr: 0.000500  loss_labels: 0.5336 (0.5467)  loss: 0.5038 (0.5467)  time: 0.1680\n",
      "Epoch: [7]  [2400/5008]  eta: 0:07:46  lr: 0.000500  loss_labels: 0.5143 (0.5457)  loss: 0.5352 (0.5457)  time: 0.1766\n",
      "Epoch: [7]  [2500/5008]  eta: 0:07:27  lr: 0.000500  loss_labels: 0.5224 (0.5447)  loss: 0.5206 (0.5447)  time: 0.1670\n",
      "Epoch: [7]  [2600/5008]  eta: 0:07:08  lr: 0.000500  loss_labels: 0.5179 (0.5440)  loss: 0.4818 (0.5440)  time: 0.1677\n",
      "Epoch: [7]  [2700/5008]  eta: 0:06:49  lr: 0.000500  loss_labels: 0.5250 (0.5432)  loss: 0.5273 (0.5432)  time: 0.1671\n",
      "Epoch: [7]  [2800/5008]  eta: 0:06:31  lr: 0.000500  loss_labels: 0.5183 (0.5425)  loss: 0.4962 (0.5425)  time: 0.1673\n",
      "Epoch: [7]  [2900/5008]  eta: 0:06:12  lr: 0.000500  loss_labels: 0.5095 (0.5416)  loss: 0.5237 (0.5416)  time: 0.1684\n",
      "Epoch: [7]  [3000/5008]  eta: 0:05:54  lr: 0.000500  loss_labels: 0.5302 (0.5409)  loss: 0.4878 (0.5409)  time: 0.1691\n",
      "Epoch: [7]  [3100/5008]  eta: 0:05:36  lr: 0.000500  loss_labels: 0.5032 (0.5400)  loss: 0.4831 (0.5400)  time: 0.1682\n",
      "Epoch: [7]  [3200/5008]  eta: 0:05:18  lr: 0.000500  loss_labels: 0.4951 (0.5391)  loss: 0.5053 (0.5391)  time: 0.1681\n",
      "Epoch: [7]  [3300/5008]  eta: 0:05:00  lr: 0.000500  loss_labels: 0.5066 (0.5383)  loss: 0.4983 (0.5383)  time: 0.1669\n",
      "Epoch: [7]  [3400/5008]  eta: 0:04:42  lr: 0.000500  loss_labels: 0.4907 (0.5369)  loss: 0.4918 (0.5369)  time: 0.1660\n",
      "Epoch: [7]  [3500/5008]  eta: 0:04:24  lr: 0.000500  loss_labels: 0.5169 (0.5367)  loss: 0.5378 (0.5367)  time: 0.1673\n",
      "Epoch: [7]  [3600/5008]  eta: 0:04:06  lr: 0.000500  loss_labels: 0.5033 (0.5358)  loss: 0.4906 (0.5358)  time: 0.1673\n",
      "Epoch: [7]  [3700/5008]  eta: 0:03:48  lr: 0.000500  loss_labels: 0.4981 (0.5349)  loss: 0.4981 (0.5349)  time: 0.1670\n",
      "Epoch: [7]  [3800/5008]  eta: 0:03:31  lr: 0.000500  loss_labels: 0.4992 (0.5341)  loss: 0.4648 (0.5341)  time: 0.1669\n",
      "Epoch: [7]  [3900/5008]  eta: 0:03:13  lr: 0.000500  loss_labels: 0.5211 (0.5336)  loss: 0.4995 (0.5336)  time: 0.1679\n",
      "Epoch: [7]  [4000/5008]  eta: 0:02:55  lr: 0.000500  loss_labels: 0.5110 (0.5329)  loss: 0.4623 (0.5329)  time: 0.1685\n",
      "Epoch: [7]  [4100/5008]  eta: 0:02:38  lr: 0.000500  loss_labels: 0.4972 (0.5321)  loss: 0.5064 (0.5321)  time: 0.1675\n",
      "Epoch: [7]  [4200/5008]  eta: 0:02:20  lr: 0.000500  loss_labels: 0.4947 (0.5312)  loss: 0.5005 (0.5312)  time: 0.1699\n",
      "Epoch: [7]  [4300/5008]  eta: 0:02:03  lr: 0.000500  loss_labels: 0.4961 (0.5302)  loss: 0.4961 (0.5302)  time: 0.1674\n",
      "Epoch: [7]  [4400/5008]  eta: 0:01:45  lr: 0.000500  loss_labels: 0.5028 (0.5299)  loss: 0.5028 (0.5299)  time: 0.1687\n",
      "Epoch: [7]  [4500/5008]  eta: 0:01:28  lr: 0.000500  loss_labels: 0.4780 (0.5290)  loss: 0.4936 (0.5290)  time: 0.1685\n",
      "Epoch: [7]  [4600/5008]  eta: 0:01:10  lr: 0.000500  loss_labels: 0.4972 (0.5282)  loss: 0.4963 (0.5282)  time: 0.1680\n",
      "Epoch: [7]  [4700/5008]  eta: 0:00:53  lr: 0.000500  loss_labels: 0.4805 (0.5273)  loss: 0.4639 (0.5273)  time: 0.1688\n",
      "Epoch: [7]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.4728 (0.5264)  loss: 0.4936 (0.5264)  time: 0.1687\n",
      "Epoch: [7]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.4792 (0.5254)  loss: 0.3917 (0.5254)  time: 0.1704\n",
      "Epoch: [7]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.4932 (0.5248)  loss: 0.4912 (0.5248)  time: 0.1708\n",
      "Epoch: [7]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.4902 (0.5247)  loss: 0.4709 (0.5247)  time: 0.1703\n",
      "Epoch: [7] Total time: 0:14:28 (0.1734 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.4902 (0.5247)  loss: 0.4709 (0.5247)\n",
      "Test:  [  0/565]  eta: 2:40:54  loss_labels: 1.3466 (1.3466)  loss: 1.3466 (1.3466)  time: 17.0885\n",
      "Test:  [100/565]  eta: 0:02:24  loss_labels: 1.3475 (1.4684)  loss: 1.3553 (1.4684)  time: 0.1324\n",
      "Test:  [200/565]  eta: 0:01:23  loss_labels: 1.2396 (1.4013)  loss: 1.3975 (1.4013)  time: 0.1442\n",
      "Test:  [300/565]  eta: 0:00:53  loss_labels: 1.1580 (1.3482)  loss: 1.0809 (1.3482)  time: 0.1527\n",
      "Test:  [400/565]  eta: 0:00:30  loss_labels: 1.4079 (1.3661)  loss: 1.4446 (1.3661)  time: 0.1360\n",
      "Test:  [500/565]  eta: 0:00:11  loss_labels: 1.1266 (1.3388)  loss: 1.3429 (1.3388)  time: 0.1272\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1750 (1.3238)  loss: 1.2746 (1.3238)  time: 0.2436\n",
      "Test: Total time: 0:01:40 (0.1784 s / it)\n",
      "Averaged stats: loss_labels: 1.1750 (1.3238)  loss: 1.2746 (1.3238)\n",
      "acc: 0.73421311378479\n",
      "top 1 and top 5 accuracies {'top1': 0.734213150168947, 'top5': 0.8630283055447848, 'loss': tensor(0.0104, device='cuda:0')}\n",
      "Epoch: [8]  [   0/5008]  eta: 1 day, 11:50:37  lr: 0.000500  loss_labels: 0.5613 (0.5613)  loss: 0.5613 (0.5613)  time: 25.7663\n",
      "Epoch: [8]  [ 100/5008]  eta: 0:34:25  lr: 0.000500  loss_labels: 0.4854 (0.4771)  loss: 0.4930 (0.4771)  time: 0.1680\n",
      "Epoch: [8]  [ 200/5008]  eta: 0:23:38  lr: 0.000500  loss_labels: 0.4706 (0.4779)  loss: 0.4304 (0.4779)  time: 0.1693\n",
      "Epoch: [8]  [ 300/5008]  eta: 0:19:50  lr: 0.000500  loss_labels: 0.4885 (0.4826)  loss: 0.5369 (0.4826)  time: 0.1683\n",
      "Epoch: [8]  [ 400/5008]  eta: 0:17:47  lr: 0.000500  loss_labels: 0.4594 (0.4776)  loss: 0.5114 (0.4776)  time: 0.1680\n",
      "Epoch: [8]  [ 500/5008]  eta: 0:16:26  lr: 0.000500  loss_labels: 0.4696 (0.4762)  loss: 0.4694 (0.4762)  time: 0.1671\n",
      "Epoch: [8]  [ 600/5008]  eta: 0:15:27  lr: 0.000500  loss_labels: 0.4623 (0.4743)  loss: 0.4574 (0.4743)  time: 0.1684\n",
      "Epoch: [8]  [ 700/5008]  eta: 0:14:40  lr: 0.000500  loss_labels: 0.4539 (0.4717)  loss: 0.4669 (0.4717)  time: 0.1673\n",
      "Epoch: [8]  [ 800/5008]  eta: 0:14:00  lr: 0.000500  loss_labels: 0.4626 (0.4718)  loss: 0.4659 (0.4718)  time: 0.1683\n",
      "Epoch: [8]  [ 900/5008]  eta: 0:13:26  lr: 0.000500  loss_labels: 0.4576 (0.4703)  loss: 0.4130 (0.4703)  time: 0.1679\n",
      "Epoch: [8]  [1000/5008]  eta: 0:12:56  lr: 0.000500  loss_labels: 0.4356 (0.4691)  loss: 0.4269 (0.4691)  time: 0.1704\n",
      "Epoch: [8]  [1100/5008]  eta: 0:12:27  lr: 0.000500  loss_labels: 0.4518 (0.4682)  loss: 0.4518 (0.4682)  time: 0.1674\n",
      "Epoch: [8]  [1200/5008]  eta: 0:12:01  lr: 0.000500  loss_labels: 0.4488 (0.4672)  loss: 0.4703 (0.4672)  time: 0.1672\n",
      "Epoch: [8]  [1300/5008]  eta: 0:11:35  lr: 0.000500  loss_labels: 0.4470 (0.4659)  loss: 0.4645 (0.4659)  time: 0.1677\n",
      "Epoch: [8]  [1400/5008]  eta: 0:11:12  lr: 0.000500  loss_labels: 0.4441 (0.4651)  loss: 0.4760 (0.4651)  time: 0.1680\n",
      "Epoch: [8]  [1500/5008]  eta: 0:10:49  lr: 0.000500  loss_labels: 0.4651 (0.4656)  loss: 0.4100 (0.4656)  time: 0.1689\n",
      "Epoch: [8]  [1600/5008]  eta: 0:10:27  lr: 0.000500  loss_labels: 0.4541 (0.4655)  loss: 0.4521 (0.4655)  time: 0.1722\n",
      "Epoch: [8]  [1700/5008]  eta: 0:10:06  lr: 0.000500  loss_labels: 0.4515 (0.4655)  loss: 0.4376 (0.4655)  time: 0.1685\n",
      "Epoch: [8]  [1800/5008]  eta: 0:09:45  lr: 0.000500  loss_labels: 0.4529 (0.4649)  loss: 0.4294 (0.4649)  time: 0.1670\n",
      "Epoch: [8]  [1900/5008]  eta: 0:09:24  lr: 0.000500  loss_labels: 0.4462 (0.4640)  loss: 0.4577 (0.4640)  time: 0.1689\n",
      "Epoch: [8]  [2000/5008]  eta: 0:09:04  lr: 0.000500  loss_labels: 0.4446 (0.4635)  loss: 0.4028 (0.4635)  time: 0.1680\n",
      "Epoch: [8]  [2100/5008]  eta: 0:08:44  lr: 0.000500  loss_labels: 0.4415 (0.4623)  loss: 0.4565 (0.4623)  time: 0.1673\n",
      "Epoch: [8]  [2200/5008]  eta: 0:08:25  lr: 0.000500  loss_labels: 0.4247 (0.4610)  loss: 0.4363 (0.4610)  time: 0.1684\n",
      "Epoch: [8]  [2300/5008]  eta: 0:08:05  lr: 0.000500  loss_labels: 0.4606 (0.4612)  loss: 0.4657 (0.4612)  time: 0.1691\n",
      "Epoch: [8]  [2400/5008]  eta: 0:07:46  lr: 0.000500  loss_labels: 0.4412 (0.4604)  loss: 0.4445 (0.4604)  time: 0.1698\n",
      "Epoch: [8]  [2500/5008]  eta: 0:07:27  lr: 0.000500  loss_labels: 0.4363 (0.4592)  loss: 0.4426 (0.4592)  time: 0.1680\n",
      "Epoch: [8]  [2600/5008]  eta: 0:07:09  lr: 0.000500  loss_labels: 0.4532 (0.4591)  loss: 0.3857 (0.4591)  time: 0.1682\n",
      "Epoch: [8]  [2700/5008]  eta: 0:06:50  lr: 0.000500  loss_labels: 0.4518 (0.4589)  loss: 0.4409 (0.4589)  time: 0.1677\n",
      "Epoch: [8]  [2800/5008]  eta: 0:06:31  lr: 0.000500  loss_labels: 0.4344 (0.4584)  loss: 0.4456 (0.4584)  time: 0.1671\n",
      "Epoch: [8]  [2900/5008]  eta: 0:06:13  lr: 0.000500  loss_labels: 0.4276 (0.4578)  loss: 0.4098 (0.4578)  time: 0.1674\n",
      "Epoch: [8]  [3000/5008]  eta: 0:05:54  lr: 0.000500  loss_labels: 0.4341 (0.4570)  loss: 0.4068 (0.4570)  time: 0.1678\n",
      "Epoch: [8]  [3100/5008]  eta: 0:05:36  lr: 0.000500  loss_labels: 0.4218 (0.4565)  loss: 0.4188 (0.4565)  time: 0.1681\n",
      "Epoch: [8]  [3200/5008]  eta: 0:05:18  lr: 0.000500  loss_labels: 0.4237 (0.4557)  loss: 0.4657 (0.4557)  time: 0.1683\n",
      "Epoch: [8]  [3300/5008]  eta: 0:05:00  lr: 0.000500  loss_labels: 0.4274 (0.4550)  loss: 0.4204 (0.4550)  time: 0.1683\n",
      "Epoch: [8]  [3400/5008]  eta: 0:04:42  lr: 0.000500  loss_labels: 0.4296 (0.4543)  loss: 0.4039 (0.4543)  time: 0.1672\n",
      "Epoch: [8]  [3500/5008]  eta: 0:04:24  lr: 0.000500  loss_labels: 0.4508 (0.4541)  loss: 0.5094 (0.4541)  time: 0.1682\n",
      "Epoch: [8]  [3600/5008]  eta: 0:04:06  lr: 0.000500  loss_labels: 0.4254 (0.4533)  loss: 0.4045 (0.4533)  time: 0.1663\n",
      "Epoch: [8]  [3700/5008]  eta: 0:03:49  lr: 0.000500  loss_labels: 0.4271 (0.4527)  loss: 0.4205 (0.4527)  time: 0.1662\n",
      "Epoch: [8]  [3800/5008]  eta: 0:03:31  lr: 0.000500  loss_labels: 0.4122 (0.4519)  loss: 0.3882 (0.4519)  time: 0.1687\n",
      "Epoch: [8]  [3900/5008]  eta: 0:03:13  lr: 0.000500  loss_labels: 0.4376 (0.4515)  loss: 0.4301 (0.4515)  time: 0.1681\n",
      "Epoch: [8]  [4000/5008]  eta: 0:02:55  lr: 0.000500  loss_labels: 0.4351 (0.4512)  loss: 0.4250 (0.4512)  time: 0.1694\n",
      "Epoch: [8]  [4100/5008]  eta: 0:02:38  lr: 0.000500  loss_labels: 0.4171 (0.4505)  loss: 0.4196 (0.4505)  time: 0.1961\n",
      "Epoch: [8]  [4200/5008]  eta: 0:02:21  lr: 0.000500  loss_labels: 0.4179 (0.4500)  loss: 0.4131 (0.4500)  time: 0.2039\n",
      "Epoch: [8]  [4300/5008]  eta: 0:02:03  lr: 0.000500  loss_labels: 0.4070 (0.4493)  loss: 0.4303 (0.4493)  time: 0.1685\n",
      "Epoch: [8]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 0.4447 (0.4492)  loss: 0.4120 (0.4492)  time: 0.1697\n",
      "Epoch: [8]  [4500/5008]  eta: 0:01:28  lr: 0.000500  loss_labels: 0.4008 (0.4484)  loss: 0.4031 (0.4484)  time: 0.1689\n",
      "Epoch: [8]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.4130 (0.4477)  loss: 0.4126 (0.4477)  time: 0.1708\n",
      "Epoch: [8]  [4700/5008]  eta: 0:00:53  lr: 0.000500  loss_labels: 0.3975 (0.4468)  loss: 0.4022 (0.4468)  time: 0.1706\n",
      "Epoch: [8]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.4025 (0.4461)  loss: 0.4128 (0.4461)  time: 0.1688\n",
      "Epoch: [8]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.4201 (0.4454)  loss: 0.3829 (0.4454)  time: 0.1685\n",
      "Epoch: [8]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.4191 (0.4451)  loss: 0.3864 (0.4451)  time: 0.1688\n",
      "Epoch: [8]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.4180 (0.4450)  loss: 0.3867 (0.4450)  time: 0.1691\n",
      "Epoch: [8] Total time: 0:14:31 (0.1740 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.4180 (0.4450)  loss: 0.3867 (0.4450)\n",
      "Test:  [  0/565]  eta: 3:22:58  loss_labels: 1.2079 (1.2079)  loss: 1.2079 (1.2079)  time: 21.5548\n",
      "Test:  [100/565]  eta: 0:02:40  loss_labels: 1.2890 (1.4052)  loss: 1.4336 (1.4052)  time: 0.1267\n",
      "Test:  [200/565]  eta: 0:01:29  loss_labels: 1.1551 (1.3410)  loss: 1.3754 (1.3410)  time: 0.1399\n",
      "Test:  [300/565]  eta: 0:00:56  loss_labels: 1.0994 (1.2805)  loss: 0.8960 (1.2805)  time: 0.1566\n",
      "Test:  [400/565]  eta: 0:00:32  loss_labels: 1.3317 (1.2976)  loss: 1.2901 (1.2976)  time: 0.1567\n",
      "Test:  [500/565]  eta: 0:00:12  loss_labels: 1.1130 (1.2712)  loss: 1.3103 (1.2712)  time: 0.1645\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1362 (1.2535)  loss: 1.0034 (1.2535)  time: 0.1665\n",
      "Test: Total time: 0:01:45 (0.1862 s / it)\n",
      "Averaged stats: loss_labels: 1.1362 (1.2535)  loss: 1.0034 (1.2535)\n",
      "acc: 0.7489336729049683\n",
      "top 1 and top 5 accuracies {'top1': 0.7489336952307096, 'top5': 0.8737744419210104, 'loss': tensor(0.0098, device='cuda:0')}\n",
      "Epoch: [9]  [   0/5008]  eta: 2 days, 2:27:10  lr: 0.000500  loss_labels: 0.5392 (0.5392)  loss: 0.5392 (0.5392)  time: 36.2682\n",
      "Epoch: [9]  [ 100/5008]  eta: 0:43:00  lr: 0.000500  loss_labels: 0.3992 (0.4031)  loss: 0.4069 (0.4031)  time: 0.1678\n",
      "Epoch: [9]  [ 200/5008]  eta: 0:27:48  lr: 0.000500  loss_labels: 0.4019 (0.4030)  loss: 0.4202 (0.4030)  time: 0.1677\n",
      "Epoch: [9]  [ 300/5008]  eta: 0:23:26  lr: 0.000500  loss_labels: 0.4165 (0.4101)  loss: 0.4884 (0.4101)  time: 0.1979\n",
      "Epoch: [9]  [ 400/5008]  eta: 0:22:01  lr: 0.000500  loss_labels: 0.3868 (0.4054)  loss: 0.4175 (0.4054)  time: 0.2265\n",
      "Epoch: [9]  [ 500/5008]  eta: 0:20:35  lr: 0.000500  loss_labels: 0.3941 (0.4029)  loss: 0.3926 (0.4029)  time: 0.2707\n",
      "Epoch: [9]  [ 600/5008]  eta: 0:19:45  lr: 0.000500  loss_labels: 0.3746 (0.4015)  loss: 0.3590 (0.4015)  time: 0.1636\n",
      "Epoch: [9]  [ 700/5008]  eta: 0:18:36  lr: 0.000500  loss_labels: 0.3836 (0.4001)  loss: 0.4221 (0.4001)  time: 0.2270\n",
      "Epoch: [9]  [ 800/5008]  eta: 0:17:57  lr: 0.000500  loss_labels: 0.3944 (0.4004)  loss: 0.3914 (0.4004)  time: 0.3461\n",
      "Epoch: [9]  [ 900/5008]  eta: 0:17:11  lr: 0.000500  loss_labels: 0.3848 (0.3999)  loss: 0.3848 (0.3999)  time: 0.2090\n",
      "Epoch: [9]  [1000/5008]  eta: 0:16:21  lr: 0.000500  loss_labels: 0.3882 (0.3995)  loss: 0.3925 (0.3995)  time: 0.1930\n",
      "Epoch: [9]  [1100/5008]  eta: 0:15:39  lr: 0.000500  loss_labels: 0.3759 (0.3982)  loss: 0.3662 (0.3982)  time: 0.1989\n",
      "Epoch: [9]  [1200/5008]  eta: 0:15:01  lr: 0.000500  loss_labels: 0.3748 (0.3980)  loss: 0.3944 (0.3980)  time: 0.1692\n",
      "Epoch: [9]  [1300/5008]  eta: 0:14:18  lr: 0.000500  loss_labels: 0.3826 (0.3978)  loss: 0.3828 (0.3978)  time: 0.1696\n",
      "Epoch: [9]  [1400/5008]  eta: 0:13:39  lr: 0.000500  loss_labels: 0.3828 (0.3974)  loss: 0.3783 (0.3974)  time: 0.1698\n",
      "Epoch: [9]  [1500/5008]  eta: 0:13:03  lr: 0.000500  loss_labels: 0.3993 (0.3981)  loss: 0.3677 (0.3981)  time: 0.1717\n",
      "Epoch: [9]  [1600/5008]  eta: 0:12:30  lr: 0.000500  loss_labels: 0.3852 (0.3980)  loss: 0.3974 (0.3980)  time: 0.1675\n",
      "Epoch: [9]  [1700/5008]  eta: 0:11:57  lr: 0.000500  loss_labels: 0.3952 (0.3982)  loss: 0.3702 (0.3982)  time: 0.1690\n",
      "Epoch: [9]  [1800/5008]  eta: 0:11:27  lr: 0.000500  loss_labels: 0.3878 (0.3979)  loss: 0.3876 (0.3979)  time: 0.1676\n",
      "Epoch: [9]  [1900/5008]  eta: 0:10:58  lr: 0.000500  loss_labels: 0.3866 (0.3977)  loss: 0.4145 (0.3977)  time: 0.1671\n",
      "Epoch: [9]  [2000/5008]  eta: 0:10:30  lr: 0.000500  loss_labels: 0.3873 (0.3977)  loss: 0.3476 (0.3977)  time: 0.1669\n",
      "Epoch: [9]  [2100/5008]  eta: 0:10:03  lr: 0.000500  loss_labels: 0.3801 (0.3968)  loss: 0.4137 (0.3968)  time: 0.1672\n",
      "Epoch: [9]  [2200/5008]  eta: 0:09:37  lr: 0.000500  loss_labels: 0.3692 (0.3956)  loss: 0.3547 (0.3956)  time: 0.1673\n",
      "Epoch: [9]  [2300/5008]  eta: 0:09:12  lr: 0.000500  loss_labels: 0.3949 (0.3957)  loss: 0.3711 (0.3957)  time: 0.1668\n",
      "Epoch: [9]  [2400/5008]  eta: 0:08:48  lr: 0.000500  loss_labels: 0.3828 (0.3957)  loss: 0.3921 (0.3957)  time: 0.1701\n",
      "Epoch: [9]  [2500/5008]  eta: 0:08:24  lr: 0.000500  loss_labels: 0.3697 (0.3947)  loss: 0.3697 (0.3947)  time: 0.1666\n",
      "Epoch: [9]  [2600/5008]  eta: 0:08:01  lr: 0.000500  loss_labels: 0.3764 (0.3944)  loss: 0.3387 (0.3944)  time: 0.1686\n",
      "Epoch: [9]  [2700/5008]  eta: 0:07:38  lr: 0.000500  loss_labels: 0.3823 (0.3941)  loss: 0.3954 (0.3941)  time: 0.1664\n",
      "Epoch: [9]  [2800/5008]  eta: 0:07:16  lr: 0.000500  loss_labels: 0.3726 (0.3937)  loss: 0.3700 (0.3937)  time: 0.1673\n",
      "Epoch: [9]  [2900/5008]  eta: 0:06:54  lr: 0.000500  loss_labels: 0.3765 (0.3932)  loss: 0.3550 (0.3932)  time: 0.1667\n",
      "Epoch: [9]  [3000/5008]  eta: 0:06:32  lr: 0.000500  loss_labels: 0.3783 (0.3928)  loss: 0.3551 (0.3928)  time: 0.1670\n",
      "Epoch: [9]  [3100/5008]  eta: 0:06:11  lr: 0.000500  loss_labels: 0.3736 (0.3926)  loss: 0.3388 (0.3926)  time: 0.1660\n",
      "Epoch: [9]  [3200/5008]  eta: 0:05:50  lr: 0.000500  loss_labels: 0.3569 (0.3918)  loss: 0.3713 (0.3918)  time: 0.1681\n",
      "Epoch: [9]  [3300/5008]  eta: 0:05:29  lr: 0.000500  loss_labels: 0.3856 (0.3914)  loss: 0.3575 (0.3914)  time: 0.1664\n",
      "Epoch: [9]  [3400/5008]  eta: 0:05:09  lr: 0.000500  loss_labels: 0.3706 (0.3908)  loss: 0.3729 (0.3908)  time: 0.1700\n",
      "Epoch: [9]  [3500/5008]  eta: 0:04:48  lr: 0.000500  loss_labels: 0.3794 (0.3907)  loss: 0.4321 (0.3907)  time: 0.1694\n",
      "Epoch: [9]  [3600/5008]  eta: 0:04:28  lr: 0.000500  loss_labels: 0.3493 (0.3899)  loss: 0.3350 (0.3899)  time: 0.1682\n",
      "Epoch: [9]  [3700/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.3641 (0.3894)  loss: 0.3569 (0.3894)  time: 0.1669\n",
      "Epoch: [9]  [3800/5008]  eta: 0:03:50  lr: 0.000500  loss_labels: 0.3541 (0.3885)  loss: 0.3434 (0.3885)  time: 0.1673\n",
      "Epoch: [9]  [3900/5008]  eta: 0:03:31  lr: 0.000500  loss_labels: 0.3751 (0.3884)  loss: 0.3550 (0.3884)  time: 0.1751\n",
      "Epoch: [9]  [4000/5008]  eta: 0:03:12  lr: 0.000500  loss_labels: 0.3629 (0.3879)  loss: 0.3683 (0.3879)  time: 0.1682\n",
      "Epoch: [9]  [4100/5008]  eta: 0:02:52  lr: 0.000500  loss_labels: 0.3593 (0.3875)  loss: 0.3946 (0.3875)  time: 0.1681\n",
      "Epoch: [9]  [4200/5008]  eta: 0:02:33  lr: 0.000500  loss_labels: 0.3656 (0.3870)  loss: 0.3504 (0.3870)  time: 0.1674\n",
      "Epoch: [9]  [4300/5008]  eta: 0:02:14  lr: 0.000500  loss_labels: 0.3474 (0.3863)  loss: 0.3697 (0.3863)  time: 0.1671\n",
      "Epoch: [9]  [4400/5008]  eta: 0:01:55  lr: 0.000500  loss_labels: 0.3702 (0.3863)  loss: 0.3715 (0.3863)  time: 0.2095\n",
      "Epoch: [9]  [4500/5008]  eta: 0:01:36  lr: 0.000500  loss_labels: 0.3529 (0.3857)  loss: 0.3869 (0.3857)  time: 0.1828\n",
      "Epoch: [9]  [4600/5008]  eta: 0:01:17  lr: 0.000500  loss_labels: 0.3626 (0.3854)  loss: 0.3633 (0.3854)  time: 0.1665\n",
      "Epoch: [9]  [4700/5008]  eta: 0:00:58  lr: 0.000500  loss_labels: 0.3446 (0.3847)  loss: 0.3412 (0.3847)  time: 0.1681\n",
      "Epoch: [9]  [4800/5008]  eta: 0:00:39  lr: 0.000500  loss_labels: 0.3556 (0.3842)  loss: 0.3425 (0.3842)  time: 0.1669\n",
      "Epoch: [9]  [4900/5008]  eta: 0:00:20  lr: 0.000500  loss_labels: 0.3434 (0.3835)  loss: 0.3160 (0.3835)  time: 0.1678\n",
      "Epoch: [9]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.3601 (0.3830)  loss: 0.3505 (0.3830)  time: 0.1699\n",
      "Epoch: [9]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.3555 (0.3829)  loss: 0.3499 (0.3829)  time: 0.1687\n",
      "Epoch: [9] Total time: 0:15:40 (0.1878 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.3555 (0.3829)  loss: 0.3499 (0.3829)\n",
      "Test:  [  0/565]  eta: 4:01:41  loss_labels: 1.1279 (1.1279)  loss: 1.1279 (1.1279)  time: 25.6660\n",
      "Test:  [100/565]  eta: 0:03:10  loss_labels: 1.3338 (1.4231)  loss: 1.3830 (1.4231)  time: 0.1554\n",
      "Test:  [200/565]  eta: 0:01:44  loss_labels: 1.1870 (1.3464)  loss: 1.3185 (1.3464)  time: 0.1497\n",
      "Test:  [300/565]  eta: 0:01:03  loss_labels: 1.0027 (1.2802)  loss: 0.8875 (1.2802)  time: 0.1466\n",
      "Test:  [400/565]  eta: 0:00:35  loss_labels: 1.3239 (1.2964)  loss: 1.2821 (1.2964)  time: 0.1436\n",
      "Test:  [500/565]  eta: 0:00:13  loss_labels: 1.1036 (1.2710)  loss: 1.2521 (1.2710)  time: 0.1496\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1366 (1.2537)  loss: 1.1476 (1.2537)  time: 0.1431\n",
      "Test: Total time: 0:01:50 (0.1961 s / it)\n",
      "Averaged stats: loss_labels: 1.1366 (1.2537)  loss: 1.1476 (1.2537)\n",
      "acc: 0.7528111338615417\n",
      "top 1 and top 5 accuracies {'top1': 0.7528111671190384, 'top5': 0.8764886722428405, 'loss': tensor(0.0098, device='cuda:0')}\n",
      "Epoch: [10]  [   0/5008]  eta: 1 day, 15:07:09  lr: 0.000500  loss_labels: 0.4312 (0.4312)  loss: 0.4312 (0.4312)  time: 28.1209\n",
      "Epoch: [10]  [ 100/5008]  eta: 0:36:22  lr: 0.000500  loss_labels: 0.3400 (0.3464)  loss: 0.3347 (0.3464)  time: 0.1651\n",
      "Epoch: [10]  [ 200/5008]  eta: 0:24:47  lr: 0.000500  loss_labels: 0.3455 (0.3484)  loss: 0.3521 (0.3484)  time: 0.1857\n",
      "Epoch: [10]  [ 300/5008]  eta: 0:20:47  lr: 0.000500  loss_labels: 0.3580 (0.3540)  loss: 0.3960 (0.3540)  time: 0.1653\n",
      "Epoch: [10]  [ 400/5008]  eta: 0:18:29  lr: 0.000500  loss_labels: 0.3375 (0.3492)  loss: 0.3475 (0.3492)  time: 0.1678\n",
      "Epoch: [10]  [ 500/5008]  eta: 0:17:00  lr: 0.000500  loss_labels: 0.3395 (0.3482)  loss: 0.3249 (0.3482)  time: 0.1699\n",
      "Epoch: [10]  [ 600/5008]  eta: 0:15:54  lr: 0.000500  loss_labels: 0.3362 (0.3475)  loss: 0.3362 (0.3475)  time: 0.1680\n",
      "Epoch: [10]  [ 700/5008]  eta: 0:15:04  lr: 0.000500  loss_labels: 0.3348 (0.3468)  loss: 0.3729 (0.3468)  time: 0.1738\n",
      "Epoch: [10]  [ 800/5008]  eta: 0:14:22  lr: 0.000500  loss_labels: 0.3488 (0.3476)  loss: 0.3488 (0.3476)  time: 0.1671\n",
      "Epoch: [10]  [ 900/5008]  eta: 0:13:50  lr: 0.000500  loss_labels: 0.3197 (0.3457)  loss: 0.3038 (0.3457)  time: 0.1831\n",
      "Epoch: [10]  [1000/5008]  eta: 0:13:23  lr: 0.000500  loss_labels: 0.3281 (0.3449)  loss: 0.3255 (0.3449)  time: 0.1716\n",
      "Epoch: [10]  [1100/5008]  eta: 0:12:51  lr: 0.000500  loss_labels: 0.3260 (0.3442)  loss: 0.3223 (0.3442)  time: 0.1671\n",
      "Epoch: [10]  [1200/5008]  eta: 0:12:22  lr: 0.000500  loss_labels: 0.3346 (0.3443)  loss: 0.3483 (0.3443)  time: 0.1700\n",
      "Epoch: [10]  [1300/5008]  eta: 0:11:55  lr: 0.000500  loss_labels: 0.3286 (0.3443)  loss: 0.3340 (0.3443)  time: 0.1696\n",
      "Epoch: [10]  [1400/5008]  eta: 0:11:30  lr: 0.000500  loss_labels: 0.3295 (0.3435)  loss: 0.3471 (0.3435)  time: 0.1731\n",
      "Epoch: [10]  [1500/5008]  eta: 0:11:06  lr: 0.000500  loss_labels: 0.3360 (0.3439)  loss: 0.3360 (0.3439)  time: 0.1691\n",
      "Epoch: [10]  [1600/5008]  eta: 0:10:43  lr: 0.000500  loss_labels: 0.3290 (0.3440)  loss: 0.3352 (0.3440)  time: 0.1704\n",
      "Epoch: [10]  [1700/5008]  eta: 0:10:20  lr: 0.000500  loss_labels: 0.3313 (0.3439)  loss: 0.3153 (0.3439)  time: 0.1696\n",
      "Epoch: [10]  [1800/5008]  eta: 0:09:58  lr: 0.000500  loss_labels: 0.3347 (0.3436)  loss: 0.3069 (0.3436)  time: 0.1691\n",
      "Epoch: [10]  [1900/5008]  eta: 0:09:37  lr: 0.000500  loss_labels: 0.3287 (0.3430)  loss: 0.3210 (0.3430)  time: 0.1702\n",
      "Epoch: [10]  [2000/5008]  eta: 0:09:16  lr: 0.000500  loss_labels: 0.3484 (0.3435)  loss: 0.2846 (0.3435)  time: 0.1692\n",
      "Epoch: [10]  [2100/5008]  eta: 0:08:55  lr: 0.000500  loss_labels: 0.3238 (0.3427)  loss: 0.3490 (0.3427)  time: 0.1691\n",
      "Epoch: [10]  [2200/5008]  eta: 0:08:35  lr: 0.000500  loss_labels: 0.3185 (0.3419)  loss: 0.3014 (0.3419)  time: 0.1695\n",
      "Epoch: [10]  [2300/5008]  eta: 0:08:15  lr: 0.000500  loss_labels: 0.3568 (0.3423)  loss: 0.3634 (0.3423)  time: 0.1686\n",
      "Epoch: [10]  [2400/5008]  eta: 0:07:55  lr: 0.000500  loss_labels: 0.3141 (0.3417)  loss: 0.3226 (0.3417)  time: 0.1674\n",
      "Epoch: [10]  [2500/5008]  eta: 0:07:36  lr: 0.000500  loss_labels: 0.3158 (0.3410)  loss: 0.3126 (0.3410)  time: 0.1992\n",
      "Epoch: [10]  [2600/5008]  eta: 0:07:23  lr: 0.000500  loss_labels: 0.3179 (0.3406)  loss: 0.2652 (0.3406)  time: 0.3215\n",
      "Epoch: [10]  [2700/5008]  eta: 0:07:15  lr: 0.000500  loss_labels: 0.3404 (0.3404)  loss: 0.3381 (0.3404)  time: 0.1976\n",
      "Epoch: [10]  [2800/5008]  eta: 0:06:55  lr: 0.000500  loss_labels: 0.3302 (0.3405)  loss: 0.3395 (0.3405)  time: 0.1671\n",
      "Epoch: [10]  [2900/5008]  eta: 0:06:40  lr: 0.000500  loss_labels: 0.3259 (0.3402)  loss: 0.3191 (0.3402)  time: 0.2942\n",
      "Epoch: [10]  [3000/5008]  eta: 0:06:25  lr: 0.000500  loss_labels: 0.3333 (0.3401)  loss: 0.3022 (0.3401)  time: 0.2717\n",
      "Epoch: [10]  [3100/5008]  eta: 0:06:05  lr: 0.000500  loss_labels: 0.3294 (0.3400)  loss: 0.3055 (0.3400)  time: 0.1699\n",
      "Epoch: [10]  [3200/5008]  eta: 0:05:50  lr: 0.000500  loss_labels: 0.3148 (0.3393)  loss: 0.3256 (0.3393)  time: 0.2757\n",
      "Epoch: [10]  [3300/5008]  eta: 0:05:30  lr: 0.000500  loss_labels: 0.3297 (0.3391)  loss: 0.3255 (0.3391)  time: 0.1706\n",
      "Epoch: [10]  [3400/5008]  eta: 0:05:09  lr: 0.000500  loss_labels: 0.3168 (0.3385)  loss: 0.3311 (0.3385)  time: 0.1686\n",
      "Epoch: [10]  [3500/5008]  eta: 0:04:49  lr: 0.000500  loss_labels: 0.3308 (0.3386)  loss: 0.3775 (0.3386)  time: 0.1693\n",
      "Epoch: [10]  [3600/5008]  eta: 0:04:29  lr: 0.000500  loss_labels: 0.3143 (0.3382)  loss: 0.3155 (0.3382)  time: 0.2044\n",
      "Epoch: [10]  [3700/5008]  eta: 0:04:10  lr: 0.000500  loss_labels: 0.3132 (0.3377)  loss: 0.3106 (0.3377)  time: 0.1695\n",
      "Epoch: [10]  [3800/5008]  eta: 0:03:50  lr: 0.000500  loss_labels: 0.3112 (0.3369)  loss: 0.3122 (0.3369)  time: 0.1695\n",
      "Epoch: [10]  [3900/5008]  eta: 0:03:30  lr: 0.000500  loss_labels: 0.3416 (0.3371)  loss: 0.3217 (0.3371)  time: 0.1682\n",
      "Epoch: [10]  [4000/5008]  eta: 0:03:11  lr: 0.000500  loss_labels: 0.3136 (0.3367)  loss: 0.2951 (0.3367)  time: 0.1676\n",
      "Epoch: [10]  [4100/5008]  eta: 0:02:52  lr: 0.000500  loss_labels: 0.3216 (0.3363)  loss: 0.3127 (0.3363)  time: 0.1946\n",
      "Epoch: [10]  [4200/5008]  eta: 0:02:33  lr: 0.000500  loss_labels: 0.3163 (0.3358)  loss: 0.3106 (0.3358)  time: 0.2031\n",
      "Epoch: [10]  [4300/5008]  eta: 0:02:15  lr: 0.000500  loss_labels: 0.3036 (0.3354)  loss: 0.3066 (0.3354)  time: 0.2302\n",
      "Epoch: [10]  [4400/5008]  eta: 0:01:56  lr: 0.000500  loss_labels: 0.3343 (0.3353)  loss: 0.3343 (0.3353)  time: 0.1840\n",
      "Epoch: [10]  [4500/5008]  eta: 0:01:36  lr: 0.000500  loss_labels: 0.3057 (0.3349)  loss: 0.2988 (0.3349)  time: 0.1677\n",
      "Epoch: [10]  [4600/5008]  eta: 0:01:17  lr: 0.000500  loss_labels: 0.3260 (0.3347)  loss: 0.3292 (0.3347)  time: 0.1682\n",
      "Epoch: [10]  [4700/5008]  eta: 0:00:58  lr: 0.000500  loss_labels: 0.3076 (0.3343)  loss: 0.3238 (0.3343)  time: 0.1696\n",
      "Epoch: [10]  [4800/5008]  eta: 0:00:39  lr: 0.000500  loss_labels: 0.2929 (0.3337)  loss: 0.2892 (0.3337)  time: 0.1700\n",
      "Epoch: [10]  [4900/5008]  eta: 0:00:20  lr: 0.000500  loss_labels: 0.3106 (0.3332)  loss: 0.2762 (0.3332)  time: 0.1704\n",
      "Epoch: [10]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.3076 (0.3328)  loss: 0.3014 (0.3328)  time: 0.1702\n",
      "Epoch: [10]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.3014 (0.3327)  loss: 0.2741 (0.3327)  time: 0.1697\n",
      "Epoch: [10] Total time: 0:15:43 (0.1885 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.3014 (0.3327)  loss: 0.2741 (0.3327)\n",
      "Test:  [  0/565]  eta: 3:23:21  loss_labels: 1.1114 (1.1114)  loss: 1.1114 (1.1114)  time: 21.5962\n",
      "Test:  [100/565]  eta: 0:02:44  loss_labels: 1.3643 (1.4575)  loss: 1.4699 (1.4575)  time: 0.1475\n",
      "Test:  [200/565]  eta: 0:01:30  loss_labels: 1.1888 (1.3789)  loss: 1.3862 (1.3789)  time: 0.1434\n",
      "Test:  [300/565]  eta: 0:00:56  loss_labels: 1.0448 (1.3038)  loss: 1.0084 (1.3038)  time: 0.1485\n",
      "Test:  [400/565]  eta: 0:00:32  loss_labels: 1.4076 (1.3293)  loss: 1.3435 (1.3293)  time: 0.1288\n",
      "Test:  [500/565]  eta: 0:00:11  loss_labels: 1.1356 (1.3068)  loss: 1.3262 (1.3068)  time: 0.1381\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1836 (1.2902)  loss: 1.1855 (1.2902)  time: 0.1292\n",
      "Test: Total time: 0:01:40 (0.1787 s / it)\n",
      "Averaged stats: loss_labels: 1.1836 (1.2902)  loss: 1.1855 (1.2902)\n",
      "acc: 0.7516894340515137\n",
      "top 1 and top 5 accuracies {'top1': 0.7516894698942004, 'top5': 0.8763917354456323, 'loss': tensor(0.0101, device='cuda:0')}\n",
      "Epoch: [11]  [   0/5008]  eta: 1 day, 11:13:25  lr: 0.000500  loss_labels: 0.4348 (0.4348)  loss: 0.4348 (0.4348)  time: 25.3205\n",
      "Epoch: [11]  [ 100/5008]  eta: 0:34:08  lr: 0.000500  loss_labels: 0.3133 (0.3164)  loss: 0.2983 (0.3164)  time: 0.1699\n",
      "Epoch: [11]  [ 200/5008]  eta: 0:29:26  lr: 0.000500  loss_labels: 0.3009 (0.3108)  loss: 0.2876 (0.3108)  time: 0.1708\n",
      "Epoch: [11]  [ 300/5008]  eta: 0:23:40  lr: 0.000500  loss_labels: 0.3233 (0.3163)  loss: 0.3367 (0.3163)  time: 0.1702\n",
      "Epoch: [11]  [ 400/5008]  eta: 0:20:38  lr: 0.000500  loss_labels: 0.2890 (0.3103)  loss: 0.3310 (0.3103)  time: 0.1700\n",
      "Epoch: [11]  [ 500/5008]  eta: 0:18:42  lr: 0.000500  loss_labels: 0.2844 (0.3058)  loss: 0.2606 (0.3058)  time: 0.1702\n",
      "Epoch: [11]  [ 600/5008]  eta: 0:17:19  lr: 0.000500  loss_labels: 0.3018 (0.3055)  loss: 0.2618 (0.3055)  time: 0.1690\n",
      "Epoch: [11]  [ 700/5008]  eta: 0:16:15  lr: 0.000500  loss_labels: 0.2970 (0.3047)  loss: 0.3234 (0.3047)  time: 0.1703\n",
      "Epoch: [11]  [ 800/5008]  eta: 0:15:23  lr: 0.000500  loss_labels: 0.3021 (0.3052)  loss: 0.3136 (0.3052)  time: 0.1691\n",
      "Epoch: [11]  [ 900/5008]  eta: 0:14:38  lr: 0.000500  loss_labels: 0.2941 (0.3046)  loss: 0.2704 (0.3046)  time: 0.1699\n",
      "Epoch: [11]  [1000/5008]  eta: 0:13:59  lr: 0.000500  loss_labels: 0.2875 (0.3038)  loss: 0.2816 (0.3038)  time: 0.1696\n",
      "Epoch: [11]  [1100/5008]  eta: 0:13:24  lr: 0.000500  loss_labels: 0.2927 (0.3037)  loss: 0.2618 (0.3037)  time: 0.1678\n",
      "Epoch: [11]  [1200/5008]  eta: 0:12:52  lr: 0.000500  loss_labels: 0.3002 (0.3038)  loss: 0.3119 (0.3038)  time: 0.1681\n",
      "Epoch: [11]  [1300/5008]  eta: 0:12:22  lr: 0.000500  loss_labels: 0.2884 (0.3031)  loss: 0.2918 (0.3031)  time: 0.1687\n",
      "Epoch: [11]  [1400/5008]  eta: 0:11:54  lr: 0.000500  loss_labels: 0.2937 (0.3025)  loss: 0.2942 (0.3025)  time: 0.1683\n",
      "Epoch: [11]  [1500/5008]  eta: 0:11:27  lr: 0.000500  loss_labels: 0.2945 (0.3029)  loss: 0.2765 (0.3029)  time: 0.1688\n",
      "Epoch: [11]  [1600/5008]  eta: 0:11:01  lr: 0.000500  loss_labels: 0.3018 (0.3030)  loss: 0.3257 (0.3030)  time: 0.1683\n",
      "Epoch: [11]  [1700/5008]  eta: 0:10:37  lr: 0.000500  loss_labels: 0.2964 (0.3029)  loss: 0.2876 (0.3029)  time: 0.1685\n",
      "Epoch: [11]  [1800/5008]  eta: 0:10:14  lr: 0.000500  loss_labels: 0.2800 (0.3025)  loss: 0.2664 (0.3025)  time: 0.1686\n",
      "Epoch: [11]  [1900/5008]  eta: 0:09:51  lr: 0.000500  loss_labels: 0.2858 (0.3020)  loss: 0.3084 (0.3020)  time: 0.1686\n",
      "Epoch: [11]  [2000/5008]  eta: 0:09:29  lr: 0.000500  loss_labels: 0.2948 (0.3020)  loss: 0.2651 (0.3020)  time: 0.1703\n",
      "Epoch: [11]  [2100/5008]  eta: 0:09:07  lr: 0.000500  loss_labels: 0.2746 (0.3009)  loss: 0.3192 (0.3009)  time: 0.1695\n",
      "Epoch: [11]  [2200/5008]  eta: 0:08:46  lr: 0.000500  loss_labels: 0.2930 (0.3009)  loss: 0.2930 (0.3009)  time: 0.1705\n",
      "Epoch: [11]  [2300/5008]  eta: 0:08:25  lr: 0.000500  loss_labels: 0.3102 (0.3011)  loss: 0.3102 (0.3011)  time: 0.1689\n",
      "Epoch: [11]  [2400/5008]  eta: 0:08:05  lr: 0.000500  loss_labels: 0.2876 (0.3007)  loss: 0.2921 (0.3007)  time: 0.1698\n",
      "Epoch: [11]  [2500/5008]  eta: 0:07:45  lr: 0.000500  loss_labels: 0.2894 (0.3002)  loss: 0.2633 (0.3002)  time: 0.1902\n",
      "Epoch: [11]  [2600/5008]  eta: 0:07:25  lr: 0.000500  loss_labels: 0.2818 (0.3000)  loss: 0.2273 (0.3000)  time: 0.1709\n",
      "Epoch: [11]  [2700/5008]  eta: 0:07:07  lr: 0.000500  loss_labels: 0.2888 (0.2998)  loss: 0.2909 (0.2998)  time: 0.1877\n",
      "Epoch: [11]  [2800/5008]  eta: 0:06:48  lr: 0.000500  loss_labels: 0.2791 (0.2995)  loss: 0.2728 (0.2995)  time: 0.1809\n",
      "Epoch: [11]  [2900/5008]  eta: 0:06:30  lr: 0.000500  loss_labels: 0.2765 (0.2991)  loss: 0.2441 (0.2991)  time: 0.1955\n",
      "Epoch: [11]  [3000/5008]  eta: 0:06:11  lr: 0.000500  loss_labels: 0.2953 (0.2991)  loss: 0.2730 (0.2991)  time: 0.1829\n",
      "Epoch: [11]  [3100/5008]  eta: 0:05:52  lr: 0.000500  loss_labels: 0.3012 (0.2992)  loss: 0.2474 (0.2992)  time: 0.1687\n",
      "Epoch: [11]  [3200/5008]  eta: 0:05:33  lr: 0.000500  loss_labels: 0.2842 (0.2988)  loss: 0.3167 (0.2988)  time: 0.1701\n",
      "Epoch: [11]  [3300/5008]  eta: 0:05:14  lr: 0.000500  loss_labels: 0.2890 (0.2988)  loss: 0.2872 (0.2988)  time: 0.1704\n",
      "Epoch: [11]  [3400/5008]  eta: 0:04:55  lr: 0.000500  loss_labels: 0.2713 (0.2981)  loss: 0.2713 (0.2981)  time: 0.1693\n",
      "Epoch: [11]  [3500/5008]  eta: 0:04:36  lr: 0.000500  loss_labels: 0.2860 (0.2980)  loss: 0.3334 (0.2980)  time: 0.1708\n",
      "Epoch: [11]  [3600/5008]  eta: 0:04:17  lr: 0.000500  loss_labels: 0.2697 (0.2975)  loss: 0.2777 (0.2975)  time: 0.1717\n",
      "Epoch: [11]  [3700/5008]  eta: 0:04:00  lr: 0.000500  loss_labels: 0.2767 (0.2971)  loss: 0.2655 (0.2971)  time: 0.2291\n",
      "Epoch: [11]  [3800/5008]  eta: 0:03:42  lr: 0.000500  loss_labels: 0.2755 (0.2966)  loss: 0.2813 (0.2966)  time: 0.1681\n",
      "Epoch: [11]  [3900/5008]  eta: 0:03:23  lr: 0.000500  loss_labels: 0.2917 (0.2965)  loss: 0.2570 (0.2965)  time: 0.1710\n",
      "Epoch: [11]  [4000/5008]  eta: 0:03:04  lr: 0.000500  loss_labels: 0.2737 (0.2960)  loss: 0.2527 (0.2960)  time: 0.1749\n",
      "Epoch: [11]  [4100/5008]  eta: 0:02:46  lr: 0.000500  loss_labels: 0.2814 (0.2955)  loss: 0.2797 (0.2955)  time: 0.1697\n",
      "Epoch: [11]  [4200/5008]  eta: 0:02:27  lr: 0.000500  loss_labels: 0.2741 (0.2952)  loss: 0.2699 (0.2952)  time: 0.1926\n",
      "Epoch: [11]  [4300/5008]  eta: 0:02:09  lr: 0.000500  loss_labels: 0.2623 (0.2946)  loss: 0.2827 (0.2946)  time: 0.1696\n",
      "Epoch: [11]  [4400/5008]  eta: 0:01:51  lr: 0.000500  loss_labels: 0.2854 (0.2946)  loss: 0.2932 (0.2946)  time: 0.1693\n",
      "Epoch: [11]  [4500/5008]  eta: 0:01:32  lr: 0.000500  loss_labels: 0.2846 (0.2945)  loss: 0.2684 (0.2945)  time: 0.1707\n",
      "Epoch: [11]  [4600/5008]  eta: 0:01:14  lr: 0.000500  loss_labels: 0.2760 (0.2941)  loss: 0.2691 (0.2941)  time: 0.1697\n",
      "Epoch: [11]  [4700/5008]  eta: 0:00:55  lr: 0.000500  loss_labels: 0.2645 (0.2937)  loss: 0.2674 (0.2937)  time: 0.1698\n",
      "Epoch: [11]  [4800/5008]  eta: 0:00:37  lr: 0.000500  loss_labels: 0.2682 (0.2933)  loss: 0.2573 (0.2933)  time: 0.1688\n",
      "Epoch: [11]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.2603 (0.2928)  loss: 0.2302 (0.2928)  time: 0.2024\n",
      "Epoch: [11]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.2669 (0.2924)  loss: 0.2780 (0.2924)  time: 0.1728\n",
      "Epoch: [11]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.2651 (0.2923)  loss: 0.2748 (0.2923)  time: 0.1721\n",
      "Epoch: [11] Total time: 0:15:10 (0.1819 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.2651 (0.2923)  loss: 0.2748 (0.2923)\n",
      "Test:  [  0/565]  eta: 3:54:14  loss_labels: 1.0828 (1.0828)  loss: 1.0828 (1.0828)  time: 24.8755\n",
      "Test:  [100/565]  eta: 0:03:04  loss_labels: 1.3525 (1.4478)  loss: 1.4241 (1.4478)  time: 0.1647\n",
      "Test:  [200/565]  eta: 0:01:41  loss_labels: 1.1484 (1.3602)  loss: 1.4084 (1.3602)  time: 0.1543\n",
      "Test:  [300/565]  eta: 0:01:04  loss_labels: 1.0829 (1.2909)  loss: 0.9968 (1.2909)  time: 0.1589\n",
      "Test:  [400/565]  eta: 0:00:36  loss_labels: 1.3785 (1.3135)  loss: 1.3299 (1.3135)  time: 0.1542\n",
      "Test:  [500/565]  eta: 0:00:13  loss_labels: 1.1149 (1.2892)  loss: 1.2353 (1.2892)  time: 0.1533\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1785 (1.2736)  loss: 1.1923 (1.2736)  time: 0.1732\n",
      "Test: Total time: 0:01:56 (0.2060 s / it)\n",
      "Averaged stats: loss_labels: 1.1785 (1.2736)  loss: 1.1923 (1.2736)\n",
      "acc: 0.7599983215332031\n",
      "top 1 and top 5 accuracies {'top1': 0.7599983382263336, 'top5': 0.8795906497535035, 'loss': tensor(0.0100, device='cuda:0')}\n",
      "Epoch: [12]  [   0/5008]  eta: 1 day, 20:16:14  lr: 0.000500  loss_labels: 0.3019 (0.3019)  loss: 0.3019 (0.3019)  time: 31.8240\n",
      "Epoch: [12]  [ 100/5008]  eta: 0:39:57  lr: 0.000500  loss_labels: 0.2613 (0.2698)  loss: 0.2658 (0.2698)  time: 0.1703\n",
      "Epoch: [12]  [ 200/5008]  eta: 0:26:28  lr: 0.000500  loss_labels: 0.2703 (0.2714)  loss: 0.2686 (0.2714)  time: 0.1707\n",
      "Epoch: [12]  [ 300/5008]  eta: 0:21:42  lr: 0.000500  loss_labels: 0.2849 (0.2781)  loss: 0.3321 (0.2781)  time: 0.1684\n",
      "Epoch: [12]  [ 400/5008]  eta: 0:19:13  lr: 0.000500  loss_labels: 0.2570 (0.2730)  loss: 0.2776 (0.2730)  time: 0.1675\n",
      "Epoch: [12]  [ 500/5008]  eta: 0:17:34  lr: 0.000500  loss_labels: 0.2661 (0.2713)  loss: 0.2624 (0.2713)  time: 0.1675\n",
      "Epoch: [12]  [ 600/5008]  eta: 0:16:23  lr: 0.000500  loss_labels: 0.2602 (0.2709)  loss: 0.2378 (0.2709)  time: 0.1689\n",
      "Epoch: [12]  [ 700/5008]  eta: 0:15:27  lr: 0.000500  loss_labels: 0.2653 (0.2704)  loss: 0.2727 (0.2704)  time: 0.1693\n",
      "Epoch: [12]  [ 800/5008]  eta: 0:14:48  lr: 0.000500  loss_labels: 0.2645 (0.2711)  loss: 0.2992 (0.2711)  time: 0.1826\n",
      "Epoch: [12]  [ 900/5008]  eta: 0:14:14  lr: 0.000500  loss_labels: 0.2493 (0.2697)  loss: 0.2408 (0.2697)  time: 0.1782\n",
      "Epoch: [12]  [1000/5008]  eta: 0:13:56  lr: 0.000500  loss_labels: 0.2595 (0.2690)  loss: 0.2590 (0.2690)  time: 0.3523\n",
      "Epoch: [12]  [1100/5008]  eta: 0:14:31  lr: 0.000500  loss_labels: 0.2542 (0.2683)  loss: 0.2535 (0.2683)  time: 0.2276\n",
      "Epoch: [12]  [1200/5008]  eta: 0:14:08  lr: 0.000500  loss_labels: 0.2578 (0.2682)  loss: 0.2760 (0.2682)  time: 0.2379\n",
      "Epoch: [12]  [1300/5008]  eta: 0:13:50  lr: 0.000500  loss_labels: 0.2516 (0.2677)  loss: 0.2539 (0.2677)  time: 0.2967\n",
      "Epoch: [12]  [1400/5008]  eta: 0:13:26  lr: 0.000500  loss_labels: 0.2485 (0.2670)  loss: 0.2513 (0.2670)  time: 0.1811\n",
      "Epoch: [12]  [1500/5008]  eta: 0:12:51  lr: 0.000500  loss_labels: 0.2667 (0.2670)  loss: 0.2678 (0.2670)  time: 0.1686\n",
      "Epoch: [12]  [1600/5008]  eta: 0:12:19  lr: 0.000500  loss_labels: 0.2713 (0.2671)  loss: 0.2629 (0.2671)  time: 0.1716\n",
      "Epoch: [12]  [1700/5008]  eta: 0:11:48  lr: 0.000500  loss_labels: 0.2641 (0.2672)  loss: 0.2558 (0.2672)  time: 0.1713\n",
      "Epoch: [12]  [1800/5008]  eta: 0:11:19  lr: 0.000500  loss_labels: 0.2522 (0.2668)  loss: 0.2508 (0.2668)  time: 0.1705\n",
      "Epoch: [12]  [1900/5008]  eta: 0:10:51  lr: 0.000500  loss_labels: 0.2595 (0.2665)  loss: 0.2809 (0.2665)  time: 0.1703\n",
      "Epoch: [12]  [2000/5008]  eta: 0:10:25  lr: 0.000500  loss_labels: 0.2562 (0.2665)  loss: 0.2131 (0.2665)  time: 0.1857\n",
      "Epoch: [12]  [2100/5008]  eta: 0:10:00  lr: 0.000500  loss_labels: 0.2517 (0.2660)  loss: 0.2756 (0.2660)  time: 0.1704\n",
      "Epoch: [12]  [2200/5008]  eta: 0:09:35  lr: 0.000500  loss_labels: 0.2562 (0.2657)  loss: 0.2545 (0.2657)  time: 0.1747\n",
      "Epoch: [12]  [2300/5008]  eta: 0:09:18  lr: 0.000500  loss_labels: 0.2591 (0.2659)  loss: 0.2498 (0.2659)  time: 0.2016\n",
      "Epoch: [12]  [2400/5008]  eta: 0:08:57  lr: 0.000500  loss_labels: 0.2490 (0.2654)  loss: 0.2309 (0.2654)  time: 0.1822\n",
      "Epoch: [12]  [2500/5008]  eta: 0:08:35  lr: 0.000500  loss_labels: 0.2468 (0.2649)  loss: 0.2646 (0.2649)  time: 0.1828\n",
      "Epoch: [12]  [2600/5008]  eta: 0:08:17  lr: 0.000500  loss_labels: 0.2656 (0.2651)  loss: 0.2381 (0.2651)  time: 0.3172\n",
      "Epoch: [12]  [2700/5008]  eta: 0:07:59  lr: 0.000500  loss_labels: 0.2576 (0.2648)  loss: 0.2569 (0.2648)  time: 0.2345\n",
      "Epoch: [12]  [2800/5008]  eta: 0:07:38  lr: 0.000500  loss_labels: 0.2531 (0.2646)  loss: 0.2384 (0.2646)  time: 0.2159\n",
      "Epoch: [12]  [2900/5008]  eta: 0:07:19  lr: 0.000500  loss_labels: 0.2439 (0.2642)  loss: 0.2428 (0.2642)  time: 0.1712\n",
      "Epoch: [12]  [3000/5008]  eta: 0:06:56  lr: 0.000500  loss_labels: 0.2483 (0.2640)  loss: 0.2318 (0.2640)  time: 0.1712\n",
      "Epoch: [12]  [3100/5008]  eta: 0:06:33  lr: 0.000500  loss_labels: 0.2545 (0.2640)  loss: 0.2359 (0.2640)  time: 0.1723\n",
      "Epoch: [12]  [3200/5008]  eta: 0:06:11  lr: 0.000500  loss_labels: 0.2495 (0.2637)  loss: 0.2812 (0.2637)  time: 0.1718\n",
      "Epoch: [12]  [3300/5008]  eta: 0:05:48  lr: 0.000500  loss_labels: 0.2509 (0.2634)  loss: 0.2377 (0.2634)  time: 0.1715\n",
      "Epoch: [12]  [3400/5008]  eta: 0:05:26  lr: 0.000500  loss_labels: 0.2396 (0.2628)  loss: 0.2391 (0.2628)  time: 0.1703\n",
      "Epoch: [12]  [3500/5008]  eta: 0:05:05  lr: 0.000500  loss_labels: 0.2462 (0.2627)  loss: 0.2581 (0.2627)  time: 0.1748\n",
      "Epoch: [12]  [3600/5008]  eta: 0:04:43  lr: 0.000500  loss_labels: 0.2458 (0.2623)  loss: 0.2612 (0.2623)  time: 0.1724\n",
      "Epoch: [12]  [3700/5008]  eta: 0:04:22  lr: 0.000500  loss_labels: 0.2401 (0.2620)  loss: 0.2369 (0.2620)  time: 0.1704\n",
      "Epoch: [12]  [3800/5008]  eta: 0:04:01  lr: 0.000500  loss_labels: 0.2370 (0.2616)  loss: 0.2342 (0.2616)  time: 0.1706\n",
      "Epoch: [12]  [3900/5008]  eta: 0:03:40  lr: 0.000500  loss_labels: 0.2559 (0.2616)  loss: 0.2557 (0.2616)  time: 0.1701\n",
      "Epoch: [12]  [4000/5008]  eta: 0:03:20  lr: 0.000500  loss_labels: 0.2491 (0.2614)  loss: 0.2171 (0.2614)  time: 0.1710\n",
      "Epoch: [12]  [4100/5008]  eta: 0:02:59  lr: 0.000500  loss_labels: 0.2512 (0.2611)  loss: 0.2660 (0.2611)  time: 0.1709\n",
      "Epoch: [12]  [4200/5008]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.2448 (0.2609)  loss: 0.2381 (0.2609)  time: 0.1732\n",
      "Epoch: [12]  [4300/5008]  eta: 0:02:19  lr: 0.000500  loss_labels: 0.2325 (0.2604)  loss: 0.2571 (0.2604)  time: 0.1696\n",
      "Epoch: [12]  [4400/5008]  eta: 0:01:59  lr: 0.000500  loss_labels: 0.2546 (0.2605)  loss: 0.2668 (0.2605)  time: 0.1702\n",
      "Epoch: [12]  [4500/5008]  eta: 0:01:39  lr: 0.000500  loss_labels: 0.2403 (0.2602)  loss: 0.2445 (0.2602)  time: 0.1710\n",
      "Epoch: [12]  [4600/5008]  eta: 0:01:19  lr: 0.000500  loss_labels: 0.2439 (0.2600)  loss: 0.2439 (0.2600)  time: 0.1720\n",
      "Epoch: [12]  [4700/5008]  eta: 0:00:59  lr: 0.000500  loss_labels: 0.2280 (0.2595)  loss: 0.2439 (0.2595)  time: 0.1721\n",
      "Epoch: [12]  [4800/5008]  eta: 0:00:40  lr: 0.000500  loss_labels: 0.2353 (0.2592)  loss: 0.2202 (0.2592)  time: 0.1720\n",
      "Epoch: [12]  [4900/5008]  eta: 0:00:20  lr: 0.000500  loss_labels: 0.2300 (0.2588)  loss: 0.2264 (0.2588)  time: 0.1733\n",
      "Epoch: [12]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.2360 (0.2585)  loss: 0.2237 (0.2585)  time: 0.1710\n",
      "Epoch: [12]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.2301 (0.2584)  loss: 0.2050 (0.2584)  time: 0.1710\n",
      "Epoch: [12] Total time: 0:16:07 (0.1931 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.2301 (0.2584)  loss: 0.2050 (0.2584)\n",
      "Test:  [  0/565]  eta: 5:27:43  loss_labels: 1.0955 (1.0955)  loss: 1.0955 (1.0955)  time: 34.8022\n",
      "Test:  [100/565]  eta: 0:04:09  loss_labels: 1.3433 (1.3663)  loss: 1.3572 (1.3663)  time: 0.1769\n",
      "Test:  [200/565]  eta: 0:02:08  loss_labels: 1.1727 (1.3046)  loss: 1.2092 (1.3046)  time: 0.1452\n",
      "Test:  [300/565]  eta: 0:01:18  loss_labels: 1.0351 (1.2385)  loss: 0.8194 (1.2385)  time: 0.3420\n",
      "Test:  [400/565]  eta: 0:00:45  loss_labels: 1.3112 (1.2613)  loss: 1.2715 (1.2613)  time: 0.1745\n",
      "Test:  [500/565]  eta: 0:00:16  loss_labels: 1.0322 (1.2367)  loss: 1.1416 (1.2367)  time: 0.1560\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1055 (1.2198)  loss: 1.1736 (1.2198)  time: 0.1362\n",
      "Test: Total time: 0:02:18 (0.2457 s / it)\n",
      "Averaged stats: loss_labels: 1.1055 (1.2198)  loss: 1.1736 (1.2198)\n",
      "acc: 0.7691657543182373\n",
      "top 1 and top 5 accuracies {'top1': 0.7691657896194538, 'top5': 0.885503794383205, 'loss': tensor(0.0095, device='cuda:0')}\n",
      "Epoch: [13]  [   0/5008]  eta: 1 day, 22:00:28  lr: 0.000500  loss_labels: 0.3154 (0.3154)  loss: 0.3154 (0.3154)  time: 33.0727\n",
      "Epoch: [13]  [ 100/5008]  eta: 0:42:30  lr: 0.000500  loss_labels: 0.2360 (0.2459)  loss: 0.2195 (0.2459)  time: 0.1674\n",
      "Epoch: [13]  [ 200/5008]  eta: 0:27:35  lr: 0.000500  loss_labels: 0.2214 (0.2389)  loss: 0.2308 (0.2389)  time: 0.1669\n",
      "Epoch: [13]  [ 300/5008]  eta: 0:22:26  lr: 0.000500  loss_labels: 0.2473 (0.2440)  loss: 0.2726 (0.2440)  time: 0.1678\n",
      "Epoch: [13]  [ 400/5008]  eta: 0:19:41  lr: 0.000500  loss_labels: 0.2215 (0.2401)  loss: 0.2362 (0.2401)  time: 0.1675\n",
      "Epoch: [13]  [ 500/5008]  eta: 0:17:56  lr: 0.000500  loss_labels: 0.2150 (0.2379)  loss: 0.2091 (0.2379)  time: 0.1714\n",
      "Epoch: [13]  [ 600/5008]  eta: 0:16:41  lr: 0.000500  loss_labels: 0.2322 (0.2385)  loss: 0.2090 (0.2385)  time: 0.1666\n",
      "Epoch: [13]  [ 700/5008]  eta: 0:15:43  lr: 0.000500  loss_labels: 0.2268 (0.2375)  loss: 0.2429 (0.2375)  time: 0.1694\n",
      "Epoch: [13]  [ 800/5008]  eta: 0:14:55  lr: 0.000500  loss_labels: 0.2302 (0.2377)  loss: 0.2430 (0.2377)  time: 0.1683\n",
      "Epoch: [13]  [ 900/5008]  eta: 0:14:14  lr: 0.000500  loss_labels: 0.2248 (0.2369)  loss: 0.2101 (0.2369)  time: 0.1719\n",
      "Epoch: [13]  [1000/5008]  eta: 0:13:38  lr: 0.000500  loss_labels: 0.2238 (0.2368)  loss: 0.2371 (0.2368)  time: 0.1692\n",
      "Epoch: [13]  [1100/5008]  eta: 0:13:06  lr: 0.000500  loss_labels: 0.2279 (0.2365)  loss: 0.2062 (0.2365)  time: 0.1681\n",
      "Epoch: [13]  [1200/5008]  eta: 0:12:37  lr: 0.000500  loss_labels: 0.2248 (0.2366)  loss: 0.2243 (0.2366)  time: 0.1777\n",
      "Epoch: [13]  [1300/5008]  eta: 0:12:09  lr: 0.000500  loss_labels: 0.2278 (0.2366)  loss: 0.2252 (0.2366)  time: 0.1698\n",
      "Epoch: [13]  [1400/5008]  eta: 0:11:42  lr: 0.000500  loss_labels: 0.2190 (0.2359)  loss: 0.2182 (0.2359)  time: 0.1702\n",
      "Epoch: [13]  [1500/5008]  eta: 0:11:16  lr: 0.000500  loss_labels: 0.2304 (0.2359)  loss: 0.1987 (0.2359)  time: 0.1675\n",
      "Epoch: [13]  [1600/5008]  eta: 0:10:52  lr: 0.000500  loss_labels: 0.2311 (0.2359)  loss: 0.2177 (0.2359)  time: 0.1726\n",
      "Epoch: [13]  [1700/5008]  eta: 0:10:30  lr: 0.000500  loss_labels: 0.2270 (0.2359)  loss: 0.2299 (0.2359)  time: 0.1700\n",
      "Epoch: [13]  [1800/5008]  eta: 0:10:16  lr: 0.000500  loss_labels: 0.2230 (0.2359)  loss: 0.2014 (0.2359)  time: 0.4291\n",
      "Epoch: [13]  [1900/5008]  eta: 0:09:53  lr: 0.000500  loss_labels: 0.2268 (0.2358)  loss: 0.2467 (0.2358)  time: 0.1720\n",
      "Epoch: [13]  [2000/5008]  eta: 0:09:31  lr: 0.000500  loss_labels: 0.2300 (0.2358)  loss: 0.1842 (0.2358)  time: 0.1719\n",
      "Epoch: [13]  [2100/5008]  eta: 0:09:10  lr: 0.000500  loss_labels: 0.2147 (0.2350)  loss: 0.2402 (0.2350)  time: 0.1709\n",
      "Epoch: [13]  [2200/5008]  eta: 0:08:49  lr: 0.000500  loss_labels: 0.2317 (0.2350)  loss: 0.2364 (0.2350)  time: 0.1702\n",
      "Epoch: [13]  [2300/5008]  eta: 0:08:29  lr: 0.000500  loss_labels: 0.2335 (0.2352)  loss: 0.2392 (0.2352)  time: 0.1727\n",
      "Epoch: [13]  [2400/5008]  eta: 0:08:08  lr: 0.000500  loss_labels: 0.2266 (0.2348)  loss: 0.2289 (0.2348)  time: 0.1707\n",
      "Epoch: [13]  [2500/5008]  eta: 0:07:50  lr: 0.000500  loss_labels: 0.2084 (0.2343)  loss: 0.2173 (0.2343)  time: 0.2895\n",
      "Epoch: [13]  [2600/5008]  eta: 0:07:35  lr: 0.000500  loss_labels: 0.2187 (0.2342)  loss: 0.2161 (0.2342)  time: 0.1979\n",
      "Epoch: [13]  [2700/5008]  eta: 0:07:17  lr: 0.000500  loss_labels: 0.2239 (0.2341)  loss: 0.2196 (0.2341)  time: 0.1872\n",
      "Epoch: [13]  [2800/5008]  eta: 0:06:56  lr: 0.000500  loss_labels: 0.2233 (0.2340)  loss: 0.2259 (0.2340)  time: 0.1701\n",
      "Epoch: [13]  [2900/5008]  eta: 0:06:36  lr: 0.000500  loss_labels: 0.2186 (0.2337)  loss: 0.2147 (0.2337)  time: 0.1700\n",
      "Epoch: [13]  [3000/5008]  eta: 0:06:22  lr: 0.000500  loss_labels: 0.2256 (0.2335)  loss: 0.2112 (0.2335)  time: 0.1688\n",
      "Epoch: [13]  [3100/5008]  eta: 0:06:02  lr: 0.000500  loss_labels: 0.2248 (0.2334)  loss: 0.2036 (0.2334)  time: 0.1687\n",
      "Epoch: [13]  [3200/5008]  eta: 0:05:42  lr: 0.000500  loss_labels: 0.2111 (0.2331)  loss: 0.2363 (0.2331)  time: 0.1696\n",
      "Epoch: [13]  [3300/5008]  eta: 0:05:22  lr: 0.000500  loss_labels: 0.2213 (0.2327)  loss: 0.1835 (0.2327)  time: 0.1827\n",
      "Epoch: [13]  [3400/5008]  eta: 0:05:05  lr: 0.000500  loss_labels: 0.2098 (0.2322)  loss: 0.2155 (0.2322)  time: 0.2591\n",
      "Epoch: [13]  [3500/5008]  eta: 0:04:46  lr: 0.000500  loss_labels: 0.2173 (0.2322)  loss: 0.2898 (0.2322)  time: 0.1708\n",
      "Epoch: [13]  [3600/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.2106 (0.2318)  loss: 0.2067 (0.2318)  time: 0.1696\n",
      "Epoch: [13]  [3700/5008]  eta: 0:04:07  lr: 0.000500  loss_labels: 0.2183 (0.2316)  loss: 0.2148 (0.2316)  time: 0.1712\n",
      "Epoch: [13]  [3800/5008]  eta: 0:03:47  lr: 0.000500  loss_labels: 0.2174 (0.2311)  loss: 0.1903 (0.2311)  time: 0.1727\n",
      "Epoch: [13]  [3900/5008]  eta: 0:03:29  lr: 0.000500  loss_labels: 0.2270 (0.2312)  loss: 0.2208 (0.2312)  time: 0.1964\n",
      "Epoch: [13]  [4000/5008]  eta: 0:03:11  lr: 0.000500  loss_labels: 0.2082 (0.2309)  loss: 0.1726 (0.2309)  time: 0.1764\n",
      "Epoch: [13]  [4100/5008]  eta: 0:02:52  lr: 0.000500  loss_labels: 0.2205 (0.2306)  loss: 0.2405 (0.2306)  time: 0.2123\n",
      "Epoch: [13]  [4200/5008]  eta: 0:02:36  lr: 0.000500  loss_labels: 0.2225 (0.2303)  loss: 0.2110 (0.2303)  time: 0.1953\n",
      "Epoch: [13]  [4300/5008]  eta: 0:02:16  lr: 0.000500  loss_labels: 0.2044 (0.2300)  loss: 0.2141 (0.2300)  time: 0.1663\n",
      "Epoch: [13]  [4400/5008]  eta: 0:01:57  lr: 0.000500  loss_labels: 0.2273 (0.2299)  loss: 0.2189 (0.2299)  time: 0.1716\n",
      "Epoch: [13]  [4500/5008]  eta: 0:01:37  lr: 0.000500  loss_labels: 0.2056 (0.2296)  loss: 0.2163 (0.2296)  time: 0.2290\n",
      "Epoch: [13]  [4600/5008]  eta: 0:01:18  lr: 0.000500  loss_labels: 0.2098 (0.2295)  loss: 0.2069 (0.2295)  time: 0.2047\n",
      "Epoch: [13]  [4700/5008]  eta: 0:00:59  lr: 0.000500  loss_labels: 0.2035 (0.2290)  loss: 0.1998 (0.2290)  time: 0.1671\n",
      "Epoch: [13]  [4800/5008]  eta: 0:00:39  lr: 0.000500  loss_labels: 0.2213 (0.2289)  loss: 0.2267 (0.2289)  time: 0.1675\n",
      "Epoch: [13]  [4900/5008]  eta: 0:00:20  lr: 0.000500  loss_labels: 0.2092 (0.2286)  loss: 0.2082 (0.2286)  time: 0.1691\n",
      "Epoch: [13]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.2141 (0.2285)  loss: 0.2202 (0.2285)  time: 0.1732\n",
      "Epoch: [13]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.2059 (0.2285)  loss: 0.2013 (0.2285)  time: 0.1726\n",
      "Epoch: [13] Total time: 0:15:54 (0.1906 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.2059 (0.2285)  loss: 0.2013 (0.2285)\n",
      "Test:  [  0/565]  eta: 3:42:55  loss_labels: 1.1876 (1.1876)  loss: 1.1876 (1.1876)  time: 23.6734\n",
      "Test:  [100/565]  eta: 0:02:55  loss_labels: 1.3286 (1.4387)  loss: 1.3334 (1.4387)  time: 0.1451\n",
      "Test:  [200/565]  eta: 0:01:36  loss_labels: 1.1242 (1.3500)  loss: 1.3933 (1.3500)  time: 0.1471\n",
      "Test:  [300/565]  eta: 0:01:00  loss_labels: 1.0774 (1.2846)  loss: 0.9177 (1.2846)  time: 0.1646\n",
      "Test:  [400/565]  eta: 0:00:34  loss_labels: 1.3807 (1.3127)  loss: 1.3842 (1.3127)  time: 0.1548\n",
      "Test:  [500/565]  eta: 0:00:12  loss_labels: 1.1071 (1.2874)  loss: 1.2173 (1.2874)  time: 0.1351\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1180 (1.2697)  loss: 1.1970 (1.2697)  time: 0.1542\n",
      "Test: Total time: 0:01:49 (0.1947 s / it)\n",
      "Averaged stats: loss_labels: 1.1180 (1.2697)  loss: 1.1970 (1.2697)\n",
      "acc: 0.767836332321167\n",
      "top 1 and top 5 accuracies {'top1': 0.7678363706863125, 'top5': 0.8839805018556472, 'loss': tensor(0.0099, device='cuda:0')}\n",
      "Epoch: [14]  [   0/5008]  eta: 1 day, 14:42:02  lr: 0.000500  loss_labels: 0.3081 (0.3081)  loss: 0.3081 (0.3081)  time: 27.8201\n",
      "Epoch: [14]  [ 100/5008]  eta: 0:36:17  lr: 0.000500  loss_labels: 0.2130 (0.2185)  loss: 0.2131 (0.2185)  time: 0.1704\n",
      "Epoch: [14]  [ 200/5008]  eta: 0:24:38  lr: 0.000500  loss_labels: 0.1957 (0.2139)  loss: 0.2110 (0.2139)  time: 0.1703\n",
      "Epoch: [14]  [ 300/5008]  eta: 0:20:32  lr: 0.000500  loss_labels: 0.2140 (0.2167)  loss: 0.2371 (0.2167)  time: 0.1692\n",
      "Epoch: [14]  [ 400/5008]  eta: 0:18:24  lr: 0.000500  loss_labels: 0.1966 (0.2133)  loss: 0.2115 (0.2133)  time: 0.1691\n",
      "Epoch: [14]  [ 500/5008]  eta: 0:16:58  lr: 0.000500  loss_labels: 0.2030 (0.2112)  loss: 0.1819 (0.2112)  time: 0.1710\n",
      "Epoch: [14]  [ 600/5008]  eta: 0:15:55  lr: 0.000500  loss_labels: 0.2070 (0.2112)  loss: 0.1888 (0.2112)  time: 0.1704\n",
      "Epoch: [14]  [ 700/5008]  eta: 0:15:04  lr: 0.000500  loss_labels: 0.2070 (0.2108)  loss: 0.2244 (0.2108)  time: 0.1702\n",
      "Epoch: [14]  [ 800/5008]  eta: 0:14:22  lr: 0.000500  loss_labels: 0.2006 (0.2104)  loss: 0.2055 (0.2104)  time: 0.1716\n",
      "Epoch: [14]  [ 900/5008]  eta: 0:13:46  lr: 0.000500  loss_labels: 0.2049 (0.2103)  loss: 0.1757 (0.2103)  time: 0.1711\n",
      "Epoch: [14]  [1000/5008]  eta: 0:13:14  lr: 0.000500  loss_labels: 0.1885 (0.2094)  loss: 0.1885 (0.2094)  time: 0.1716\n",
      "Epoch: [14]  [1100/5008]  eta: 0:12:44  lr: 0.000500  loss_labels: 0.1920 (0.2086)  loss: 0.1925 (0.2086)  time: 0.1694\n",
      "Epoch: [14]  [1200/5008]  eta: 0:12:16  lr: 0.000500  loss_labels: 0.2066 (0.2087)  loss: 0.2213 (0.2087)  time: 0.1696\n",
      "Epoch: [14]  [1300/5008]  eta: 0:11:50  lr: 0.000500  loss_labels: 0.1900 (0.2082)  loss: 0.1912 (0.2082)  time: 0.1685\n",
      "Epoch: [14]  [1400/5008]  eta: 0:11:25  lr: 0.000500  loss_labels: 0.2033 (0.2082)  loss: 0.2224 (0.2082)  time: 0.1707\n",
      "Epoch: [14]  [1500/5008]  eta: 0:11:02  lr: 0.000500  loss_labels: 0.2020 (0.2082)  loss: 0.1933 (0.2082)  time: 0.1707\n",
      "Epoch: [14]  [1600/5008]  eta: 0:10:39  lr: 0.000500  loss_labels: 0.2053 (0.2086)  loss: 0.2163 (0.2086)  time: 0.1692\n",
      "Epoch: [14]  [1700/5008]  eta: 0:10:17  lr: 0.000500  loss_labels: 0.2089 (0.2091)  loss: 0.2081 (0.2091)  time: 0.1707\n",
      "Epoch: [14]  [1800/5008]  eta: 0:09:55  lr: 0.000500  loss_labels: 0.1977 (0.2088)  loss: 0.1862 (0.2088)  time: 0.1726\n",
      "Epoch: [14]  [1900/5008]  eta: 0:09:35  lr: 0.000500  loss_labels: 0.1995 (0.2086)  loss: 0.1988 (0.2086)  time: 0.1723\n",
      "Epoch: [14]  [2000/5008]  eta: 0:09:14  lr: 0.000500  loss_labels: 0.2014 (0.2084)  loss: 0.1663 (0.2084)  time: 0.1722\n",
      "Epoch: [14]  [2100/5008]  eta: 0:08:54  lr: 0.000500  loss_labels: 0.1965 (0.2080)  loss: 0.1972 (0.2080)  time: 0.1705\n",
      "Epoch: [14]  [2200/5008]  eta: 0:08:34  lr: 0.000500  loss_labels: 0.1949 (0.2078)  loss: 0.2052 (0.2078)  time: 0.1723\n",
      "Epoch: [14]  [2300/5008]  eta: 0:08:14  lr: 0.000500  loss_labels: 0.2013 (0.2080)  loss: 0.1897 (0.2080)  time: 0.1731\n",
      "Epoch: [14]  [2400/5008]  eta: 0:07:55  lr: 0.000500  loss_labels: 0.1976 (0.2078)  loss: 0.1905 (0.2078)  time: 0.1712\n",
      "Epoch: [14]  [2500/5008]  eta: 0:07:35  lr: 0.000500  loss_labels: 0.1890 (0.2073)  loss: 0.1871 (0.2073)  time: 0.1707\n",
      "Epoch: [14]  [2600/5008]  eta: 0:07:16  lr: 0.000500  loss_labels: 0.1934 (0.2075)  loss: 0.1844 (0.2075)  time: 0.1716\n",
      "Epoch: [14]  [2700/5008]  eta: 0:06:57  lr: 0.000500  loss_labels: 0.2012 (0.2075)  loss: 0.2024 (0.2075)  time: 0.1716\n",
      "Epoch: [14]  [2800/5008]  eta: 0:06:39  lr: 0.000500  loss_labels: 0.1989 (0.2077)  loss: 0.1921 (0.2077)  time: 0.1708\n",
      "Epoch: [14]  [2900/5008]  eta: 0:06:20  lr: 0.000500  loss_labels: 0.1959 (0.2074)  loss: 0.1713 (0.2074)  time: 0.1700\n",
      "Epoch: [14]  [3000/5008]  eta: 0:06:01  lr: 0.000500  loss_labels: 0.2013 (0.2074)  loss: 0.1898 (0.2074)  time: 0.1709\n",
      "Epoch: [14]  [3100/5008]  eta: 0:05:43  lr: 0.000500  loss_labels: 0.1979 (0.2073)  loss: 0.1828 (0.2073)  time: 0.1719\n",
      "Epoch: [14]  [3200/5008]  eta: 0:05:24  lr: 0.000500  loss_labels: 0.1914 (0.2071)  loss: 0.2187 (0.2071)  time: 0.1725\n",
      "Epoch: [14]  [3300/5008]  eta: 0:05:06  lr: 0.000500  loss_labels: 0.1870 (0.2068)  loss: 0.1803 (0.2068)  time: 0.1715\n",
      "Epoch: [14]  [3400/5008]  eta: 0:04:48  lr: 0.000500  loss_labels: 0.1826 (0.2064)  loss: 0.2014 (0.2064)  time: 0.1727\n",
      "Epoch: [14]  [3500/5008]  eta: 0:04:29  lr: 0.000500  loss_labels: 0.2125 (0.2067)  loss: 0.2306 (0.2067)  time: 0.1719\n",
      "Epoch: [14]  [3600/5008]  eta: 0:04:11  lr: 0.000500  loss_labels: 0.1862 (0.2063)  loss: 0.1781 (0.2063)  time: 0.1714\n",
      "Epoch: [14]  [3700/5008]  eta: 0:03:53  lr: 0.000500  loss_labels: 0.1863 (0.2060)  loss: 0.2036 (0.2060)  time: 0.1748\n",
      "Epoch: [14]  [3800/5008]  eta: 0:03:36  lr: 0.000500  loss_labels: 0.1852 (0.2057)  loss: 0.1918 (0.2057)  time: 0.1726\n",
      "Epoch: [14]  [3900/5008]  eta: 0:03:18  lr: 0.000500  loss_labels: 0.2004 (0.2057)  loss: 0.2009 (0.2057)  time: 0.1749\n",
      "Epoch: [14]  [4000/5008]  eta: 0:03:00  lr: 0.000500  loss_labels: 0.1996 (0.2055)  loss: 0.1877 (0.2055)  time: 0.1736\n",
      "Epoch: [14]  [4100/5008]  eta: 0:02:42  lr: 0.000500  loss_labels: 0.1880 (0.2053)  loss: 0.1822 (0.2053)  time: 0.1733\n",
      "Epoch: [14]  [4200/5008]  eta: 0:02:24  lr: 0.000500  loss_labels: 0.1843 (0.2050)  loss: 0.1639 (0.2050)  time: 0.1716\n",
      "Epoch: [14]  [4300/5008]  eta: 0:02:06  lr: 0.000500  loss_labels: 0.1863 (0.2047)  loss: 0.2162 (0.2047)  time: 0.1758\n",
      "Epoch: [14]  [4400/5008]  eta: 0:01:48  lr: 0.000500  loss_labels: 0.2074 (0.2050)  loss: 0.2076 (0.2050)  time: 0.1708\n",
      "Epoch: [14]  [4500/5008]  eta: 0:01:30  lr: 0.000500  loss_labels: 0.1721 (0.2047)  loss: 0.1944 (0.2047)  time: 0.1709\n",
      "Epoch: [14]  [4600/5008]  eta: 0:01:13  lr: 0.000500  loss_labels: 0.1971 (0.2046)  loss: 0.1864 (0.2046)  time: 0.1858\n",
      "Epoch: [14]  [4700/5008]  eta: 0:00:55  lr: 0.000500  loss_labels: 0.1865 (0.2044)  loss: 0.1826 (0.2044)  time: 0.1680\n",
      "Epoch: [14]  [4800/5008]  eta: 0:00:37  lr: 0.000500  loss_labels: 0.1937 (0.2043)  loss: 0.1909 (0.2043)  time: 0.1691\n",
      "Epoch: [14]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.1860 (0.2041)  loss: 0.1711 (0.2041)  time: 0.1695\n",
      "Epoch: [14]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.2013 (0.2039)  loss: 0.1952 (0.2039)  time: 0.1725\n",
      "Epoch: [14]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.1938 (0.2039)  loss: 0.1795 (0.2039)  time: 0.1731\n",
      "Epoch: [14] Total time: 0:14:53 (0.1785 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.1938 (0.2039)  loss: 0.1795 (0.2039)\n",
      "Test:  [  0/565]  eta: 3:02:49  loss_labels: 1.1871 (1.1871)  loss: 1.1871 (1.1871)  time: 19.4155\n",
      "Test:  [100/565]  eta: 0:02:36  loss_labels: 1.2834 (1.4182)  loss: 1.3736 (1.4182)  time: 0.1338\n",
      "Test:  [200/565]  eta: 0:01:26  loss_labels: 1.1349 (1.3451)  loss: 1.4739 (1.3451)  time: 0.1265\n",
      "Test:  [300/565]  eta: 0:00:54  loss_labels: 1.0583 (1.2739)  loss: 0.9423 (1.2739)  time: 0.1522\n",
      "Test:  [400/565]  eta: 0:00:31  loss_labels: 1.3861 (1.3063)  loss: 1.4997 (1.3063)  time: 0.1413\n",
      "Test:  [500/565]  eta: 0:00:11  loss_labels: 1.0916 (1.2828)  loss: 1.2737 (1.2828)  time: 0.1368\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1171 (1.2665)  loss: 1.1566 (1.2665)  time: 0.1425\n",
      "Test: Total time: 0:01:40 (0.1770 s / it)\n",
      "Averaged stats: loss_labels: 1.1171 (1.2665)  loss: 1.1566 (1.2665)\n",
      "acc: 0.772267758846283\n",
      "top 1 and top 5 accuracies {'top1': 0.7722677671301169, 'top5': 0.8874425303273694, 'loss': tensor(0.0099, device='cuda:0')}\n",
      "Epoch: [15]  [   0/5008]  eta: 1 day, 14:51:58  lr: 0.000500  loss_labels: 0.2365 (0.2365)  loss: 0.2365 (0.2365)  time: 27.9391\n",
      "Epoch: [15]  [ 100/5008]  eta: 0:36:31  lr: 0.000500  loss_labels: 0.1955 (0.1992)  loss: 0.1861 (0.1992)  time: 0.1711\n",
      "Epoch: [15]  [ 200/5008]  eta: 0:24:47  lr: 0.000500  loss_labels: 0.1838 (0.1950)  loss: 0.1812 (0.1950)  time: 0.1715\n",
      "Epoch: [15]  [ 300/5008]  eta: 0:20:42  lr: 0.000500  loss_labels: 0.2005 (0.1995)  loss: 0.2345 (0.1995)  time: 0.1719\n",
      "Epoch: [15]  [ 400/5008]  eta: 0:18:29  lr: 0.000500  loss_labels: 0.1759 (0.1950)  loss: 0.2021 (0.1950)  time: 0.1719\n",
      "Epoch: [15]  [ 500/5008]  eta: 0:17:02  lr: 0.000500  loss_labels: 0.1830 (0.1927)  loss: 0.1765 (0.1927)  time: 0.1704\n",
      "Epoch: [15]  [ 600/5008]  eta: 0:15:59  lr: 0.000500  loss_labels: 0.1855 (0.1924)  loss: 0.1619 (0.1924)  time: 0.1719\n",
      "Epoch: [15]  [ 700/5008]  eta: 0:15:09  lr: 0.000500  loss_labels: 0.1774 (0.1912)  loss: 0.1948 (0.1912)  time: 0.1714\n",
      "Epoch: [15]  [ 800/5008]  eta: 0:14:28  lr: 0.000500  loss_labels: 0.1878 (0.1922)  loss: 0.2077 (0.1922)  time: 0.1697\n",
      "Epoch: [15]  [ 900/5008]  eta: 0:13:51  lr: 0.000500  loss_labels: 0.1829 (0.1915)  loss: 0.1666 (0.1915)  time: 0.1708\n",
      "Epoch: [15]  [1000/5008]  eta: 0:13:18  lr: 0.000500  loss_labels: 0.1806 (0.1910)  loss: 0.1724 (0.1910)  time: 0.1717\n",
      "Epoch: [15]  [1100/5008]  eta: 0:12:48  lr: 0.000500  loss_labels: 0.1803 (0.1906)  loss: 0.1803 (0.1906)  time: 0.1691\n",
      "Epoch: [15]  [1200/5008]  eta: 0:12:20  lr: 0.000500  loss_labels: 0.1753 (0.1900)  loss: 0.1968 (0.1900)  time: 0.1725\n",
      "Epoch: [15]  [1300/5008]  eta: 0:11:54  lr: 0.000500  loss_labels: 0.1736 (0.1897)  loss: 0.1964 (0.1897)  time: 0.1712\n",
      "Epoch: [15]  [1400/5008]  eta: 0:11:30  lr: 0.000500  loss_labels: 0.1819 (0.1895)  loss: 0.1819 (0.1895)  time: 0.1722\n",
      "Epoch: [15]  [1500/5008]  eta: 0:11:06  lr: 0.000500  loss_labels: 0.1730 (0.1894)  loss: 0.1478 (0.1894)  time: 0.1711\n",
      "Epoch: [15]  [1600/5008]  eta: 0:10:43  lr: 0.000500  loss_labels: 0.1890 (0.1900)  loss: 0.1912 (0.1900)  time: 0.1746\n",
      "Epoch: [15]  [1700/5008]  eta: 0:10:21  lr: 0.000500  loss_labels: 0.1836 (0.1897)  loss: 0.1696 (0.1897)  time: 0.1728\n",
      "Epoch: [15]  [1800/5008]  eta: 0:09:59  lr: 0.000500  loss_labels: 0.1837 (0.1898)  loss: 0.1734 (0.1898)  time: 0.1698\n",
      "Epoch: [15]  [1900/5008]  eta: 0:09:38  lr: 0.000500  loss_labels: 0.1612 (0.1890)  loss: 0.1881 (0.1890)  time: 0.1698\n",
      "Epoch: [15]  [2000/5008]  eta: 0:09:17  lr: 0.000500  loss_labels: 0.1777 (0.1891)  loss: 0.1483 (0.1891)  time: 0.1735\n",
      "Epoch: [15]  [2100/5008]  eta: 0:08:57  lr: 0.000500  loss_labels: 0.1739 (0.1883)  loss: 0.1814 (0.1883)  time: 0.1712\n",
      "Epoch: [15]  [2200/5008]  eta: 0:08:37  lr: 0.000500  loss_labels: 0.1817 (0.1880)  loss: 0.1856 (0.1880)  time: 0.1714\n",
      "Epoch: [15]  [2300/5008]  eta: 0:08:17  lr: 0.000500  loss_labels: 0.1842 (0.1879)  loss: 0.1677 (0.1879)  time: 0.1744\n",
      "Epoch: [15]  [2400/5008]  eta: 0:07:57  lr: 0.000500  loss_labels: 0.1761 (0.1879)  loss: 0.1761 (0.1879)  time: 0.1720\n",
      "Epoch: [15]  [2500/5008]  eta: 0:07:38  lr: 0.000500  loss_labels: 0.1739 (0.1876)  loss: 0.1758 (0.1876)  time: 0.1680\n",
      "Epoch: [15]  [2600/5008]  eta: 0:07:18  lr: 0.000500  loss_labels: 0.1777 (0.1874)  loss: 0.1770 (0.1874)  time: 0.1696\n",
      "Epoch: [15]  [2700/5008]  eta: 0:06:59  lr: 0.000500  loss_labels: 0.1737 (0.1873)  loss: 0.1709 (0.1873)  time: 0.1716\n",
      "Epoch: [15]  [2800/5008]  eta: 0:06:40  lr: 0.000500  loss_labels: 0.1801 (0.1873)  loss: 0.1701 (0.1873)  time: 0.1723\n",
      "Epoch: [15]  [2900/5008]  eta: 0:06:21  lr: 0.000500  loss_labels: 0.1715 (0.1870)  loss: 0.1711 (0.1870)  time: 0.1719\n",
      "Epoch: [15]  [3000/5008]  eta: 0:06:02  lr: 0.000500  loss_labels: 0.1807 (0.1869)  loss: 0.1812 (0.1869)  time: 0.1723\n",
      "Epoch: [15]  [3100/5008]  eta: 0:05:44  lr: 0.000500  loss_labels: 0.1727 (0.1868)  loss: 0.1499 (0.1868)  time: 0.1715\n",
      "Epoch: [15]  [3200/5008]  eta: 0:05:25  lr: 0.000500  loss_labels: 0.1630 (0.1864)  loss: 0.2022 (0.1864)  time: 0.1716\n",
      "Epoch: [15]  [3300/5008]  eta: 0:05:07  lr: 0.000500  loss_labels: 0.1684 (0.1860)  loss: 0.1468 (0.1860)  time: 0.1701\n",
      "Epoch: [15]  [3400/5008]  eta: 0:04:49  lr: 0.000500  loss_labels: 0.1759 (0.1858)  loss: 0.1848 (0.1858)  time: 0.1707\n",
      "Epoch: [15]  [3500/5008]  eta: 0:04:30  lr: 0.000500  loss_labels: 0.1834 (0.1860)  loss: 0.2069 (0.1860)  time: 0.1711\n",
      "Epoch: [15]  [3600/5008]  eta: 0:04:12  lr: 0.000500  loss_labels: 0.1760 (0.1859)  loss: 0.1860 (0.1859)  time: 0.1709\n",
      "Epoch: [15]  [3700/5008]  eta: 0:03:54  lr: 0.000500  loss_labels: 0.1753 (0.1857)  loss: 0.1600 (0.1857)  time: 0.1722\n",
      "Epoch: [15]  [3800/5008]  eta: 0:03:36  lr: 0.000500  loss_labels: 0.1683 (0.1856)  loss: 0.1617 (0.1856)  time: 0.1736\n",
      "Epoch: [15]  [3900/5008]  eta: 0:03:18  lr: 0.000500  loss_labels: 0.1806 (0.1856)  loss: 0.1776 (0.1856)  time: 0.1725\n",
      "Epoch: [15]  [4000/5008]  eta: 0:02:59  lr: 0.000500  loss_labels: 0.1726 (0.1855)  loss: 0.1498 (0.1855)  time: 0.1710\n",
      "Epoch: [15]  [4100/5008]  eta: 0:02:41  lr: 0.000500  loss_labels: 0.1821 (0.1854)  loss: 0.1923 (0.1854)  time: 0.1710\n",
      "Epoch: [15]  [4200/5008]  eta: 0:02:23  lr: 0.000500  loss_labels: 0.1823 (0.1853)  loss: 0.1684 (0.1853)  time: 0.1713\n",
      "Epoch: [15]  [4300/5008]  eta: 0:02:06  lr: 0.000500  loss_labels: 0.1694 (0.1851)  loss: 0.1822 (0.1851)  time: 0.1710\n",
      "Epoch: [15]  [4400/5008]  eta: 0:01:48  lr: 0.000500  loss_labels: 0.1809 (0.1852)  loss: 0.1824 (0.1852)  time: 0.1717\n",
      "Epoch: [15]  [4500/5008]  eta: 0:01:30  lr: 0.000500  loss_labels: 0.1669 (0.1851)  loss: 0.1663 (0.1851)  time: 0.1737\n",
      "Epoch: [15]  [4600/5008]  eta: 0:01:12  lr: 0.000500  loss_labels: 0.1607 (0.1848)  loss: 0.1563 (0.1848)  time: 0.1725\n",
      "Epoch: [15]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.1679 (0.1844)  loss: 0.1708 (0.1844)  time: 0.1710\n",
      "Epoch: [15]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.1732 (0.1843)  loss: 0.1712 (0.1843)  time: 0.1714\n",
      "Epoch: [15]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.1727 (0.1842)  loss: 0.1289 (0.1842)  time: 0.1718\n",
      "Epoch: [15]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.1806 (0.1842)  loss: 0.1505 (0.1842)  time: 0.1753\n",
      "Epoch: [15]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.1805 (0.1841)  loss: 0.1742 (0.1841)  time: 0.1756\n",
      "Epoch: [15] Total time: 0:14:48 (0.1774 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.1805 (0.1841)  loss: 0.1742 (0.1841)\n",
      "Test:  [  0/565]  eta: 3:18:45  loss_labels: 1.0724 (1.0724)  loss: 1.0724 (1.0724)  time: 21.1077\n",
      "Test:  [100/565]  eta: 0:02:38  loss_labels: 1.3106 (1.4317)  loss: 1.2934 (1.4317)  time: 0.1355\n",
      "Test:  [200/565]  eta: 0:01:27  loss_labels: 1.1915 (1.3599)  loss: 1.3933 (1.3599)  time: 0.1369\n",
      "Test:  [300/565]  eta: 0:00:55  loss_labels: 1.1163 (1.2910)  loss: 0.8569 (1.2910)  time: 0.1525\n",
      "Test:  [400/565]  eta: 0:00:31  loss_labels: 1.3756 (1.3096)  loss: 1.3219 (1.3096)  time: 0.1331\n",
      "Test:  [500/565]  eta: 0:00:11  loss_labels: 1.0772 (1.2855)  loss: 1.1988 (1.2855)  time: 0.1286\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1196 (1.2709)  loss: 1.2826 (1.2709)  time: 0.1263\n",
      "Test: Total time: 0:01:39 (0.1765 s / it)\n",
      "Averaged stats: loss_labels: 1.1196 (1.2709)  loss: 1.2826 (1.2709)\n",
      "acc: 0.7734725475311279\n",
      "top 1 and top 5 accuracies {'top1': 0.7734725530382762, 'top5': 0.8876502520356727, 'loss': tensor(0.0099, device='cuda:0')}\n",
      "Epoch: [16]  [   0/5008]  eta: 1 day, 6:53:23  lr: 0.000500  loss_labels: 0.2106 (0.2106)  loss: 0.2106 (0.2106)  time: 22.2052\n",
      "Epoch: [16]  [ 100/5008]  eta: 0:32:03  lr: 0.000500  loss_labels: 0.1636 (0.1726)  loss: 0.1305 (0.1726)  time: 0.1740\n",
      "Epoch: [16]  [ 200/5008]  eta: 0:22:41  lr: 0.000500  loss_labels: 0.1672 (0.1731)  loss: 0.1730 (0.1731)  time: 0.1738\n",
      "Epoch: [16]  [ 300/5008]  eta: 0:19:20  lr: 0.000500  loss_labels: 0.1630 (0.1743)  loss: 0.1734 (0.1743)  time: 0.1706\n",
      "Epoch: [16]  [ 400/5008]  eta: 0:17:29  lr: 0.000500  loss_labels: 0.1552 (0.1708)  loss: 0.1770 (0.1708)  time: 0.1704\n",
      "Epoch: [16]  [ 500/5008]  eta: 0:16:16  lr: 0.000500  loss_labels: 0.1576 (0.1691)  loss: 0.1320 (0.1691)  time: 0.1742\n",
      "Epoch: [16]  [ 600/5008]  eta: 0:15:22  lr: 0.000500  loss_labels: 0.1721 (0.1693)  loss: 0.1645 (0.1693)  time: 0.1731\n",
      "Epoch: [16]  [ 700/5008]  eta: 0:14:38  lr: 0.000500  loss_labels: 0.1659 (0.1696)  loss: 0.1954 (0.1696)  time: 0.1735\n",
      "Epoch: [16]  [ 800/5008]  eta: 0:14:01  lr: 0.000500  loss_labels: 0.1735 (0.1709)  loss: 0.1735 (0.1709)  time: 0.1722\n",
      "Epoch: [16]  [ 900/5008]  eta: 0:13:29  lr: 0.000500  loss_labels: 0.1555 (0.1701)  loss: 0.1554 (0.1701)  time: 0.1723\n",
      "Epoch: [16]  [1000/5008]  eta: 0:13:00  lr: 0.000500  loss_labels: 0.1598 (0.1699)  loss: 0.1754 (0.1699)  time: 0.1729\n",
      "Epoch: [16]  [1100/5008]  eta: 0:12:32  lr: 0.000500  loss_labels: 0.1620 (0.1696)  loss: 0.1551 (0.1696)  time: 0.1712\n",
      "Epoch: [16]  [1200/5008]  eta: 0:12:06  lr: 0.000500  loss_labels: 0.1555 (0.1691)  loss: 0.1609 (0.1691)  time: 0.1711\n",
      "Epoch: [16]  [1300/5008]  eta: 0:11:41  lr: 0.000500  loss_labels: 0.1495 (0.1686)  loss: 0.1432 (0.1686)  time: 0.1724\n",
      "Epoch: [16]  [1400/5008]  eta: 0:11:18  lr: 0.000500  loss_labels: 0.1647 (0.1685)  loss: 0.1659 (0.1685)  time: 0.1704\n",
      "Epoch: [16]  [1500/5008]  eta: 0:10:55  lr: 0.000500  loss_labels: 0.1644 (0.1689)  loss: 0.1502 (0.1689)  time: 0.1714\n",
      "Epoch: [16]  [1600/5008]  eta: 0:10:33  lr: 0.000500  loss_labels: 0.1595 (0.1690)  loss: 0.1558 (0.1690)  time: 0.1707\n",
      "Epoch: [16]  [1700/5008]  eta: 0:10:12  lr: 0.000500  loss_labels: 0.1670 (0.1695)  loss: 0.1488 (0.1695)  time: 0.1744\n",
      "Epoch: [16]  [1800/5008]  eta: 0:09:51  lr: 0.000500  loss_labels: 0.1561 (0.1692)  loss: 0.1453 (0.1692)  time: 0.1724\n",
      "Epoch: [16]  [1900/5008]  eta: 0:09:31  lr: 0.000500  loss_labels: 0.1660 (0.1691)  loss: 0.1780 (0.1691)  time: 0.1731\n",
      "Epoch: [16]  [2000/5008]  eta: 0:09:11  lr: 0.000500  loss_labels: 0.1591 (0.1689)  loss: 0.1315 (0.1689)  time: 0.1731\n",
      "Epoch: [16]  [2100/5008]  eta: 0:08:51  lr: 0.000500  loss_labels: 0.1602 (0.1687)  loss: 0.1718 (0.1687)  time: 0.1727\n",
      "Epoch: [16]  [2200/5008]  eta: 0:08:31  lr: 0.000500  loss_labels: 0.1574 (0.1683)  loss: 0.1661 (0.1683)  time: 0.1716\n",
      "Epoch: [16]  [2300/5008]  eta: 0:08:12  lr: 0.000500  loss_labels: 0.1667 (0.1684)  loss: 0.1533 (0.1684)  time: 0.1688\n",
      "Epoch: [16]  [2400/5008]  eta: 0:07:52  lr: 0.000500  loss_labels: 0.1591 (0.1684)  loss: 0.1553 (0.1684)  time: 0.1714\n",
      "Epoch: [16]  [2500/5008]  eta: 0:07:33  lr: 0.000500  loss_labels: 0.1454 (0.1679)  loss: 0.1527 (0.1679)  time: 0.1727\n",
      "Epoch: [16]  [2600/5008]  eta: 0:07:14  lr: 0.000500  loss_labels: 0.1530 (0.1679)  loss: 0.1370 (0.1679)  time: 0.1711\n",
      "Epoch: [16]  [2700/5008]  eta: 0:06:55  lr: 0.000500  loss_labels: 0.1655 (0.1677)  loss: 0.1564 (0.1677)  time: 0.1718\n",
      "Epoch: [16]  [2800/5008]  eta: 0:06:37  lr: 0.000500  loss_labels: 0.1614 (0.1677)  loss: 0.1578 (0.1677)  time: 0.1735\n",
      "Epoch: [16]  [2900/5008]  eta: 0:06:18  lr: 0.000500  loss_labels: 0.1500 (0.1675)  loss: 0.1304 (0.1675)  time: 0.1716\n",
      "Epoch: [16]  [3000/5008]  eta: 0:06:00  lr: 0.000500  loss_labels: 0.1559 (0.1674)  loss: 0.1547 (0.1674)  time: 0.1703\n",
      "Epoch: [16]  [3100/5008]  eta: 0:05:41  lr: 0.000500  loss_labels: 0.1608 (0.1674)  loss: 0.1470 (0.1674)  time: 0.1718\n",
      "Epoch: [16]  [3200/5008]  eta: 0:05:23  lr: 0.000500  loss_labels: 0.1441 (0.1670)  loss: 0.1712 (0.1670)  time: 0.1732\n",
      "Epoch: [16]  [3300/5008]  eta: 0:05:05  lr: 0.000500  loss_labels: 0.1479 (0.1666)  loss: 0.1309 (0.1666)  time: 0.1717\n",
      "Epoch: [16]  [3400/5008]  eta: 0:04:47  lr: 0.000500  loss_labels: 0.1560 (0.1663)  loss: 0.1529 (0.1663)  time: 0.1745\n",
      "Epoch: [16]  [3500/5008]  eta: 0:04:28  lr: 0.000500  loss_labels: 0.1647 (0.1666)  loss: 0.1760 (0.1666)  time: 0.1709\n",
      "Epoch: [16]  [3600/5008]  eta: 0:04:10  lr: 0.000500  loss_labels: 0.1554 (0.1666)  loss: 0.1705 (0.1666)  time: 0.1706\n",
      "Epoch: [16]  [3700/5008]  eta: 0:03:52  lr: 0.000500  loss_labels: 0.1514 (0.1663)  loss: 0.1352 (0.1663)  time: 0.1694\n",
      "Epoch: [16]  [3800/5008]  eta: 0:03:34  lr: 0.000500  loss_labels: 0.1477 (0.1660)  loss: 0.1227 (0.1660)  time: 0.1702\n",
      "Epoch: [16]  [3900/5008]  eta: 0:03:16  lr: 0.000500  loss_labels: 0.1680 (0.1662)  loss: 0.1680 (0.1662)  time: 0.1708\n",
      "Epoch: [16]  [4000/5008]  eta: 0:02:58  lr: 0.000500  loss_labels: 0.1630 (0.1661)  loss: 0.1364 (0.1661)  time: 0.1702\n",
      "Epoch: [16]  [4100/5008]  eta: 0:02:40  lr: 0.000500  loss_labels: 0.1603 (0.1660)  loss: 0.1537 (0.1660)  time: 0.1726\n",
      "Epoch: [16]  [4200/5008]  eta: 0:02:23  lr: 0.000500  loss_labels: 0.1520 (0.1657)  loss: 0.1485 (0.1657)  time: 0.1719\n",
      "Epoch: [16]  [4300/5008]  eta: 0:02:05  lr: 0.000500  loss_labels: 0.1551 (0.1654)  loss: 0.1640 (0.1654)  time: 0.1735\n",
      "Epoch: [16]  [4400/5008]  eta: 0:01:47  lr: 0.000500  loss_labels: 0.1663 (0.1657)  loss: 0.1778 (0.1657)  time: 0.1709\n",
      "Epoch: [16]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.1489 (0.1656)  loss: 0.1748 (0.1656)  time: 0.1884\n",
      "Epoch: [16]  [4600/5008]  eta: 0:01:12  lr: 0.000500  loss_labels: 0.1569 (0.1656)  loss: 0.1543 (0.1656)  time: 0.1696\n",
      "Epoch: [16]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.1428 (0.1653)  loss: 0.1553 (0.1653)  time: 0.1705\n",
      "Epoch: [16]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.1538 (0.1652)  loss: 0.1611 (0.1652)  time: 0.1716\n",
      "Epoch: [16]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.1614 (0.1651)  loss: 0.1372 (0.1651)  time: 0.1689\n",
      "Epoch: [16]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.1464 (0.1649)  loss: 0.1293 (0.1649)  time: 0.1716\n",
      "Epoch: [16]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.1453 (0.1649)  loss: 0.1342 (0.1649)  time: 0.1717\n",
      "Epoch: [16] Total time: 0:14:44 (0.1765 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.1453 (0.1649)  loss: 0.1342 (0.1649)\n",
      "Test:  [  0/565]  eta: 3:49:41  loss_labels: 0.9489 (0.9489)  loss: 0.9489 (0.9489)  time: 24.3920\n",
      "Test:  [100/565]  eta: 0:02:58  loss_labels: 1.2760 (1.4059)  loss: 1.3713 (1.4059)  time: 0.1432\n",
      "Test:  [200/565]  eta: 0:01:37  loss_labels: 1.1369 (1.3359)  loss: 1.3618 (1.3359)  time: 0.1326\n",
      "Test:  [300/565]  eta: 0:01:00  loss_labels: 1.0565 (1.2750)  loss: 0.9531 (1.2750)  time: 0.1592\n",
      "Test:  [400/565]  eta: 0:00:34  loss_labels: 1.3417 (1.2974)  loss: 1.2218 (1.2974)  time: 0.1327\n",
      "Test:  [500/565]  eta: 0:00:12  loss_labels: 1.0624 (1.2708)  loss: 1.1555 (1.2708)  time: 0.1502\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.0764 (1.2522)  loss: 1.0940 (1.2522)  time: 0.1411\n",
      "Test: Total time: 0:01:49 (0.1932 s / it)\n",
      "Averaged stats: loss_labels: 1.0764 (1.2522)  loss: 1.0940 (1.2522)\n",
      "acc: 0.7788733243942261\n",
      "top 1 and top 5 accuracies {'top1': 0.7788733174541628, 'top5': 0.8923032183016674, 'loss': tensor(0.0098, device='cuda:0')}\n",
      "Epoch: [17]  [   0/5008]  eta: 1 day, 16:36:08  lr: 0.000500  loss_labels: 0.2023 (0.2023)  loss: 0.2023 (0.2023)  time: 29.1871\n",
      "Epoch: [17]  [ 100/5008]  eta: 0:37:26  lr: 0.000500  loss_labels: 0.1655 (0.1669)  loss: 0.1546 (0.1669)  time: 0.1685\n",
      "Epoch: [17]  [ 200/5008]  eta: 0:25:39  lr: 0.000500  loss_labels: 0.1448 (0.1632)  loss: 0.1417 (0.1632)  time: 0.1849\n",
      "Epoch: [17]  [ 300/5008]  eta: 0:21:10  lr: 0.000500  loss_labels: 0.1489 (0.1626)  loss: 0.1784 (0.1626)  time: 0.1682\n",
      "Epoch: [17]  [ 400/5008]  eta: 0:18:51  lr: 0.000500  loss_labels: 0.1442 (0.1592)  loss: 0.1590 (0.1592)  time: 0.1887\n",
      "Epoch: [17]  [ 500/5008]  eta: 0:17:20  lr: 0.000500  loss_labels: 0.1368 (0.1563)  loss: 0.1279 (0.1563)  time: 0.1718\n",
      "Epoch: [17]  [ 600/5008]  eta: 0:16:13  lr: 0.000500  loss_labels: 0.1493 (0.1569)  loss: 0.1336 (0.1569)  time: 0.1704\n",
      "Epoch: [17]  [ 700/5008]  eta: 0:15:19  lr: 0.000500  loss_labels: 0.1631 (0.1574)  loss: 0.1776 (0.1574)  time: 0.1698\n",
      "Epoch: [17]  [ 800/5008]  eta: 0:14:35  lr: 0.000500  loss_labels: 0.1632 (0.1588)  loss: 0.1771 (0.1588)  time: 0.1705\n",
      "Epoch: [17]  [ 900/5008]  eta: 0:13:57  lr: 0.000500  loss_labels: 0.1553 (0.1581)  loss: 0.1376 (0.1581)  time: 0.1689\n",
      "Epoch: [17]  [1000/5008]  eta: 0:13:23  lr: 0.000500  loss_labels: 0.1457 (0.1574)  loss: 0.1332 (0.1574)  time: 0.1687\n",
      "Epoch: [17]  [1100/5008]  eta: 0:12:51  lr: 0.000500  loss_labels: 0.1445 (0.1564)  loss: 0.1424 (0.1564)  time: 0.1686\n",
      "Epoch: [17]  [1200/5008]  eta: 0:12:22  lr: 0.000500  loss_labels: 0.1481 (0.1561)  loss: 0.1635 (0.1561)  time: 0.1685\n",
      "Epoch: [17]  [1300/5008]  eta: 0:11:55  lr: 0.000500  loss_labels: 0.1334 (0.1558)  loss: 0.1396 (0.1558)  time: 0.1693\n",
      "Epoch: [17]  [1400/5008]  eta: 0:11:30  lr: 0.000500  loss_labels: 0.1441 (0.1558)  loss: 0.1737 (0.1558)  time: 0.1695\n",
      "Epoch: [17]  [1500/5008]  eta: 0:11:06  lr: 0.000500  loss_labels: 0.1517 (0.1559)  loss: 0.1281 (0.1559)  time: 0.1684\n",
      "Epoch: [17]  [1600/5008]  eta: 0:10:42  lr: 0.000500  loss_labels: 0.1464 (0.1556)  loss: 0.1268 (0.1556)  time: 0.1699\n",
      "Epoch: [17]  [1700/5008]  eta: 0:10:20  lr: 0.000500  loss_labels: 0.1529 (0.1557)  loss: 0.1485 (0.1557)  time: 0.1704\n",
      "Epoch: [17]  [1800/5008]  eta: 0:09:58  lr: 0.000500  loss_labels: 0.1431 (0.1551)  loss: 0.1294 (0.1551)  time: 0.1692\n",
      "Epoch: [17]  [1900/5008]  eta: 0:09:36  lr: 0.000500  loss_labels: 0.1572 (0.1552)  loss: 0.1669 (0.1552)  time: 0.1702\n",
      "Epoch: [17]  [2000/5008]  eta: 0:09:15  lr: 0.000500  loss_labels: 0.1479 (0.1551)  loss: 0.1147 (0.1551)  time: 0.1709\n",
      "Epoch: [17]  [2100/5008]  eta: 0:08:55  lr: 0.000500  loss_labels: 0.1346 (0.1544)  loss: 0.1416 (0.1544)  time: 0.1700\n",
      "Epoch: [17]  [2200/5008]  eta: 0:08:35  lr: 0.000500  loss_labels: 0.1386 (0.1542)  loss: 0.1316 (0.1542)  time: 0.1710\n",
      "Epoch: [17]  [2300/5008]  eta: 0:08:15  lr: 0.000500  loss_labels: 0.1537 (0.1546)  loss: 0.1513 (0.1546)  time: 0.1707\n",
      "Epoch: [17]  [2400/5008]  eta: 0:07:55  lr: 0.000500  loss_labels: 0.1453 (0.1545)  loss: 0.1450 (0.1545)  time: 0.1672\n",
      "Epoch: [17]  [2500/5008]  eta: 0:07:35  lr: 0.000500  loss_labels: 0.1458 (0.1542)  loss: 0.1351 (0.1542)  time: 0.1691\n",
      "Epoch: [17]  [2600/5008]  eta: 0:07:16  lr: 0.000500  loss_labels: 0.1445 (0.1543)  loss: 0.1425 (0.1543)  time: 0.1716\n",
      "Epoch: [17]  [2700/5008]  eta: 0:06:57  lr: 0.000500  loss_labels: 0.1445 (0.1541)  loss: 0.1457 (0.1541)  time: 0.1709\n",
      "Epoch: [17]  [2800/5008]  eta: 0:06:38  lr: 0.000500  loss_labels: 0.1529 (0.1540)  loss: 0.1322 (0.1540)  time: 0.1709\n",
      "Epoch: [17]  [2900/5008]  eta: 0:06:20  lr: 0.000500  loss_labels: 0.1383 (0.1537)  loss: 0.1282 (0.1537)  time: 0.1700\n",
      "Epoch: [17]  [3000/5008]  eta: 0:06:02  lr: 0.000500  loss_labels: 0.1432 (0.1536)  loss: 0.1432 (0.1536)  time: 0.1910\n",
      "Epoch: [17]  [3100/5008]  eta: 0:05:44  lr: 0.000500  loss_labels: 0.1401 (0.1534)  loss: 0.1444 (0.1534)  time: 0.1726\n",
      "Epoch: [17]  [3200/5008]  eta: 0:05:25  lr: 0.000500  loss_labels: 0.1346 (0.1530)  loss: 0.1411 (0.1530)  time: 0.1762\n",
      "Epoch: [17]  [3300/5008]  eta: 0:05:08  lr: 0.000500  loss_labels: 0.1387 (0.1526)  loss: 0.1300 (0.1526)  time: 0.1731\n",
      "Epoch: [17]  [3400/5008]  eta: 0:04:49  lr: 0.000500  loss_labels: 0.1385 (0.1523)  loss: 0.1309 (0.1523)  time: 0.1719\n",
      "Epoch: [17]  [3500/5008]  eta: 0:04:31  lr: 0.000500  loss_labels: 0.1481 (0.1524)  loss: 0.1628 (0.1524)  time: 0.1777\n",
      "Epoch: [17]  [3600/5008]  eta: 0:04:13  lr: 0.000500  loss_labels: 0.1375 (0.1523)  loss: 0.1580 (0.1523)  time: 0.1716\n",
      "Epoch: [17]  [3700/5008]  eta: 0:03:55  lr: 0.000500  loss_labels: 0.1351 (0.1520)  loss: 0.1373 (0.1520)  time: 0.1687\n",
      "Epoch: [17]  [3800/5008]  eta: 0:03:36  lr: 0.000500  loss_labels: 0.1382 (0.1517)  loss: 0.1406 (0.1517)  time: 0.1687\n",
      "Epoch: [17]  [3900/5008]  eta: 0:03:18  lr: 0.000500  loss_labels: 0.1531 (0.1518)  loss: 0.1503 (0.1518)  time: 0.1710\n",
      "Epoch: [17]  [4000/5008]  eta: 0:03:00  lr: 0.000500  loss_labels: 0.1360 (0.1517)  loss: 0.1289 (0.1517)  time: 0.1682\n",
      "Epoch: [17]  [4100/5008]  eta: 0:02:42  lr: 0.000500  loss_labels: 0.1430 (0.1515)  loss: 0.1499 (0.1515)  time: 0.1712\n",
      "Epoch: [17]  [4200/5008]  eta: 0:02:24  lr: 0.000500  loss_labels: 0.1350 (0.1513)  loss: 0.1174 (0.1513)  time: 0.1726\n",
      "Epoch: [17]  [4300/5008]  eta: 0:02:06  lr: 0.000500  loss_labels: 0.1412 (0.1512)  loss: 0.1336 (0.1512)  time: 0.1727\n",
      "Epoch: [17]  [4400/5008]  eta: 0:01:48  lr: 0.000500  loss_labels: 0.1486 (0.1514)  loss: 0.1556 (0.1514)  time: 0.1722\n",
      "Epoch: [17]  [4500/5008]  eta: 0:01:30  lr: 0.000500  loss_labels: 0.1423 (0.1513)  loss: 0.1625 (0.1513)  time: 0.1707\n",
      "Epoch: [17]  [4600/5008]  eta: 0:01:12  lr: 0.000500  loss_labels: 0.1530 (0.1514)  loss: 0.1382 (0.1514)  time: 0.1747\n",
      "Epoch: [17]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.1291 (0.1511)  loss: 0.1326 (0.1511)  time: 0.2141\n",
      "Epoch: [17]  [4800/5008]  eta: 0:00:37  lr: 0.000500  loss_labels: 0.1356 (0.1511)  loss: 0.1423 (0.1511)  time: 0.1839\n",
      "Epoch: [17]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.1410 (0.1510)  loss: 0.1279 (0.1510)  time: 0.2124\n",
      "Epoch: [17]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.1389 (0.1508)  loss: 0.1500 (0.1508)  time: 0.1812\n",
      "Epoch: [17]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.1350 (0.1508)  loss: 0.1153 (0.1508)  time: 0.2227\n",
      "Epoch: [17] Total time: 0:14:59 (0.1796 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.1350 (0.1508)  loss: 0.1153 (0.1508)\n",
      "Test:  [  0/565]  eta: 4:26:22  loss_labels: 1.1845 (1.1845)  loss: 1.1845 (1.1845)  time: 28.2871\n",
      "Test:  [100/565]  eta: 0:03:33  loss_labels: 1.3402 (1.4291)  loss: 1.3402 (1.4291)  time: 0.1903\n",
      "Test:  [200/565]  eta: 0:01:52  loss_labels: 1.1272 (1.3472)  loss: 1.4120 (1.3472)  time: 0.1263\n",
      "Test:  [300/565]  eta: 0:01:06  loss_labels: 1.0288 (1.2711)  loss: 0.8958 (1.2711)  time: 0.1432\n",
      "Test:  [400/565]  eta: 0:00:36  loss_labels: 1.3163 (1.2870)  loss: 1.2769 (1.2870)  time: 0.1289\n",
      "Test:  [500/565]  eta: 0:00:13  loss_labels: 1.0564 (1.2610)  loss: 1.1884 (1.2610)  time: 0.1268\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.0820 (1.2429)  loss: 1.0214 (1.2429)  time: 0.1397\n",
      "Test: Total time: 0:01:50 (0.1964 s / it)\n",
      "Averaged stats: loss_labels: 1.0820 (1.2429)  loss: 1.0214 (1.2429)\n",
      "acc: 0.7849110960960388\n",
      "top 1 and top 5 accuracies {'top1': 0.7849110951088462, 'top5': 0.8943942834985875, 'loss': tensor(0.0097, device='cuda:0')}\n",
      "Epoch: [18]  [   0/5008]  eta: 1 day, 6:43:02  lr: 0.000500  loss_labels: 0.2348 (0.2348)  loss: 0.2348 (0.2348)  time: 22.0812\n",
      "Epoch: [18]  [ 100/5008]  eta: 0:31:42  lr: 0.000500  loss_labels: 0.1410 (0.1455)  loss: 0.1337 (0.1455)  time: 0.1720\n",
      "Epoch: [18]  [ 200/5008]  eta: 0:22:25  lr: 0.000500  loss_labels: 0.1345 (0.1458)  loss: 0.1418 (0.1458)  time: 0.1727\n",
      "Epoch: [18]  [ 300/5008]  eta: 0:19:07  lr: 0.000500  loss_labels: 0.1340 (0.1457)  loss: 0.1660 (0.1457)  time: 0.1704\n",
      "Epoch: [18]  [ 400/5008]  eta: 0:17:19  lr: 0.000500  loss_labels: 0.1348 (0.1437)  loss: 0.1702 (0.1437)  time: 0.1726\n",
      "Epoch: [18]  [ 500/5008]  eta: 0:16:08  lr: 0.000500  loss_labels: 0.1280 (0.1426)  loss: 0.1252 (0.1426)  time: 0.1705\n",
      "Epoch: [18]  [ 600/5008]  eta: 0:15:14  lr: 0.000500  loss_labels: 0.1328 (0.1414)  loss: 0.1056 (0.1414)  time: 0.1711\n",
      "Epoch: [18]  [ 700/5008]  eta: 0:14:31  lr: 0.000500  loss_labels: 0.1279 (0.1411)  loss: 0.1409 (0.1411)  time: 0.1711\n",
      "Epoch: [18]  [ 800/5008]  eta: 0:13:54  lr: 0.000500  loss_labels: 0.1353 (0.1415)  loss: 0.1244 (0.1415)  time: 0.1712\n",
      "Epoch: [18]  [ 900/5008]  eta: 0:13:22  lr: 0.000500  loss_labels: 0.1307 (0.1411)  loss: 0.1284 (0.1411)  time: 0.1704\n",
      "Epoch: [18]  [1000/5008]  eta: 0:12:53  lr: 0.000500  loss_labels: 0.1228 (0.1405)  loss: 0.1268 (0.1405)  time: 0.1721\n",
      "Epoch: [18]  [1100/5008]  eta: 0:12:26  lr: 0.000500  loss_labels: 0.1328 (0.1398)  loss: 0.1349 (0.1398)  time: 0.1725\n",
      "Epoch: [18]  [1200/5008]  eta: 0:12:00  lr: 0.000500  loss_labels: 0.1377 (0.1401)  loss: 0.1582 (0.1401)  time: 0.1701\n",
      "Epoch: [18]  [1300/5008]  eta: 0:11:36  lr: 0.000500  loss_labels: 0.1287 (0.1400)  loss: 0.1263 (0.1400)  time: 0.1704\n",
      "Epoch: [18]  [1400/5008]  eta: 0:11:13  lr: 0.000500  loss_labels: 0.1342 (0.1400)  loss: 0.1342 (0.1400)  time: 0.1710\n",
      "Epoch: [18]  [1500/5008]  eta: 0:10:51  lr: 0.000500  loss_labels: 0.1300 (0.1399)  loss: 0.1038 (0.1399)  time: 0.1723\n",
      "Epoch: [18]  [1600/5008]  eta: 0:10:29  lr: 0.000500  loss_labels: 0.1358 (0.1400)  loss: 0.1409 (0.1400)  time: 0.1693\n",
      "Epoch: [18]  [1700/5008]  eta: 0:10:08  lr: 0.000500  loss_labels: 0.1346 (0.1398)  loss: 0.1387 (0.1398)  time: 0.1717\n",
      "Epoch: [18]  [1800/5008]  eta: 0:09:47  lr: 0.000500  loss_labels: 0.1277 (0.1397)  loss: 0.1154 (0.1397)  time: 0.1712\n",
      "Epoch: [18]  [1900/5008]  eta: 0:09:27  lr: 0.000500  loss_labels: 0.1249 (0.1392)  loss: 0.1643 (0.1392)  time: 0.1697\n",
      "Epoch: [18]  [2000/5008]  eta: 0:09:07  lr: 0.000500  loss_labels: 0.1314 (0.1390)  loss: 0.0992 (0.1390)  time: 0.1703\n",
      "Epoch: [18]  [2100/5008]  eta: 0:08:47  lr: 0.000500  loss_labels: 0.1350 (0.1389)  loss: 0.1286 (0.1389)  time: 0.1713\n",
      "Epoch: [18]  [2200/5008]  eta: 0:08:28  lr: 0.000500  loss_labels: 0.1299 (0.1387)  loss: 0.1352 (0.1387)  time: 0.1709\n",
      "Epoch: [18]  [2300/5008]  eta: 0:08:09  lr: 0.000500  loss_labels: 0.1393 (0.1389)  loss: 0.1256 (0.1389)  time: 0.1709\n",
      "Epoch: [18]  [2400/5008]  eta: 0:07:49  lr: 0.000500  loss_labels: 0.1351 (0.1390)  loss: 0.1488 (0.1390)  time: 0.1703\n",
      "Epoch: [18]  [2500/5008]  eta: 0:07:30  lr: 0.000500  loss_labels: 0.1188 (0.1388)  loss: 0.1112 (0.1388)  time: 0.1708\n",
      "Epoch: [18]  [2600/5008]  eta: 0:07:12  lr: 0.000500  loss_labels: 0.1274 (0.1388)  loss: 0.1211 (0.1388)  time: 0.1711\n",
      "Epoch: [18]  [2700/5008]  eta: 0:06:53  lr: 0.000500  loss_labels: 0.1417 (0.1390)  loss: 0.1473 (0.1390)  time: 0.1715\n",
      "Epoch: [18]  [2800/5008]  eta: 0:06:34  lr: 0.000500  loss_labels: 0.1252 (0.1389)  loss: 0.1216 (0.1389)  time: 0.1713\n",
      "Epoch: [18]  [2900/5008]  eta: 0:06:16  lr: 0.000500  loss_labels: 0.1314 (0.1387)  loss: 0.1271 (0.1387)  time: 0.1716\n",
      "Epoch: [18]  [3000/5008]  eta: 0:05:58  lr: 0.000500  loss_labels: 0.1300 (0.1386)  loss: 0.1396 (0.1386)  time: 0.1698\n",
      "Epoch: [18]  [3100/5008]  eta: 0:05:40  lr: 0.000500  loss_labels: 0.1225 (0.1385)  loss: 0.1207 (0.1385)  time: 0.1700\n",
      "Epoch: [18]  [3200/5008]  eta: 0:05:22  lr: 0.000500  loss_labels: 0.1289 (0.1384)  loss: 0.1474 (0.1384)  time: 0.1708\n",
      "Epoch: [18]  [3300/5008]  eta: 0:05:04  lr: 0.000500  loss_labels: 0.1284 (0.1383)  loss: 0.1112 (0.1383)  time: 0.1718\n",
      "Epoch: [18]  [3400/5008]  eta: 0:04:46  lr: 0.000500  loss_labels: 0.1305 (0.1381)  loss: 0.1197 (0.1381)  time: 0.1726\n",
      "Epoch: [18]  [3500/5008]  eta: 0:04:27  lr: 0.000500  loss_labels: 0.1407 (0.1384)  loss: 0.1526 (0.1384)  time: 0.1705\n",
      "Epoch: [18]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.1254 (0.1383)  loss: 0.1175 (0.1383)  time: 0.1750\n",
      "Epoch: [18]  [3700/5008]  eta: 0:03:52  lr: 0.000500  loss_labels: 0.1285 (0.1381)  loss: 0.1405 (0.1381)  time: 0.1732\n",
      "Epoch: [18]  [3800/5008]  eta: 0:03:34  lr: 0.000500  loss_labels: 0.1252 (0.1378)  loss: 0.1292 (0.1378)  time: 0.1727\n",
      "Epoch: [18]  [3900/5008]  eta: 0:03:16  lr: 0.000500  loss_labels: 0.1294 (0.1378)  loss: 0.1487 (0.1378)  time: 0.1741\n",
      "Epoch: [18]  [4000/5008]  eta: 0:02:58  lr: 0.000500  loss_labels: 0.1215 (0.1376)  loss: 0.1196 (0.1376)  time: 0.1726\n",
      "Epoch: [18]  [4100/5008]  eta: 0:02:40  lr: 0.000500  loss_labels: 0.1260 (0.1374)  loss: 0.1362 (0.1374)  time: 0.1704\n",
      "Epoch: [18]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.1234 (0.1372)  loss: 0.1202 (0.1372)  time: 0.1706\n",
      "Epoch: [18]  [4300/5008]  eta: 0:02:05  lr: 0.000500  loss_labels: 0.1233 (0.1370)  loss: 0.1184 (0.1370)  time: 0.1685\n",
      "Epoch: [18]  [4400/5008]  eta: 0:01:47  lr: 0.000500  loss_labels: 0.1368 (0.1372)  loss: 0.1463 (0.1372)  time: 0.1727\n",
      "Epoch: [18]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.1188 (0.1370)  loss: 0.1416 (0.1370)  time: 0.1729\n",
      "Epoch: [18]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.1309 (0.1370)  loss: 0.1245 (0.1370)  time: 0.1722\n",
      "Epoch: [18]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.1213 (0.1368)  loss: 0.1288 (0.1368)  time: 0.1695\n",
      "Epoch: [18]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.1236 (0.1367)  loss: 0.1251 (0.1367)  time: 0.1686\n",
      "Epoch: [18]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.1200 (0.1366)  loss: 0.1053 (0.1366)  time: 0.1718\n",
      "Epoch: [18]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.1227 (0.1364)  loss: 0.1094 (0.1364)  time: 0.1718\n",
      "Epoch: [18]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.1242 (0.1363)  loss: 0.1036 (0.1363)  time: 0.1705\n",
      "Epoch: [18] Total time: 0:14:41 (0.1761 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.1242 (0.1363)  loss: 0.1036 (0.1363)\n",
      "Test:  [  0/565]  eta: 2:35:38  loss_labels: 1.0141 (1.0141)  loss: 1.0141 (1.0141)  time: 16.5292\n",
      "Test:  [100/565]  eta: 0:02:11  loss_labels: 1.3763 (1.4451)  loss: 1.3369 (1.4451)  time: 0.1164\n",
      "Test:  [200/565]  eta: 0:01:14  loss_labels: 1.1770 (1.3665)  loss: 1.3156 (1.3665)  time: 0.1146\n",
      "Test:  [300/565]  eta: 0:00:46  loss_labels: 1.1063 (1.2938)  loss: 0.9631 (1.2938)  time: 0.1327\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.4184 (1.3170)  loss: 1.3198 (1.3170)  time: 0.1104\n",
      "Test:  [500/565]  eta: 0:00:09  loss_labels: 1.0959 (1.2908)  loss: 1.2397 (1.2908)  time: 0.1084\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1048 (1.2713)  loss: 1.1582 (1.2713)  time: 0.1467\n",
      "Test: Total time: 0:01:26 (0.1523 s / it)\n",
      "Averaged stats: loss_labels: 1.1048 (1.2713)  loss: 1.1582 (1.2713)\n",
      "acc: 0.7833877801895142\n",
      "top 1 and top 5 accuracies {'top1': 0.7833878025812885, 'top5': 0.8935495485514873, 'loss': tensor(0.0099, device='cuda:0')}\n",
      "Epoch: [19]  [   0/5008]  eta: 1 day, 5:38:53  lr: 0.000500  loss_labels: 0.1673 (0.1673)  loss: 0.1673 (0.1673)  time: 21.3125\n",
      "Epoch: [19]  [ 100/5008]  eta: 0:30:57  lr: 0.000500  loss_labels: 0.1230 (0.1316)  loss: 0.1198 (0.1316)  time: 0.1695\n",
      "Epoch: [19]  [ 200/5008]  eta: 0:21:59  lr: 0.000500  loss_labels: 0.1247 (0.1328)  loss: 0.1320 (0.1328)  time: 0.1693\n",
      "Epoch: [19]  [ 300/5008]  eta: 0:18:50  lr: 0.000500  loss_labels: 0.1209 (0.1314)  loss: 0.1376 (0.1314)  time: 0.1713\n",
      "Epoch: [19]  [ 400/5008]  eta: 0:17:07  lr: 0.000500  loss_labels: 0.1182 (0.1292)  loss: 0.1256 (0.1292)  time: 0.1734\n",
      "Epoch: [19]  [ 500/5008]  eta: 0:15:57  lr: 0.000500  loss_labels: 0.1151 (0.1280)  loss: 0.1083 (0.1280)  time: 0.1698\n",
      "Epoch: [19]  [ 600/5008]  eta: 0:15:06  lr: 0.000500  loss_labels: 0.1230 (0.1277)  loss: 0.1170 (0.1277)  time: 0.1708\n",
      "Epoch: [19]  [ 700/5008]  eta: 0:14:23  lr: 0.000500  loss_labels: 0.1246 (0.1279)  loss: 0.1385 (0.1279)  time: 0.1689\n",
      "Epoch: [19]  [ 800/5008]  eta: 0:13:47  lr: 0.000500  loss_labels: 0.1232 (0.1289)  loss: 0.1264 (0.1289)  time: 0.1697\n",
      "Epoch: [19]  [ 900/5008]  eta: 0:13:14  lr: 0.000500  loss_labels: 0.1234 (0.1291)  loss: 0.1117 (0.1291)  time: 0.1685\n",
      "Epoch: [19]  [1000/5008]  eta: 0:12:45  lr: 0.000500  loss_labels: 0.1227 (0.1296)  loss: 0.1246 (0.1296)  time: 0.1695\n",
      "Epoch: [19]  [1100/5008]  eta: 0:12:18  lr: 0.000500  loss_labels: 0.1116 (0.1285)  loss: 0.1184 (0.1285)  time: 0.1705\n",
      "Epoch: [19]  [1200/5008]  eta: 0:11:54  lr: 0.000500  loss_labels: 0.1185 (0.1282)  loss: 0.1197 (0.1282)  time: 0.1707\n",
      "Epoch: [19]  [1300/5008]  eta: 0:11:30  lr: 0.000500  loss_labels: 0.1221 (0.1280)  loss: 0.1231 (0.1280)  time: 0.1704\n",
      "Epoch: [19]  [1400/5008]  eta: 0:11:08  lr: 0.000500  loss_labels: 0.1250 (0.1280)  loss: 0.1186 (0.1280)  time: 0.1716\n",
      "Epoch: [19]  [1500/5008]  eta: 0:10:46  lr: 0.000500  loss_labels: 0.1251 (0.1283)  loss: 0.1035 (0.1283)  time: 0.1708\n",
      "Epoch: [19]  [1600/5008]  eta: 0:10:25  lr: 0.000500  loss_labels: 0.1121 (0.1279)  loss: 0.1261 (0.1279)  time: 0.1706\n",
      "Epoch: [19]  [1700/5008]  eta: 0:10:04  lr: 0.000500  loss_labels: 0.1239 (0.1278)  loss: 0.1216 (0.1278)  time: 0.1711\n",
      "Epoch: [19]  [1800/5008]  eta: 0:09:44  lr: 0.000500  loss_labels: 0.1110 (0.1274)  loss: 0.1185 (0.1274)  time: 0.1718\n",
      "Epoch: [19]  [1900/5008]  eta: 0:09:24  lr: 0.000500  loss_labels: 0.1184 (0.1275)  loss: 0.1156 (0.1275)  time: 0.1705\n",
      "Epoch: [19]  [2000/5008]  eta: 0:09:04  lr: 0.000500  loss_labels: 0.1140 (0.1275)  loss: 0.1063 (0.1275)  time: 0.1708\n",
      "Epoch: [19]  [2100/5008]  eta: 0:08:44  lr: 0.000500  loss_labels: 0.1167 (0.1274)  loss: 0.1251 (0.1274)  time: 0.1707\n",
      "Epoch: [19]  [2200/5008]  eta: 0:08:25  lr: 0.000500  loss_labels: 0.1161 (0.1270)  loss: 0.1172 (0.1270)  time: 0.1715\n",
      "Epoch: [19]  [2300/5008]  eta: 0:08:06  lr: 0.000500  loss_labels: 0.1184 (0.1270)  loss: 0.1054 (0.1270)  time: 0.1717\n",
      "Epoch: [19]  [2400/5008]  eta: 0:07:47  lr: 0.000500  loss_labels: 0.1209 (0.1271)  loss: 0.1326 (0.1271)  time: 0.1718\n",
      "Epoch: [19]  [2500/5008]  eta: 0:07:29  lr: 0.000500  loss_labels: 0.1202 (0.1269)  loss: 0.1202 (0.1269)  time: 0.1716\n",
      "Epoch: [19]  [2600/5008]  eta: 0:07:10  lr: 0.000500  loss_labels: 0.1211 (0.1272)  loss: 0.1093 (0.1272)  time: 0.1714\n",
      "Epoch: [19]  [2700/5008]  eta: 0:06:52  lr: 0.000500  loss_labels: 0.1203 (0.1271)  loss: 0.1127 (0.1271)  time: 0.1719\n",
      "Epoch: [19]  [2800/5008]  eta: 0:06:33  lr: 0.000500  loss_labels: 0.1226 (0.1273)  loss: 0.1189 (0.1273)  time: 0.1708\n",
      "Epoch: [19]  [2900/5008]  eta: 0:06:15  lr: 0.000500  loss_labels: 0.1114 (0.1270)  loss: 0.1120 (0.1270)  time: 0.1704\n",
      "Epoch: [19]  [3000/5008]  eta: 0:05:57  lr: 0.000500  loss_labels: 0.1273 (0.1271)  loss: 0.1273 (0.1271)  time: 0.1709\n",
      "Epoch: [19]  [3100/5008]  eta: 0:05:38  lr: 0.000500  loss_labels: 0.1113 (0.1269)  loss: 0.0930 (0.1269)  time: 0.1719\n",
      "Epoch: [19]  [3200/5008]  eta: 0:05:20  lr: 0.000500  loss_labels: 0.1176 (0.1268)  loss: 0.1364 (0.1268)  time: 0.1723\n",
      "Epoch: [19]  [3300/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.1112 (0.1266)  loss: 0.1188 (0.1266)  time: 0.1730\n",
      "Epoch: [19]  [3400/5008]  eta: 0:04:44  lr: 0.000500  loss_labels: 0.1178 (0.1264)  loss: 0.1219 (0.1264)  time: 0.1726\n",
      "Epoch: [19]  [3500/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.1183 (0.1265)  loss: 0.1479 (0.1265)  time: 0.1709\n",
      "Epoch: [19]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.1188 (0.1265)  loss: 0.1268 (0.1265)  time: 0.1719\n",
      "Epoch: [19]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 0.1067 (0.1261)  loss: 0.1066 (0.1261)  time: 0.1715\n",
      "Epoch: [19]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 0.1134 (0.1260)  loss: 0.0994 (0.1260)  time: 0.1710\n",
      "Epoch: [19]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.1187 (0.1260)  loss: 0.1125 (0.1260)  time: 0.1703\n",
      "Epoch: [19]  [4000/5008]  eta: 0:02:57  lr: 0.000500  loss_labels: 0.1175 (0.1260)  loss: 0.1170 (0.1260)  time: 0.1708\n",
      "Epoch: [19]  [4100/5008]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.1091 (0.1258)  loss: 0.1263 (0.1258)  time: 0.1714\n",
      "Epoch: [19]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.1187 (0.1257)  loss: 0.1106 (0.1257)  time: 0.1720\n",
      "Epoch: [19]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.1146 (0.1256)  loss: 0.1187 (0.1256)  time: 0.1705\n",
      "Epoch: [19]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 0.1175 (0.1257)  loss: 0.1130 (0.1257)  time: 0.1712\n",
      "Epoch: [19]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.1153 (0.1257)  loss: 0.1237 (0.1257)  time: 0.1705\n",
      "Epoch: [19]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.1162 (0.1257)  loss: 0.1190 (0.1257)  time: 0.1699\n",
      "Epoch: [19]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.1126 (0.1254)  loss: 0.1154 (0.1254)  time: 0.1738\n",
      "Epoch: [19]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.1139 (0.1254)  loss: 0.1185 (0.1254)  time: 0.1751\n",
      "Epoch: [19]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.1161 (0.1253)  loss: 0.1109 (0.1253)  time: 0.1754\n",
      "Epoch: [19]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.1167 (0.1252)  loss: 0.1167 (0.1252)  time: 0.1743\n",
      "Epoch: [19]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.1108 (0.1252)  loss: 0.1123 (0.1252)  time: 0.1733\n",
      "Epoch: [19] Total time: 0:14:39 (0.1756 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.1108 (0.1252)  loss: 0.1123 (0.1252)\n",
      "Test:  [  0/565]  eta: 2:33:12  loss_labels: 0.9611 (0.9611)  loss: 0.9611 (0.9611)  time: 16.2700\n",
      "Test:  [100/565]  eta: 0:02:09  loss_labels: 1.3407 (1.4324)  loss: 1.2624 (1.4324)  time: 0.1151\n",
      "Test:  [200/565]  eta: 0:01:13  loss_labels: 1.1541 (1.3489)  loss: 1.3492 (1.3489)  time: 0.1166\n",
      "Test:  [300/565]  eta: 0:00:46  loss_labels: 1.0395 (1.2832)  loss: 0.8922 (1.2832)  time: 0.1232\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.3503 (1.3051)  loss: 1.4175 (1.3051)  time: 0.1157\n",
      "Test:  [500/565]  eta: 0:00:09  loss_labels: 1.1030 (1.2804)  loss: 1.2700 (1.2804)  time: 0.1126\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1195 (1.2629)  loss: 1.0974 (1.2629)  time: 0.1340\n",
      "Test: Total time: 0:01:25 (0.1522 s / it)\n",
      "Averaged stats: loss_labels: 1.1195 (1.2629)  loss: 1.0974 (1.2629)\n",
      "acc: 0.785437285900116\n",
      "top 1 and top 5 accuracies {'top1': 0.7854373234365479, 'top5': 0.8947681825735335, 'loss': tensor(0.0099, device='cuda:0')}\n",
      "Epoch: [20]  [   0/5008]  eta: 1 day, 4:32:02  lr: 0.000500  loss_labels: 0.1553 (0.1553)  loss: 0.1553 (0.1553)  time: 20.5116\n",
      "Epoch: [20]  [ 100/5008]  eta: 0:30:17  lr: 0.000500  loss_labels: 0.1224 (0.1266)  loss: 0.1082 (0.1266)  time: 0.1690\n",
      "Epoch: [20]  [ 200/5008]  eta: 0:21:40  lr: 0.000500  loss_labels: 0.1060 (0.1224)  loss: 0.1317 (0.1224)  time: 0.1695\n",
      "Epoch: [20]  [ 300/5008]  eta: 0:18:35  lr: 0.000500  loss_labels: 0.1182 (0.1241)  loss: 0.1291 (0.1241)  time: 0.1696\n",
      "Epoch: [20]  [ 400/5008]  eta: 0:16:56  lr: 0.000500  loss_labels: 0.1105 (0.1216)  loss: 0.1201 (0.1216)  time: 0.1719\n",
      "Epoch: [20]  [ 500/5008]  eta: 0:15:50  lr: 0.000500  loss_labels: 0.1132 (0.1204)  loss: 0.1005 (0.1204)  time: 0.1719\n",
      "Epoch: [20]  [ 600/5008]  eta: 0:15:00  lr: 0.000500  loss_labels: 0.1110 (0.1195)  loss: 0.1094 (0.1195)  time: 0.1722\n",
      "Epoch: [20]  [ 700/5008]  eta: 0:14:20  lr: 0.000500  loss_labels: 0.1062 (0.1188)  loss: 0.1209 (0.1188)  time: 0.1726\n",
      "Epoch: [20]  [ 800/5008]  eta: 0:13:45  lr: 0.000500  loss_labels: 0.1127 (0.1194)  loss: 0.1118 (0.1194)  time: 0.1697\n",
      "Epoch: [20]  [ 900/5008]  eta: 0:13:13  lr: 0.000500  loss_labels: 0.1095 (0.1188)  loss: 0.1181 (0.1188)  time: 0.1710\n",
      "Epoch: [20]  [1000/5008]  eta: 0:12:45  lr: 0.000500  loss_labels: 0.1099 (0.1190)  loss: 0.1183 (0.1190)  time: 0.1702\n",
      "Epoch: [20]  [1100/5008]  eta: 0:12:18  lr: 0.000500  loss_labels: 0.1037 (0.1181)  loss: 0.0969 (0.1181)  time: 0.1707\n",
      "Epoch: [20]  [1200/5008]  eta: 0:11:53  lr: 0.000500  loss_labels: 0.1112 (0.1180)  loss: 0.1346 (0.1180)  time: 0.1710\n",
      "Epoch: [20]  [1300/5008]  eta: 0:11:30  lr: 0.000500  loss_labels: 0.1058 (0.1176)  loss: 0.0911 (0.1176)  time: 0.1695\n",
      "Epoch: [20]  [1400/5008]  eta: 0:11:07  lr: 0.000500  loss_labels: 0.1151 (0.1178)  loss: 0.1188 (0.1178)  time: 0.1697\n",
      "Epoch: [20]  [1500/5008]  eta: 0:10:45  lr: 0.000500  loss_labels: 0.1127 (0.1180)  loss: 0.0963 (0.1180)  time: 0.1724\n",
      "Epoch: [20]  [1600/5008]  eta: 0:10:24  lr: 0.000500  loss_labels: 0.1134 (0.1181)  loss: 0.1125 (0.1181)  time: 0.1728\n",
      "Epoch: [20]  [1700/5008]  eta: 0:10:03  lr: 0.000500  loss_labels: 0.1104 (0.1180)  loss: 0.1032 (0.1180)  time: 0.1704\n",
      "Epoch: [20]  [1800/5008]  eta: 0:09:43  lr: 0.000500  loss_labels: 0.1070 (0.1180)  loss: 0.0919 (0.1180)  time: 0.1686\n",
      "Epoch: [20]  [1900/5008]  eta: 0:09:22  lr: 0.000500  loss_labels: 0.1012 (0.1175)  loss: 0.1100 (0.1175)  time: 0.1684\n",
      "Epoch: [20]  [2000/5008]  eta: 0:09:02  lr: 0.000500  loss_labels: 0.1123 (0.1177)  loss: 0.0938 (0.1177)  time: 0.1691\n",
      "Epoch: [20]  [2100/5008]  eta: 0:08:43  lr: 0.000500  loss_labels: 0.1073 (0.1177)  loss: 0.1101 (0.1177)  time: 0.1701\n",
      "Epoch: [20]  [2200/5008]  eta: 0:08:23  lr: 0.000500  loss_labels: 0.1000 (0.1171)  loss: 0.1133 (0.1171)  time: 0.1689\n",
      "Epoch: [20]  [2300/5008]  eta: 0:08:04  lr: 0.000500  loss_labels: 0.1144 (0.1173)  loss: 0.0895 (0.1173)  time: 0.1692\n",
      "Epoch: [20]  [2400/5008]  eta: 0:07:45  lr: 0.000500  loss_labels: 0.1194 (0.1174)  loss: 0.1110 (0.1174)  time: 0.1694\n",
      "Epoch: [20]  [2500/5008]  eta: 0:07:27  lr: 0.000500  loss_labels: 0.1032 (0.1171)  loss: 0.1046 (0.1171)  time: 0.1714\n",
      "Epoch: [20]  [2600/5008]  eta: 0:07:08  lr: 0.000500  loss_labels: 0.1069 (0.1171)  loss: 0.0980 (0.1171)  time: 0.1693\n",
      "Epoch: [20]  [2700/5008]  eta: 0:06:49  lr: 0.000500  loss_labels: 0.1126 (0.1172)  loss: 0.1015 (0.1172)  time: 0.1697\n",
      "Epoch: [20]  [2800/5008]  eta: 0:06:31  lr: 0.000500  loss_labels: 0.1172 (0.1174)  loss: 0.1263 (0.1174)  time: 0.1702\n",
      "Epoch: [20]  [2900/5008]  eta: 0:06:13  lr: 0.000500  loss_labels: 0.1022 (0.1173)  loss: 0.0950 (0.1173)  time: 0.1685\n",
      "Epoch: [20]  [3000/5008]  eta: 0:05:54  lr: 0.000500  loss_labels: 0.1161 (0.1172)  loss: 0.1124 (0.1172)  time: 0.1683\n",
      "Epoch: [20]  [3100/5008]  eta: 0:05:36  lr: 0.000500  loss_labels: 0.1104 (0.1173)  loss: 0.1042 (0.1173)  time: 0.1702\n",
      "Epoch: [20]  [3200/5008]  eta: 0:05:18  lr: 0.000500  loss_labels: 0.1070 (0.1170)  loss: 0.1219 (0.1170)  time: 0.1727\n",
      "Epoch: [20]  [3300/5008]  eta: 0:05:00  lr: 0.000500  loss_labels: 0.1102 (0.1169)  loss: 0.1046 (0.1169)  time: 0.1699\n",
      "Epoch: [20]  [3400/5008]  eta: 0:04:43  lr: 0.000500  loss_labels: 0.1067 (0.1167)  loss: 0.1150 (0.1167)  time: 0.1704\n",
      "Epoch: [20]  [3500/5008]  eta: 0:04:25  lr: 0.000500  loss_labels: 0.1173 (0.1169)  loss: 0.1361 (0.1169)  time: 0.1698\n",
      "Epoch: [20]  [3600/5008]  eta: 0:04:07  lr: 0.000500  loss_labels: 0.1069 (0.1169)  loss: 0.1120 (0.1169)  time: 0.1700\n",
      "Epoch: [20]  [3700/5008]  eta: 0:03:49  lr: 0.000500  loss_labels: 0.1025 (0.1166)  loss: 0.0980 (0.1166)  time: 0.1743\n",
      "Epoch: [20]  [3800/5008]  eta: 0:03:32  lr: 0.000500  loss_labels: 0.1123 (0.1166)  loss: 0.1082 (0.1166)  time: 0.1737\n",
      "Epoch: [20]  [3900/5008]  eta: 0:03:14  lr: 0.000500  loss_labels: 0.1141 (0.1167)  loss: 0.0998 (0.1167)  time: 0.1721\n",
      "Epoch: [20]  [4000/5008]  eta: 0:02:56  lr: 0.000500  loss_labels: 0.1119 (0.1167)  loss: 0.1052 (0.1167)  time: 0.1739\n",
      "Epoch: [20]  [4100/5008]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.1199 (0.1167)  loss: 0.1393 (0.1167)  time: 0.1734\n",
      "Epoch: [20]  [4200/5008]  eta: 0:02:21  lr: 0.000500  loss_labels: 0.1013 (0.1164)  loss: 0.0896 (0.1164)  time: 0.1743\n",
      "Epoch: [20]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.1044 (0.1163)  loss: 0.1107 (0.1163)  time: 0.1722\n",
      "Epoch: [20]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 0.1140 (0.1164)  loss: 0.1374 (0.1164)  time: 0.1721\n",
      "Epoch: [20]  [4500/5008]  eta: 0:01:28  lr: 0.000500  loss_labels: 0.1023 (0.1163)  loss: 0.1207 (0.1163)  time: 0.1681\n",
      "Epoch: [20]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.0981 (0.1161)  loss: 0.0923 (0.1161)  time: 0.1713\n",
      "Epoch: [20]  [4700/5008]  eta: 0:00:53  lr: 0.000500  loss_labels: 0.1025 (0.1159)  loss: 0.0997 (0.1159)  time: 0.1720\n",
      "Epoch: [20]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.1023 (0.1157)  loss: 0.1115 (0.1157)  time: 0.1730\n",
      "Epoch: [20]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.1088 (0.1157)  loss: 0.1026 (0.1157)  time: 0.1714\n",
      "Epoch: [20]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.1034 (0.1156)  loss: 0.0966 (0.1156)  time: 0.1727\n",
      "Epoch: [20]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.1010 (0.1156)  loss: 0.0870 (0.1156)  time: 0.1720\n",
      "Epoch: [20] Total time: 0:14:35 (0.1749 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.1010 (0.1156)  loss: 0.0870 (0.1156)\n",
      "Test:  [  0/565]  eta: 2:34:46  loss_labels: 1.0696 (1.0696)  loss: 1.0696 (1.0696)  time: 16.4368\n",
      "Test:  [100/565]  eta: 0:02:11  loss_labels: 1.3890 (1.4550)  loss: 1.4162 (1.4550)  time: 0.1192\n",
      "Test:  [200/565]  eta: 0:01:13  loss_labels: 1.2002 (1.3696)  loss: 1.4854 (1.3696)  time: 0.1176\n",
      "Test:  [300/565]  eta: 0:00:46  loss_labels: 1.0399 (1.3018)  loss: 0.9242 (1.3018)  time: 0.1333\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.3448 (1.3220)  loss: 1.3959 (1.3220)  time: 0.1145\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.1029 (1.2998)  loss: 1.3206 (1.2998)  time: 0.1178\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1162 (1.2794)  loss: 1.0916 (1.2794)  time: 0.1196\n",
      "Test: Total time: 0:01:25 (0.1515 s / it)\n",
      "Averaged stats: loss_labels: 1.1162 (1.2794)  loss: 1.0916 (1.2794)\n",
      "acc: 0.7892178297042847\n",
      "top 1 and top 5 accuracies {'top1': 0.7892178585276686, 'top5': 0.8962222345316568, 'loss': tensor(0.0100, device='cuda:0')}\n",
      "Epoch: [21]  [   0/5008]  eta: 1 day, 5:10:17  lr: 0.000500  loss_labels: 0.1264 (0.1264)  loss: 0.1264 (0.1264)  time: 20.9699\n",
      "Epoch: [21]  [ 100/5008]  eta: 0:30:50  lr: 0.000500  loss_labels: 0.1009 (0.1108)  loss: 0.1022 (0.1108)  time: 0.1745\n",
      "Epoch: [21]  [ 200/5008]  eta: 0:22:06  lr: 0.000500  loss_labels: 0.1098 (0.1122)  loss: 0.1116 (0.1122)  time: 0.1739\n",
      "Epoch: [21]  [ 300/5008]  eta: 0:18:59  lr: 0.000500  loss_labels: 0.1069 (0.1121)  loss: 0.1106 (0.1121)  time: 0.1743\n",
      "Epoch: [21]  [ 400/5008]  eta: 0:17:17  lr: 0.000500  loss_labels: 0.0999 (0.1101)  loss: 0.1186 (0.1101)  time: 0.1746\n",
      "Epoch: [21]  [ 500/5008]  eta: 0:16:08  lr: 0.000500  loss_labels: 0.1092 (0.1106)  loss: 0.1080 (0.1106)  time: 0.1722\n",
      "Epoch: [21]  [ 600/5008]  eta: 0:15:16  lr: 0.000500  loss_labels: 0.1097 (0.1109)  loss: 0.1087 (0.1109)  time: 0.1722\n",
      "Epoch: [21]  [ 700/5008]  eta: 0:14:32  lr: 0.000500  loss_labels: 0.1075 (0.1115)  loss: 0.1167 (0.1115)  time: 0.1716\n",
      "Epoch: [21]  [ 800/5008]  eta: 0:13:55  lr: 0.000500  loss_labels: 0.1037 (0.1114)  loss: 0.0999 (0.1114)  time: 0.1692\n",
      "Epoch: [21]  [ 900/5008]  eta: 0:13:22  lr: 0.000500  loss_labels: 0.0995 (0.1110)  loss: 0.0981 (0.1110)  time: 0.1700\n",
      "Epoch: [21]  [1000/5008]  eta: 0:12:52  lr: 0.000500  loss_labels: 0.1044 (0.1106)  loss: 0.0947 (0.1106)  time: 0.1704\n",
      "Epoch: [21]  [1100/5008]  eta: 0:12:25  lr: 0.000500  loss_labels: 0.0959 (0.1097)  loss: 0.0987 (0.1097)  time: 0.1711\n",
      "Epoch: [21]  [1200/5008]  eta: 0:12:01  lr: 0.000500  loss_labels: 0.0954 (0.1092)  loss: 0.1079 (0.1092)  time: 0.1754\n",
      "Epoch: [21]  [1300/5008]  eta: 0:11:37  lr: 0.000500  loss_labels: 0.0980 (0.1090)  loss: 0.0907 (0.1090)  time: 0.1728\n",
      "Epoch: [21]  [1400/5008]  eta: 0:11:14  lr: 0.000500  loss_labels: 0.1114 (0.1094)  loss: 0.1307 (0.1094)  time: 0.1734\n",
      "Epoch: [21]  [1500/5008]  eta: 0:10:53  lr: 0.000500  loss_labels: 0.1043 (0.1097)  loss: 0.0801 (0.1097)  time: 0.1739\n",
      "Epoch: [21]  [1600/5008]  eta: 0:10:31  lr: 0.000500  loss_labels: 0.0953 (0.1092)  loss: 0.0902 (0.1092)  time: 0.1699\n",
      "Epoch: [21]  [1700/5008]  eta: 0:10:10  lr: 0.000500  loss_labels: 0.1012 (0.1090)  loss: 0.1019 (0.1090)  time: 0.1708\n",
      "Epoch: [21]  [1800/5008]  eta: 0:09:49  lr: 0.000500  loss_labels: 0.0913 (0.1085)  loss: 0.0827 (0.1085)  time: 0.1696\n",
      "Epoch: [21]  [1900/5008]  eta: 0:09:28  lr: 0.000500  loss_labels: 0.1005 (0.1085)  loss: 0.1045 (0.1085)  time: 0.1698\n",
      "Epoch: [21]  [2000/5008]  eta: 0:09:08  lr: 0.000500  loss_labels: 0.1068 (0.1085)  loss: 0.0784 (0.1085)  time: 0.1688\n",
      "Epoch: [21]  [2100/5008]  eta: 0:08:48  lr: 0.000500  loss_labels: 0.1033 (0.1084)  loss: 0.1074 (0.1084)  time: 0.1695\n",
      "Epoch: [21]  [2200/5008]  eta: 0:08:28  lr: 0.000500  loss_labels: 0.1052 (0.1084)  loss: 0.0975 (0.1084)  time: 0.1688\n",
      "Epoch: [21]  [2300/5008]  eta: 0:08:08  lr: 0.000500  loss_labels: 0.1012 (0.1083)  loss: 0.1057 (0.1083)  time: 0.1684\n",
      "Epoch: [21]  [2400/5008]  eta: 0:07:49  lr: 0.000500  loss_labels: 0.1016 (0.1082)  loss: 0.1145 (0.1082)  time: 0.1686\n",
      "Epoch: [21]  [2500/5008]  eta: 0:07:30  lr: 0.000500  loss_labels: 0.0984 (0.1079)  loss: 0.1017 (0.1079)  time: 0.1704\n",
      "Epoch: [21]  [2600/5008]  eta: 0:07:11  lr: 0.000500  loss_labels: 0.0985 (0.1079)  loss: 0.0891 (0.1079)  time: 0.1703\n",
      "Epoch: [21]  [2700/5008]  eta: 0:06:52  lr: 0.000500  loss_labels: 0.1007 (0.1077)  loss: 0.1127 (0.1077)  time: 0.1704\n",
      "Epoch: [21]  [2800/5008]  eta: 0:06:34  lr: 0.000500  loss_labels: 0.1036 (0.1079)  loss: 0.1060 (0.1079)  time: 0.1698\n",
      "Epoch: [21]  [2900/5008]  eta: 0:06:15  lr: 0.000500  loss_labels: 0.0989 (0.1078)  loss: 0.1135 (0.1078)  time: 0.1690\n",
      "Epoch: [21]  [3000/5008]  eta: 0:05:57  lr: 0.000500  loss_labels: 0.1039 (0.1079)  loss: 0.0893 (0.1079)  time: 0.1709\n",
      "Epoch: [21]  [3100/5008]  eta: 0:05:39  lr: 0.000500  loss_labels: 0.1005 (0.1080)  loss: 0.0909 (0.1080)  time: 0.1708\n",
      "Epoch: [21]  [3200/5008]  eta: 0:05:20  lr: 0.000500  loss_labels: 0.1016 (0.1079)  loss: 0.1282 (0.1079)  time: 0.1700\n",
      "Epoch: [21]  [3300/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.1026 (0.1078)  loss: 0.0817 (0.1078)  time: 0.1702\n",
      "Epoch: [21]  [3400/5008]  eta: 0:04:44  lr: 0.000500  loss_labels: 0.1025 (0.1078)  loss: 0.1073 (0.1078)  time: 0.1706\n",
      "Epoch: [21]  [3500/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.1028 (0.1079)  loss: 0.1263 (0.1079)  time: 0.1720\n",
      "Epoch: [21]  [3600/5008]  eta: 0:04:08  lr: 0.000500  loss_labels: 0.1129 (0.1080)  loss: 0.1045 (0.1080)  time: 0.1713\n",
      "Epoch: [21]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 0.0937 (0.1077)  loss: 0.0969 (0.1077)  time: 0.1723\n",
      "Epoch: [21]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 0.0972 (0.1076)  loss: 0.1041 (0.1076)  time: 0.1713\n",
      "Epoch: [21]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.1079 (0.1077)  loss: 0.0965 (0.1077)  time: 0.1695\n",
      "Epoch: [21]  [4000/5008]  eta: 0:02:57  lr: 0.000500  loss_labels: 0.0973 (0.1076)  loss: 0.0996 (0.1076)  time: 0.1699\n",
      "Epoch: [21]  [4100/5008]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.1055 (0.1076)  loss: 0.1269 (0.1076)  time: 0.1705\n",
      "Epoch: [21]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.0915 (0.1074)  loss: 0.0896 (0.1074)  time: 0.1698\n",
      "Epoch: [21]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.0923 (0.1071)  loss: 0.0922 (0.1071)  time: 0.1708\n",
      "Epoch: [21]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 0.1023 (0.1071)  loss: 0.0947 (0.1071)  time: 0.1726\n",
      "Epoch: [21]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.1014 (0.1071)  loss: 0.1080 (0.1071)  time: 0.1726\n",
      "Epoch: [21]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.1027 (0.1072)  loss: 0.1030 (0.1072)  time: 0.1728\n",
      "Epoch: [21]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.0970 (0.1070)  loss: 0.0928 (0.1070)  time: 0.1733\n",
      "Epoch: [21]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.0932 (0.1069)  loss: 0.1011 (0.1069)  time: 0.1714\n",
      "Epoch: [21]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.1047 (0.1069)  loss: 0.0876 (0.1069)  time: 0.1729\n",
      "Epoch: [21]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0930 (0.1067)  loss: 0.0835 (0.1067)  time: 0.1706\n",
      "Epoch: [21]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0928 (0.1067)  loss: 0.0835 (0.1067)  time: 0.1709\n",
      "Epoch: [21] Total time: 0:14:39 (0.1755 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0928 (0.1067)  loss: 0.0835 (0.1067)\n",
      "Test:  [  0/565]  eta: 2:48:08  loss_labels: 1.1831 (1.1831)  loss: 1.1831 (1.1831)  time: 17.8561\n",
      "Test:  [100/565]  eta: 0:02:18  loss_labels: 1.4166 (1.4870)  loss: 1.5273 (1.4870)  time: 0.1162\n",
      "Test:  [200/565]  eta: 0:01:17  loss_labels: 1.1737 (1.4067)  loss: 1.4298 (1.4067)  time: 0.1649\n",
      "Test:  [300/565]  eta: 0:00:48  loss_labels: 1.0737 (1.3361)  loss: 0.9120 (1.3361)  time: 0.1271\n",
      "Test:  [400/565]  eta: 0:00:27  loss_labels: 1.4239 (1.3649)  loss: 1.4325 (1.3649)  time: 0.1109\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.1477 (1.3398)  loss: 1.3338 (1.3398)  time: 0.1106\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1479 (1.3174)  loss: 1.0225 (1.3174)  time: 0.1199\n",
      "Test: Total time: 0:01:26 (0.1534 s / it)\n",
      "Averaged stats: loss_labels: 1.1479 (1.3174)  loss: 1.0225 (1.3174)\n",
      "acc: 0.7827507853507996\n",
      "top 1 and top 5 accuracies {'top1': 0.7827507893424915, 'top5': 0.89371572591813, 'loss': tensor(0.0103, device='cuda:0')}\n",
      "Epoch: [22]  [   0/5008]  eta: 1 day, 7:56:02  lr: 0.000500  loss_labels: 0.1714 (0.1714)  loss: 0.1714 (0.1714)  time: 22.9557\n",
      "Epoch: [22]  [ 100/5008]  eta: 0:32:36  lr: 0.000500  loss_labels: 0.1011 (0.1064)  loss: 0.0955 (0.1064)  time: 0.1727\n",
      "Epoch: [22]  [ 200/5008]  eta: 0:22:55  lr: 0.000500  loss_labels: 0.0960 (0.1058)  loss: 0.1021 (0.1058)  time: 0.1720\n",
      "Epoch: [22]  [ 300/5008]  eta: 0:19:26  lr: 0.000500  loss_labels: 0.0961 (0.1052)  loss: 0.0963 (0.1052)  time: 0.1726\n",
      "Epoch: [22]  [ 400/5008]  eta: 0:17:35  lr: 0.000500  loss_labels: 0.0931 (0.1039)  loss: 0.1145 (0.1039)  time: 0.1727\n",
      "Epoch: [22]  [ 500/5008]  eta: 0:16:23  lr: 0.000500  loss_labels: 0.0972 (0.1027)  loss: 0.0902 (0.1027)  time: 0.1714\n",
      "Epoch: [22]  [ 600/5008]  eta: 0:15:26  lr: 0.000500  loss_labels: 0.1030 (0.1029)  loss: 0.1022 (0.1029)  time: 0.1703\n",
      "Epoch: [22]  [ 700/5008]  eta: 0:14:41  lr: 0.000500  loss_labels: 0.0959 (0.1032)  loss: 0.1047 (0.1032)  time: 0.1685\n",
      "Epoch: [22]  [ 800/5008]  eta: 0:14:02  lr: 0.000500  loss_labels: 0.1004 (0.1035)  loss: 0.0883 (0.1035)  time: 0.1695\n",
      "Epoch: [22]  [ 900/5008]  eta: 0:13:30  lr: 0.000500  loss_labels: 0.0910 (0.1026)  loss: 0.0798 (0.1026)  time: 0.1740\n",
      "Epoch: [22]  [1000/5008]  eta: 0:13:02  lr: 0.000500  loss_labels: 0.0939 (0.1023)  loss: 0.0929 (0.1023)  time: 0.1790\n",
      "Epoch: [22]  [1100/5008]  eta: 0:12:35  lr: 0.000500  loss_labels: 0.0940 (0.1018)  loss: 0.1087 (0.1018)  time: 0.1754\n",
      "Epoch: [22]  [1200/5008]  eta: 0:12:10  lr: 0.000500  loss_labels: 0.0964 (0.1020)  loss: 0.1060 (0.1020)  time: 0.1765\n",
      "Epoch: [22]  [1300/5008]  eta: 0:11:46  lr: 0.000500  loss_labels: 0.0909 (0.1016)  loss: 0.0943 (0.1016)  time: 0.1734\n",
      "Epoch: [22]  [1400/5008]  eta: 0:11:23  lr: 0.000500  loss_labels: 0.0989 (0.1018)  loss: 0.1037 (0.1018)  time: 0.1746\n",
      "Epoch: [22]  [1500/5008]  eta: 0:11:01  lr: 0.000500  loss_labels: 0.0981 (0.1017)  loss: 0.0955 (0.1017)  time: 0.1757\n",
      "Epoch: [22]  [1600/5008]  eta: 0:10:40  lr: 0.000500  loss_labels: 0.0890 (0.1012)  loss: 0.0881 (0.1012)  time: 0.1758\n",
      "Epoch: [22]  [1700/5008]  eta: 0:10:18  lr: 0.000500  loss_labels: 0.0905 (0.1008)  loss: 0.0871 (0.1008)  time: 0.1748\n",
      "Epoch: [22]  [1800/5008]  eta: 0:09:58  lr: 0.000500  loss_labels: 0.0905 (0.1005)  loss: 0.0904 (0.1005)  time: 0.1760\n",
      "Epoch: [22]  [1900/5008]  eta: 0:09:37  lr: 0.000500  loss_labels: 0.0972 (0.1007)  loss: 0.1008 (0.1007)  time: 0.1704\n",
      "Epoch: [22]  [2000/5008]  eta: 0:09:16  lr: 0.000500  loss_labels: 0.0959 (0.1007)  loss: 0.0894 (0.1007)  time: 0.1712\n",
      "Epoch: [22]  [2100/5008]  eta: 0:08:56  lr: 0.000500  loss_labels: 0.0997 (0.1009)  loss: 0.1168 (0.1009)  time: 0.1727\n",
      "Epoch: [22]  [2200/5008]  eta: 0:08:36  lr: 0.000500  loss_labels: 0.0996 (0.1008)  loss: 0.0956 (0.1008)  time: 0.1735\n",
      "Epoch: [22]  [2300/5008]  eta: 0:08:16  lr: 0.000500  loss_labels: 0.0999 (0.1009)  loss: 0.0999 (0.1009)  time: 0.1731\n",
      "Epoch: [22]  [2400/5008]  eta: 0:07:57  lr: 0.000500  loss_labels: 0.0951 (0.1009)  loss: 0.0949 (0.1009)  time: 0.1711\n",
      "Epoch: [22]  [2500/5008]  eta: 0:07:37  lr: 0.000500  loss_labels: 0.0855 (0.1005)  loss: 0.0855 (0.1005)  time: 0.1725\n",
      "Epoch: [22]  [2600/5008]  eta: 0:07:18  lr: 0.000500  loss_labels: 0.0945 (0.1007)  loss: 0.0945 (0.1007)  time: 0.1734\n",
      "Epoch: [22]  [2700/5008]  eta: 0:06:59  lr: 0.000500  loss_labels: 0.0941 (0.1007)  loss: 0.0865 (0.1007)  time: 0.1742\n",
      "Epoch: [22]  [2800/5008]  eta: 0:06:40  lr: 0.000500  loss_labels: 0.0996 (0.1008)  loss: 0.1019 (0.1008)  time: 0.1749\n",
      "Epoch: [22]  [2900/5008]  eta: 0:06:22  lr: 0.000500  loss_labels: 0.0947 (0.1008)  loss: 0.1058 (0.1008)  time: 0.1752\n",
      "Epoch: [22]  [3000/5008]  eta: 0:06:03  lr: 0.000500  loss_labels: 0.0946 (0.1007)  loss: 0.0929 (0.1007)  time: 0.1742\n",
      "Epoch: [22]  [3100/5008]  eta: 0:05:45  lr: 0.000500  loss_labels: 0.0948 (0.1006)  loss: 0.0833 (0.1006)  time: 0.1733\n",
      "Epoch: [22]  [3200/5008]  eta: 0:05:26  lr: 0.000500  loss_labels: 0.0832 (0.1005)  loss: 0.1036 (0.1005)  time: 0.1735\n",
      "Epoch: [22]  [3300/5008]  eta: 0:05:08  lr: 0.000500  loss_labels: 0.0921 (0.1004)  loss: 0.0844 (0.1004)  time: 0.1741\n",
      "Epoch: [22]  [3400/5008]  eta: 0:04:49  lr: 0.000500  loss_labels: 0.0880 (0.1002)  loss: 0.0997 (0.1002)  time: 0.1731\n",
      "Epoch: [22]  [3500/5008]  eta: 0:04:31  lr: 0.000500  loss_labels: 0.0926 (0.1002)  loss: 0.0933 (0.1002)  time: 0.1718\n",
      "Epoch: [22]  [3600/5008]  eta: 0:04:13  lr: 0.000500  loss_labels: 0.0937 (0.1002)  loss: 0.1085 (0.1002)  time: 0.1736\n",
      "Epoch: [22]  [3700/5008]  eta: 0:03:54  lr: 0.000500  loss_labels: 0.0822 (0.0999)  loss: 0.0773 (0.0999)  time: 0.1747\n",
      "Epoch: [22]  [3800/5008]  eta: 0:03:36  lr: 0.000500  loss_labels: 0.0890 (0.0998)  loss: 0.0985 (0.0998)  time: 0.1749\n",
      "Epoch: [22]  [3900/5008]  eta: 0:03:18  lr: 0.000500  loss_labels: 0.0972 (0.0999)  loss: 0.1142 (0.0999)  time: 0.1709\n",
      "Epoch: [22]  [4000/5008]  eta: 0:03:00  lr: 0.000500  loss_labels: 0.0943 (0.0998)  loss: 0.0795 (0.0998)  time: 0.1725\n",
      "Epoch: [22]  [4100/5008]  eta: 0:02:42  lr: 0.000500  loss_labels: 0.0943 (0.0998)  loss: 0.1107 (0.0998)  time: 0.1731\n",
      "Epoch: [22]  [4200/5008]  eta: 0:02:24  lr: 0.000500  loss_labels: 0.0908 (0.0997)  loss: 0.0841 (0.0997)  time: 0.1690\n",
      "Epoch: [22]  [4300/5008]  eta: 0:02:06  lr: 0.000500  loss_labels: 0.0841 (0.0994)  loss: 0.0940 (0.0994)  time: 0.1728\n",
      "Epoch: [22]  [4400/5008]  eta: 0:01:48  lr: 0.000500  loss_labels: 0.0973 (0.0995)  loss: 0.0862 (0.0995)  time: 0.1718\n",
      "Epoch: [22]  [4500/5008]  eta: 0:01:30  lr: 0.000500  loss_labels: 0.0940 (0.0994)  loss: 0.1108 (0.0994)  time: 0.1732\n",
      "Epoch: [22]  [4600/5008]  eta: 0:01:12  lr: 0.000500  loss_labels: 0.0946 (0.0994)  loss: 0.0924 (0.0994)  time: 0.1722\n",
      "Epoch: [22]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.0888 (0.0993)  loss: 0.0927 (0.0993)  time: 0.1737\n",
      "Epoch: [22]  [4800/5008]  eta: 0:00:37  lr: 0.000500  loss_labels: 0.0866 (0.0991)  loss: 0.0854 (0.0991)  time: 0.1711\n",
      "Epoch: [22]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.0815 (0.0990)  loss: 0.0729 (0.0990)  time: 0.1716\n",
      "Epoch: [22]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0906 (0.0989)  loss: 0.0911 (0.0989)  time: 0.1715\n",
      "Epoch: [22]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0869 (0.0989)  loss: 0.0822 (0.0989)  time: 0.1710\n",
      "Epoch: [22] Total time: 0:14:50 (0.1779 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0869 (0.0989)  loss: 0.0822 (0.0989)\n",
      "Test:  [  0/565]  eta: 2:44:19  loss_labels: 1.1422 (1.1422)  loss: 1.1422 (1.1422)  time: 17.4503\n",
      "Test:  [100/565]  eta: 0:02:20  loss_labels: 1.3765 (1.5176)  loss: 1.3672 (1.5176)  time: 0.1414\n",
      "Test:  [200/565]  eta: 0:01:18  loss_labels: 1.2129 (1.4397)  loss: 1.5375 (1.4397)  time: 0.1231\n",
      "Test:  [300/565]  eta: 0:00:48  loss_labels: 1.0622 (1.3622)  loss: 1.0278 (1.3622)  time: 0.1236\n",
      "Test:  [400/565]  eta: 0:00:27  loss_labels: 1.4316 (1.3883)  loss: 1.4316 (1.3883)  time: 0.1166\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.1668 (1.3645)  loss: 1.3945 (1.3645)  time: 0.1173\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1950 (1.3415)  loss: 1.1723 (1.3415)  time: 0.1119\n",
      "Test: Total time: 0:01:27 (0.1546 s / it)\n",
      "Averaged stats: loss_labels: 1.1950 (1.3415)  loss: 1.1723 (1.3415)\n",
      "acc: 0.7862266302108765\n",
      "top 1 and top 5 accuracies {'top1': 0.7862266659281006, 'top5': 0.8944358278402481, 'loss': tensor(0.0105, device='cuda:0')}\n",
      "Epoch: [23]  [   0/5008]  eta: 1 day, 5:22:00  lr: 0.000500  loss_labels: 0.1570 (0.1570)  loss: 0.1570 (0.1570)  time: 21.1104\n",
      "Epoch: [23]  [ 100/5008]  eta: 0:30:52  lr: 0.000500  loss_labels: 0.0876 (0.0931)  loss: 0.0916 (0.0931)  time: 0.1706\n",
      "Epoch: [23]  [ 200/5008]  eta: 0:22:00  lr: 0.000500  loss_labels: 0.0824 (0.0920)  loss: 0.0897 (0.0920)  time: 0.1700\n",
      "Epoch: [23]  [ 300/5008]  eta: 0:18:50  lr: 0.000500  loss_labels: 0.0932 (0.0942)  loss: 0.1045 (0.0942)  time: 0.1709\n",
      "Epoch: [23]  [ 400/5008]  eta: 0:17:06  lr: 0.000500  loss_labels: 0.0811 (0.0929)  loss: 0.0945 (0.0929)  time: 0.1698\n",
      "Epoch: [23]  [ 500/5008]  eta: 0:15:56  lr: 0.000500  loss_labels: 0.0910 (0.0933)  loss: 0.1002 (0.0933)  time: 0.1700\n",
      "Epoch: [23]  [ 600/5008]  eta: 0:15:04  lr: 0.000500  loss_labels: 0.0862 (0.0933)  loss: 0.0902 (0.0933)  time: 0.1695\n",
      "Epoch: [23]  [ 700/5008]  eta: 0:14:21  lr: 0.000500  loss_labels: 0.0953 (0.0933)  loss: 0.0954 (0.0933)  time: 0.1690\n",
      "Epoch: [23]  [ 800/5008]  eta: 0:13:45  lr: 0.000500  loss_labels: 0.0944 (0.0939)  loss: 0.1096 (0.0939)  time: 0.1690\n",
      "Epoch: [23]  [ 900/5008]  eta: 0:13:14  lr: 0.000500  loss_labels: 0.0865 (0.0934)  loss: 0.0871 (0.0934)  time: 0.1701\n",
      "Epoch: [23]  [1000/5008]  eta: 0:12:45  lr: 0.000500  loss_labels: 0.0911 (0.0937)  loss: 0.0940 (0.0937)  time: 0.1704\n",
      "Epoch: [23]  [1100/5008]  eta: 0:12:18  lr: 0.000500  loss_labels: 0.0867 (0.0934)  loss: 0.0850 (0.0934)  time: 0.1731\n",
      "Epoch: [23]  [1200/5008]  eta: 0:11:53  lr: 0.000500  loss_labels: 0.0789 (0.0928)  loss: 0.0874 (0.0928)  time: 0.1699\n",
      "Epoch: [23]  [1300/5008]  eta: 0:11:30  lr: 0.000500  loss_labels: 0.0887 (0.0930)  loss: 0.0795 (0.0930)  time: 0.1692\n",
      "Epoch: [23]  [1400/5008]  eta: 0:11:07  lr: 0.000500  loss_labels: 0.0830 (0.0930)  loss: 0.0832 (0.0930)  time: 0.1713\n",
      "Epoch: [23]  [1500/5008]  eta: 0:10:46  lr: 0.000500  loss_labels: 0.0923 (0.0934)  loss: 0.1014 (0.0934)  time: 0.1706\n",
      "Epoch: [23]  [1600/5008]  eta: 0:10:25  lr: 0.000500  loss_labels: 0.0822 (0.0933)  loss: 0.0895 (0.0933)  time: 0.1720\n",
      "Epoch: [23]  [1700/5008]  eta: 0:10:04  lr: 0.000500  loss_labels: 0.0867 (0.0932)  loss: 0.0863 (0.0932)  time: 0.1714\n",
      "Epoch: [23]  [1800/5008]  eta: 0:09:44  lr: 0.000500  loss_labels: 0.0830 (0.0929)  loss: 0.0787 (0.0929)  time: 0.1713\n",
      "Epoch: [23]  [1900/5008]  eta: 0:09:23  lr: 0.000500  loss_labels: 0.0800 (0.0927)  loss: 0.0800 (0.0927)  time: 0.1711\n",
      "Epoch: [23]  [2000/5008]  eta: 0:09:04  lr: 0.000500  loss_labels: 0.0929 (0.0929)  loss: 0.0753 (0.0929)  time: 0.1706\n",
      "Epoch: [23]  [2100/5008]  eta: 0:08:44  lr: 0.000500  loss_labels: 0.0919 (0.0930)  loss: 0.0964 (0.0930)  time: 0.1705\n",
      "Epoch: [23]  [2200/5008]  eta: 0:08:25  lr: 0.000500  loss_labels: 0.0931 (0.0931)  loss: 0.0987 (0.0931)  time: 0.1714\n",
      "Epoch: [23]  [2300/5008]  eta: 0:08:06  lr: 0.000500  loss_labels: 0.0922 (0.0932)  loss: 0.0834 (0.0932)  time: 0.1714\n",
      "Epoch: [23]  [2400/5008]  eta: 0:07:47  lr: 0.000500  loss_labels: 0.0886 (0.0931)  loss: 0.0857 (0.0931)  time: 0.1702\n",
      "Epoch: [23]  [2500/5008]  eta: 0:07:28  lr: 0.000500  loss_labels: 0.0860 (0.0931)  loss: 0.0826 (0.0931)  time: 0.1708\n",
      "Epoch: [23]  [2600/5008]  eta: 0:07:10  lr: 0.000500  loss_labels: 0.0923 (0.0932)  loss: 0.0871 (0.0932)  time: 0.1718\n",
      "Epoch: [23]  [2700/5008]  eta: 0:06:51  lr: 0.000500  loss_labels: 0.0856 (0.0932)  loss: 0.0864 (0.0932)  time: 0.1725\n",
      "Epoch: [23]  [2800/5008]  eta: 0:06:33  lr: 0.000500  loss_labels: 0.0961 (0.0933)  loss: 0.1058 (0.0933)  time: 0.1733\n",
      "Epoch: [23]  [2900/5008]  eta: 0:06:15  lr: 0.000500  loss_labels: 0.0822 (0.0932)  loss: 0.0841 (0.0932)  time: 0.1730\n",
      "Epoch: [23]  [3000/5008]  eta: 0:05:57  lr: 0.000500  loss_labels: 0.0867 (0.0932)  loss: 0.0840 (0.0932)  time: 0.1723\n",
      "Epoch: [23]  [3100/5008]  eta: 0:05:39  lr: 0.000500  loss_labels: 0.0802 (0.0931)  loss: 0.0725 (0.0931)  time: 0.1726\n",
      "Epoch: [23]  [3200/5008]  eta: 0:05:21  lr: 0.000500  loss_labels: 0.0962 (0.0932)  loss: 0.1104 (0.0932)  time: 0.1730\n",
      "Epoch: [23]  [3300/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.0889 (0.0931)  loss: 0.0869 (0.0931)  time: 0.1704\n",
      "Epoch: [23]  [3400/5008]  eta: 0:04:44  lr: 0.000500  loss_labels: 0.0795 (0.0929)  loss: 0.0761 (0.0929)  time: 0.1705\n",
      "Epoch: [23]  [3500/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.0941 (0.0931)  loss: 0.0980 (0.0931)  time: 0.1704\n",
      "Epoch: [23]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.0859 (0.0930)  loss: 0.0807 (0.0930)  time: 0.1684\n",
      "Epoch: [23]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 0.0792 (0.0928)  loss: 0.0811 (0.0928)  time: 0.1694\n",
      "Epoch: [23]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 0.0801 (0.0926)  loss: 0.0785 (0.0926)  time: 0.1713\n",
      "Epoch: [23]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.0909 (0.0927)  loss: 0.0845 (0.0927)  time: 0.1710\n",
      "Epoch: [23]  [4000/5008]  eta: 0:02:57  lr: 0.000500  loss_labels: 0.0828 (0.0926)  loss: 0.0680 (0.0926)  time: 0.1691\n",
      "Epoch: [23]  [4100/5008]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.0857 (0.0925)  loss: 0.0857 (0.0925)  time: 0.1709\n",
      "Epoch: [23]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.0863 (0.0924)  loss: 0.0809 (0.0924)  time: 0.1722\n",
      "Epoch: [23]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.0833 (0.0923)  loss: 0.0916 (0.0923)  time: 0.1703\n",
      "Epoch: [23]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 0.0892 (0.0923)  loss: 0.0892 (0.0923)  time: 0.1704\n",
      "Epoch: [23]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.0856 (0.0923)  loss: 0.0941 (0.0923)  time: 0.1713\n",
      "Epoch: [23]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.0863 (0.0924)  loss: 0.0844 (0.0924)  time: 0.1711\n",
      "Epoch: [23]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.0877 (0.0924)  loss: 0.0867 (0.0924)  time: 0.1715\n",
      "Epoch: [23]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.0820 (0.0923)  loss: 0.0841 (0.0923)  time: 0.1717\n",
      "Epoch: [23]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0888 (0.0922)  loss: 0.0898 (0.0922)  time: 0.1709\n",
      "Epoch: [23]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0806 (0.0921)  loss: 0.0793 (0.0921)  time: 0.1738\n",
      "Epoch: [23]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0809 (0.0921)  loss: 0.0811 (0.0921)  time: 0.1734\n",
      "Epoch: [23] Total time: 0:14:38 (0.1754 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0809 (0.0921)  loss: 0.0811 (0.0921)\n",
      "Test:  [  0/565]  eta: 2:51:41  loss_labels: 1.2646 (1.2646)  loss: 1.2646 (1.2646)  time: 18.2321\n",
      "Test:  [100/565]  eta: 0:02:25  loss_labels: 1.3624 (1.4863)  loss: 1.3624 (1.4863)  time: 0.1326\n",
      "Test:  [200/565]  eta: 0:01:21  loss_labels: 1.1438 (1.3940)  loss: 1.4922 (1.3940)  time: 0.1211\n",
      "Test:  [300/565]  eta: 0:00:50  loss_labels: 1.0301 (1.3175)  loss: 0.9983 (1.3175)  time: 0.1325\n",
      "Test:  [400/565]  eta: 0:00:28  loss_labels: 1.3621 (1.3392)  loss: 1.2843 (1.3392)  time: 0.1143\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.1258 (1.3133)  loss: 1.3024 (1.3133)  time: 0.1107\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1343 (1.2917)  loss: 1.0614 (1.2917)  time: 0.1380\n",
      "Test: Total time: 0:01:30 (0.1605 s / it)\n",
      "Averaged stats: loss_labels: 1.1343 (1.2917)  loss: 1.0614 (1.2917)\n",
      "acc: 0.7952556014060974\n",
      "top 1 and top 5 accuracies {'top1': 0.795255636182352, 'top5': 0.8992688195867723, 'loss': tensor(0.0101, device='cuda:0')}\n",
      "Epoch: [24]  [   0/5008]  eta: 1 day, 4:34:25  lr: 0.000500  loss_labels: 0.0936 (0.0936)  loss: 0.0936 (0.0936)  time: 20.5403\n",
      "Epoch: [24]  [ 100/5008]  eta: 0:30:40  lr: 0.000500  loss_labels: 0.0821 (0.0876)  loss: 0.0848 (0.0876)  time: 0.1728\n",
      "Epoch: [24]  [ 200/5008]  eta: 0:21:58  lr: 0.000500  loss_labels: 0.0927 (0.0909)  loss: 0.1000 (0.0909)  time: 0.1720\n",
      "Epoch: [24]  [ 300/5008]  eta: 0:18:50  lr: 0.000500  loss_labels: 0.0788 (0.0895)  loss: 0.0952 (0.0895)  time: 0.1717\n",
      "Epoch: [24]  [ 400/5008]  eta: 0:17:08  lr: 0.000500  loss_labels: 0.0763 (0.0874)  loss: 0.0830 (0.0874)  time: 0.1716\n",
      "Epoch: [24]  [ 500/5008]  eta: 0:15:59  lr: 0.000500  loss_labels: 0.0895 (0.0883)  loss: 0.0784 (0.0883)  time: 0.1725\n",
      "Epoch: [24]  [ 600/5008]  eta: 0:15:08  lr: 0.000500  loss_labels: 0.0794 (0.0878)  loss: 0.0719 (0.0878)  time: 0.1719\n",
      "Epoch: [24]  [ 700/5008]  eta: 0:14:26  lr: 0.000500  loss_labels: 0.0784 (0.0875)  loss: 0.0822 (0.0875)  time: 0.1719\n",
      "Epoch: [24]  [ 800/5008]  eta: 0:13:51  lr: 0.000500  loss_labels: 0.0903 (0.0880)  loss: 0.0949 (0.0880)  time: 0.1734\n",
      "Epoch: [24]  [ 900/5008]  eta: 0:13:19  lr: 0.000500  loss_labels: 0.0826 (0.0878)  loss: 0.0775 (0.0878)  time: 0.1704\n",
      "Epoch: [24]  [1000/5008]  eta: 0:12:50  lr: 0.000500  loss_labels: 0.0812 (0.0876)  loss: 0.0767 (0.0876)  time: 0.1698\n",
      "Epoch: [24]  [1100/5008]  eta: 0:12:23  lr: 0.000500  loss_labels: 0.0829 (0.0872)  loss: 0.0835 (0.0872)  time: 0.1706\n",
      "Epoch: [24]  [1200/5008]  eta: 0:11:58  lr: 0.000500  loss_labels: 0.0811 (0.0873)  loss: 0.0807 (0.0873)  time: 0.1711\n",
      "Epoch: [24]  [1300/5008]  eta: 0:11:34  lr: 0.000500  loss_labels: 0.0826 (0.0872)  loss: 0.0833 (0.0872)  time: 0.1708\n",
      "Epoch: [24]  [1400/5008]  eta: 0:11:11  lr: 0.000500  loss_labels: 0.0777 (0.0869)  loss: 0.0800 (0.0869)  time: 0.1707\n",
      "Epoch: [24]  [1500/5008]  eta: 0:10:49  lr: 0.000500  loss_labels: 0.0846 (0.0874)  loss: 0.0787 (0.0874)  time: 0.1720\n",
      "Epoch: [24]  [1600/5008]  eta: 0:10:27  lr: 0.000500  loss_labels: 0.0822 (0.0875)  loss: 0.0796 (0.0875)  time: 0.1707\n",
      "Epoch: [24]  [1700/5008]  eta: 0:10:06  lr: 0.000500  loss_labels: 0.0834 (0.0874)  loss: 0.0841 (0.0874)  time: 0.1720\n",
      "Epoch: [24]  [1800/5008]  eta: 0:09:46  lr: 0.000500  loss_labels: 0.0813 (0.0872)  loss: 0.0852 (0.0872)  time: 0.1703\n",
      "Epoch: [24]  [1900/5008]  eta: 0:09:26  lr: 0.000500  loss_labels: 0.0803 (0.0871)  loss: 0.0803 (0.0871)  time: 0.1725\n",
      "Epoch: [24]  [2000/5008]  eta: 0:09:06  lr: 0.000500  loss_labels: 0.0806 (0.0870)  loss: 0.0702 (0.0870)  time: 0.1713\n",
      "Epoch: [24]  [2100/5008]  eta: 0:08:46  lr: 0.000500  loss_labels: 0.0790 (0.0868)  loss: 0.0802 (0.0868)  time: 0.1721\n",
      "Epoch: [24]  [2200/5008]  eta: 0:08:27  lr: 0.000500  loss_labels: 0.0852 (0.0868)  loss: 0.0877 (0.0868)  time: 0.1712\n",
      "Epoch: [24]  [2300/5008]  eta: 0:08:08  lr: 0.000500  loss_labels: 0.0812 (0.0868)  loss: 0.0769 (0.0868)  time: 0.1693\n",
      "Epoch: [24]  [2400/5008]  eta: 0:07:49  lr: 0.000500  loss_labels: 0.0750 (0.0867)  loss: 0.0952 (0.0867)  time: 0.1713\n",
      "Epoch: [24]  [2500/5008]  eta: 0:07:30  lr: 0.000500  loss_labels: 0.0770 (0.0866)  loss: 0.0673 (0.0866)  time: 0.1708\n",
      "Epoch: [24]  [2600/5008]  eta: 0:07:11  lr: 0.000500  loss_labels: 0.0836 (0.0867)  loss: 0.0762 (0.0867)  time: 0.1707\n",
      "Epoch: [24]  [2700/5008]  eta: 0:06:53  lr: 0.000500  loss_labels: 0.0759 (0.0866)  loss: 0.0685 (0.0866)  time: 0.1717\n",
      "Epoch: [24]  [2800/5008]  eta: 0:06:34  lr: 0.000500  loss_labels: 0.0834 (0.0867)  loss: 0.0656 (0.0867)  time: 0.1727\n",
      "Epoch: [24]  [2900/5008]  eta: 0:06:16  lr: 0.000500  loss_labels: 0.0737 (0.0865)  loss: 0.0693 (0.0865)  time: 0.1715\n",
      "Epoch: [24]  [3000/5008]  eta: 0:05:57  lr: 0.000500  loss_labels: 0.0830 (0.0865)  loss: 0.0918 (0.0865)  time: 0.1716\n",
      "Epoch: [24]  [3100/5008]  eta: 0:05:39  lr: 0.000500  loss_labels: 0.0776 (0.0864)  loss: 0.0776 (0.0864)  time: 0.1703\n",
      "Epoch: [24]  [3200/5008]  eta: 0:05:21  lr: 0.000500  loss_labels: 0.0800 (0.0865)  loss: 0.0800 (0.0865)  time: 0.1701\n",
      "Epoch: [24]  [3300/5008]  eta: 0:05:03  lr: 0.000500  loss_labels: 0.0790 (0.0864)  loss: 0.0789 (0.0864)  time: 0.1709\n",
      "Epoch: [24]  [3400/5008]  eta: 0:04:45  lr: 0.000500  loss_labels: 0.0782 (0.0862)  loss: 0.0782 (0.0862)  time: 0.1708\n",
      "Epoch: [24]  [3500/5008]  eta: 0:04:27  lr: 0.000500  loss_labels: 0.0848 (0.0864)  loss: 0.1029 (0.0864)  time: 0.1712\n",
      "Epoch: [24]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.0764 (0.0863)  loss: 0.0784 (0.0863)  time: 0.1701\n",
      "Epoch: [24]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 0.0715 (0.0860)  loss: 0.0779 (0.0860)  time: 0.1708\n",
      "Epoch: [24]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 0.0784 (0.0859)  loss: 0.0754 (0.0859)  time: 0.1696\n",
      "Epoch: [24]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.0827 (0.0860)  loss: 0.0766 (0.0860)  time: 0.1719\n",
      "Epoch: [24]  [4000/5008]  eta: 0:02:57  lr: 0.000500  loss_labels: 0.0785 (0.0860)  loss: 0.0726 (0.0860)  time: 0.1720\n",
      "Epoch: [24]  [4100/5008]  eta: 0:02:40  lr: 0.000500  loss_labels: 0.0858 (0.0860)  loss: 0.0879 (0.0860)  time: 0.1719\n",
      "Epoch: [24]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.0797 (0.0859)  loss: 0.0813 (0.0859)  time: 0.1727\n",
      "Epoch: [24]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.0742 (0.0858)  loss: 0.0742 (0.0858)  time: 0.1744\n",
      "Epoch: [24]  [4400/5008]  eta: 0:01:47  lr: 0.000500  loss_labels: 0.0812 (0.0858)  loss: 0.0716 (0.0858)  time: 0.1740\n",
      "Epoch: [24]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.0819 (0.0858)  loss: 0.0915 (0.0858)  time: 0.1725\n",
      "Epoch: [24]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.0737 (0.0857)  loss: 0.0708 (0.0857)  time: 0.1724\n",
      "Epoch: [24]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.0789 (0.0856)  loss: 0.0810 (0.0856)  time: 0.1708\n",
      "Epoch: [24]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.0772 (0.0856)  loss: 0.0741 (0.0856)  time: 0.1713\n",
      "Epoch: [24]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0844 (0.0856)  loss: 0.0855 (0.0856)  time: 0.1723\n",
      "Epoch: [24]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0781 (0.0856)  loss: 0.0688 (0.0856)  time: 0.1738\n",
      "Epoch: [24]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0757 (0.0855)  loss: 0.0660 (0.0855)  time: 0.1732\n",
      "Epoch: [24] Total time: 0:14:40 (0.1758 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0757 (0.0855)  loss: 0.0660 (0.0855)\n",
      "Test:  [  0/565]  eta: 2:32:34  loss_labels: 1.0061 (1.0061)  loss: 1.0061 (1.0061)  time: 16.2029\n",
      "Test:  [100/565]  eta: 0:02:12  loss_labels: 1.3822 (1.5084)  loss: 1.5489 (1.5084)  time: 0.1269\n",
      "Test:  [200/565]  eta: 0:01:15  loss_labels: 1.1863 (1.4010)  loss: 1.4862 (1.4010)  time: 0.1245\n",
      "Test:  [300/565]  eta: 0:00:47  loss_labels: 1.0327 (1.3339)  loss: 0.9823 (1.3339)  time: 0.1297\n",
      "Test:  [400/565]  eta: 0:00:27  loss_labels: 1.4110 (1.3593)  loss: 1.3150 (1.3593)  time: 0.1126\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.1424 (1.3357)  loss: 1.4639 (1.3357)  time: 0.1118\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1380 (1.3142)  loss: 1.1312 (1.3142)  time: 0.1241\n",
      "Test: Total time: 0:01:26 (0.1529 s / it)\n",
      "Averaged stats: loss_labels: 1.1380 (1.3142)  loss: 1.1312 (1.3142)\n",
      "acc: 0.7921813130378723\n",
      "top 1 and top 5 accuracies {'top1': 0.7921813548994627, 'top5': 0.8978563119703097, 'loss': tensor(0.0103, device='cuda:0')}\n",
      "Epoch: [25]  [   0/5008]  eta: 1 day, 5:39:00  lr: 0.000500  loss_labels: 0.1870 (0.1870)  loss: 0.1870 (0.1870)  time: 21.3140\n",
      "Epoch: [25]  [ 100/5008]  eta: 0:30:56  lr: 0.000500  loss_labels: 0.0793 (0.0854)  loss: 0.0769 (0.0854)  time: 0.1691\n",
      "Epoch: [25]  [ 200/5008]  eta: 0:21:58  lr: 0.000500  loss_labels: 0.0846 (0.0861)  loss: 0.0957 (0.0861)  time: 0.1688\n",
      "Epoch: [25]  [ 300/5008]  eta: 0:18:48  lr: 0.000500  loss_labels: 0.0825 (0.0866)  loss: 0.0847 (0.0866)  time: 0.1708\n",
      "Epoch: [25]  [ 400/5008]  eta: 0:17:03  lr: 0.000500  loss_labels: 0.0780 (0.0849)  loss: 0.0852 (0.0849)  time: 0.1692\n",
      "Epoch: [25]  [ 500/5008]  eta: 0:15:54  lr: 0.000500  loss_labels: 0.0771 (0.0844)  loss: 0.0712 (0.0844)  time: 0.1713\n",
      "Epoch: [25]  [ 600/5008]  eta: 0:15:03  lr: 0.000500  loss_labels: 0.0785 (0.0840)  loss: 0.0668 (0.0840)  time: 0.1707\n",
      "Epoch: [25]  [ 700/5008]  eta: 0:14:22  lr: 0.000500  loss_labels: 0.0816 (0.0840)  loss: 0.0742 (0.0840)  time: 0.1710\n",
      "Epoch: [25]  [ 800/5008]  eta: 0:13:46  lr: 0.000500  loss_labels: 0.0755 (0.0839)  loss: 0.0787 (0.0839)  time: 0.1707\n",
      "Epoch: [25]  [ 900/5008]  eta: 0:13:15  lr: 0.000500  loss_labels: 0.0848 (0.0842)  loss: 0.0815 (0.0842)  time: 0.1703\n",
      "Epoch: [25]  [1000/5008]  eta: 0:12:47  lr: 0.000500  loss_labels: 0.0752 (0.0835)  loss: 0.0513 (0.0835)  time: 0.1700\n",
      "Epoch: [25]  [1100/5008]  eta: 0:12:20  lr: 0.000500  loss_labels: 0.0723 (0.0826)  loss: 0.0633 (0.0826)  time: 0.1699\n",
      "Epoch: [25]  [1200/5008]  eta: 0:11:55  lr: 0.000500  loss_labels: 0.0698 (0.0821)  loss: 0.0784 (0.0821)  time: 0.1700\n",
      "Epoch: [25]  [1300/5008]  eta: 0:11:32  lr: 0.000500  loss_labels: 0.0711 (0.0819)  loss: 0.0774 (0.0819)  time: 0.1711\n",
      "Epoch: [25]  [1400/5008]  eta: 0:11:09  lr: 0.000500  loss_labels: 0.0729 (0.0816)  loss: 0.0719 (0.0816)  time: 0.1730\n",
      "Epoch: [25]  [1500/5008]  eta: 0:10:47  lr: 0.000500  loss_labels: 0.0830 (0.0818)  loss: 0.0824 (0.0818)  time: 0.1714\n",
      "Epoch: [25]  [1600/5008]  eta: 0:10:26  lr: 0.000500  loss_labels: 0.0772 (0.0820)  loss: 0.0706 (0.0820)  time: 0.1691\n",
      "Epoch: [25]  [1700/5008]  eta: 0:10:05  lr: 0.000500  loss_labels: 0.0800 (0.0821)  loss: 0.0724 (0.0821)  time: 0.1731\n",
      "Epoch: [25]  [1800/5008]  eta: 0:09:44  lr: 0.000500  loss_labels: 0.0781 (0.0821)  loss: 0.0685 (0.0821)  time: 0.1712\n",
      "Epoch: [25]  [1900/5008]  eta: 0:09:25  lr: 0.000500  loss_labels: 0.0781 (0.0821)  loss: 0.0759 (0.0821)  time: 0.1718\n",
      "Epoch: [25]  [2000/5008]  eta: 0:09:05  lr: 0.000500  loss_labels: 0.0728 (0.0820)  loss: 0.0668 (0.0820)  time: 0.1707\n",
      "Epoch: [25]  [2100/5008]  eta: 0:08:45  lr: 0.000500  loss_labels: 0.0747 (0.0821)  loss: 0.0681 (0.0821)  time: 0.1716\n",
      "Epoch: [25]  [2200/5008]  eta: 0:08:26  lr: 0.000500  loss_labels: 0.0727 (0.0818)  loss: 0.0783 (0.0818)  time: 0.1728\n",
      "Epoch: [25]  [2300/5008]  eta: 0:08:07  lr: 0.000500  loss_labels: 0.0802 (0.0818)  loss: 0.0826 (0.0818)  time: 0.1707\n",
      "Epoch: [25]  [2400/5008]  eta: 0:07:48  lr: 0.000500  loss_labels: 0.0770 (0.0817)  loss: 0.0771 (0.0817)  time: 0.1720\n",
      "Epoch: [25]  [2500/5008]  eta: 0:07:30  lr: 0.000500  loss_labels: 0.0744 (0.0817)  loss: 0.0691 (0.0817)  time: 0.1703\n",
      "Epoch: [25]  [2600/5008]  eta: 0:07:11  lr: 0.000500  loss_labels: 0.0765 (0.0817)  loss: 0.0695 (0.0817)  time: 0.1700\n",
      "Epoch: [25]  [2700/5008]  eta: 0:06:52  lr: 0.000500  loss_labels: 0.0811 (0.0816)  loss: 0.0902 (0.0816)  time: 0.1704\n",
      "Epoch: [25]  [2800/5008]  eta: 0:06:34  lr: 0.000500  loss_labels: 0.0749 (0.0816)  loss: 0.0746 (0.0816)  time: 0.1707\n",
      "Epoch: [25]  [2900/5008]  eta: 0:06:15  lr: 0.000500  loss_labels: 0.0784 (0.0815)  loss: 0.0711 (0.0815)  time: 0.1705\n",
      "Epoch: [25]  [3000/5008]  eta: 0:05:57  lr: 0.000500  loss_labels: 0.0739 (0.0815)  loss: 0.0822 (0.0815)  time: 0.1697\n",
      "Epoch: [25]  [3100/5008]  eta: 0:05:39  lr: 0.000500  loss_labels: 0.0795 (0.0814)  loss: 0.0711 (0.0814)  time: 0.1707\n",
      "Epoch: [25]  [3200/5008]  eta: 0:05:21  lr: 0.000500  loss_labels: 0.0744 (0.0814)  loss: 0.0935 (0.0814)  time: 0.1729\n",
      "Epoch: [25]  [3300/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.0747 (0.0814)  loss: 0.0768 (0.0814)  time: 0.1713\n",
      "Epoch: [25]  [3400/5008]  eta: 0:04:44  lr: 0.000500  loss_labels: 0.0730 (0.0812)  loss: 0.0848 (0.0812)  time: 0.1712\n",
      "Epoch: [25]  [3500/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.0753 (0.0812)  loss: 0.0777 (0.0812)  time: 0.1720\n",
      "Epoch: [25]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.0784 (0.0812)  loss: 0.0785 (0.0812)  time: 0.1711\n",
      "Epoch: [25]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 0.0761 (0.0812)  loss: 0.0881 (0.0812)  time: 0.1707\n",
      "Epoch: [25]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 0.0749 (0.0811)  loss: 0.0721 (0.0811)  time: 0.1719\n",
      "Epoch: [25]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.0759 (0.0811)  loss: 0.0588 (0.0811)  time: 0.1710\n",
      "Epoch: [25]  [4000/5008]  eta: 0:02:57  lr: 0.000500  loss_labels: 0.0662 (0.0810)  loss: 0.0637 (0.0810)  time: 0.1706\n",
      "Epoch: [25]  [4100/5008]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.0761 (0.0809)  loss: 0.0793 (0.0809)  time: 0.1703\n",
      "Epoch: [25]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.0749 (0.0808)  loss: 0.0857 (0.0808)  time: 0.1713\n",
      "Epoch: [25]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.0751 (0.0808)  loss: 0.0677 (0.0808)  time: 0.1711\n",
      "Epoch: [25]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 0.0758 (0.0809)  loss: 0.0840 (0.0809)  time: 0.1705\n",
      "Epoch: [25]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.0701 (0.0808)  loss: 0.0756 (0.0808)  time: 0.1731\n",
      "Epoch: [25]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.0789 (0.0808)  loss: 0.0725 (0.0808)  time: 0.1714\n",
      "Epoch: [25]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.0707 (0.0807)  loss: 0.0719 (0.0807)  time: 0.1709\n",
      "Epoch: [25]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.0714 (0.0806)  loss: 0.0804 (0.0806)  time: 0.1708\n",
      "Epoch: [25]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0749 (0.0806)  loss: 0.0681 (0.0806)  time: 0.1710\n",
      "Epoch: [25]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0729 (0.0805)  loss: 0.0676 (0.0805)  time: 0.1739\n",
      "Epoch: [25]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0726 (0.0805)  loss: 0.0741 (0.0805)  time: 0.1736\n",
      "Epoch: [25] Total time: 0:14:38 (0.1755 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0726 (0.0805)  loss: 0.0741 (0.0805)\n",
      "Test:  [  0/565]  eta: 2:22:39  loss_labels: 0.9671 (0.9671)  loss: 0.9671 (0.9671)  time: 15.1500\n",
      "Test:  [100/565]  eta: 0:02:06  loss_labels: 1.4278 (1.5663)  loss: 1.5270 (1.5663)  time: 0.1253\n",
      "Test:  [200/565]  eta: 0:01:11  loss_labels: 1.2685 (1.4724)  loss: 1.4175 (1.4724)  time: 0.1189\n",
      "Test:  [300/565]  eta: 0:00:45  loss_labels: 1.1375 (1.3842)  loss: 1.0424 (1.3842)  time: 0.1190\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.4073 (1.4056)  loss: 1.3914 (1.4056)  time: 0.1092\n",
      "Test:  [500/565]  eta: 0:00:09  loss_labels: 1.2041 (1.3772)  loss: 1.3832 (1.3772)  time: 0.1112\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1425 (1.3520)  loss: 1.0295 (1.3520)  time: 0.1347\n",
      "Test: Total time: 0:01:23 (0.1487 s / it)\n",
      "Averaged stats: loss_labels: 1.1425 (1.3520)  loss: 1.0295 (1.3520)\n",
      "acc: 0.791364312171936\n",
      "top 1 and top 5 accuracies {'top1': 0.7913643161801363, 'top5': 0.8965545892649421, 'loss': tensor(0.0106, device='cuda:0')}\n",
      "Epoch: [26]  [   0/5008]  eta: 1 day, 3:44:01  lr: 0.000500  loss_labels: 0.1525 (0.1525)  loss: 0.1525 (0.1525)  time: 19.9365\n",
      "Epoch: [26]  [ 100/5008]  eta: 0:29:56  lr: 0.000500  loss_labels: 0.0739 (0.0780)  loss: 0.0597 (0.0780)  time: 0.1701\n",
      "Epoch: [26]  [ 200/5008]  eta: 0:21:28  lr: 0.000500  loss_labels: 0.0706 (0.0766)  loss: 0.0607 (0.0766)  time: 0.1687\n",
      "Epoch: [26]  [ 300/5008]  eta: 0:18:27  lr: 0.000500  loss_labels: 0.0799 (0.0794)  loss: 0.0809 (0.0794)  time: 0.1696\n",
      "Epoch: [26]  [ 400/5008]  eta: 0:16:49  lr: 0.000500  loss_labels: 0.0712 (0.0781)  loss: 0.0794 (0.0781)  time: 0.1696\n",
      "Epoch: [26]  [ 500/5008]  eta: 0:15:43  lr: 0.000500  loss_labels: 0.0706 (0.0778)  loss: 0.0585 (0.0778)  time: 0.1702\n",
      "Epoch: [26]  [ 600/5008]  eta: 0:14:53  lr: 0.000500  loss_labels: 0.0756 (0.0778)  loss: 0.0736 (0.0778)  time: 0.1702\n",
      "Epoch: [26]  [ 700/5008]  eta: 0:14:14  lr: 0.000500  loss_labels: 0.0691 (0.0775)  loss: 0.0684 (0.0775)  time: 0.1725\n",
      "Epoch: [26]  [ 800/5008]  eta: 0:13:39  lr: 0.000500  loss_labels: 0.0698 (0.0773)  loss: 0.0782 (0.0773)  time: 0.1702\n",
      "Epoch: [26]  [ 900/5008]  eta: 0:13:09  lr: 0.000500  loss_labels: 0.0754 (0.0774)  loss: 0.0733 (0.0774)  time: 0.1695\n",
      "Epoch: [26]  [1000/5008]  eta: 0:12:41  lr: 0.000500  loss_labels: 0.0663 (0.0771)  loss: 0.0775 (0.0771)  time: 0.1701\n",
      "Epoch: [26]  [1100/5008]  eta: 0:12:14  lr: 0.000500  loss_labels: 0.0730 (0.0769)  loss: 0.0696 (0.0769)  time: 0.1690\n",
      "Epoch: [26]  [1200/5008]  eta: 0:11:50  lr: 0.000500  loss_labels: 0.0728 (0.0767)  loss: 0.0747 (0.0767)  time: 0.1709\n",
      "Epoch: [26]  [1300/5008]  eta: 0:11:27  lr: 0.000500  loss_labels: 0.0655 (0.0764)  loss: 0.0655 (0.0764)  time: 0.1702\n",
      "Epoch: [26]  [1400/5008]  eta: 0:11:04  lr: 0.000500  loss_labels: 0.0715 (0.0766)  loss: 0.0858 (0.0766)  time: 0.1697\n",
      "Epoch: [26]  [1500/5008]  eta: 0:10:42  lr: 0.000500  loss_labels: 0.0702 (0.0766)  loss: 0.0669 (0.0766)  time: 0.1691\n",
      "Epoch: [26]  [1600/5008]  eta: 0:10:21  lr: 0.000500  loss_labels: 0.0722 (0.0764)  loss: 0.0746 (0.0764)  time: 0.1707\n",
      "Epoch: [26]  [1700/5008]  eta: 0:10:01  lr: 0.000500  loss_labels: 0.0741 (0.0764)  loss: 0.0743 (0.0764)  time: 0.1706\n",
      "Epoch: [26]  [1800/5008]  eta: 0:09:41  lr: 0.000500  loss_labels: 0.0682 (0.0762)  loss: 0.0659 (0.0762)  time: 0.1717\n",
      "Epoch: [26]  [1900/5008]  eta: 0:09:21  lr: 0.000500  loss_labels: 0.0722 (0.0762)  loss: 0.0725 (0.0762)  time: 0.1696\n",
      "Epoch: [26]  [2000/5008]  eta: 0:09:01  lr: 0.000500  loss_labels: 0.0771 (0.0764)  loss: 0.0638 (0.0764)  time: 0.1714\n",
      "Epoch: [26]  [2100/5008]  eta: 0:08:42  lr: 0.000500  loss_labels: 0.0683 (0.0763)  loss: 0.0683 (0.0763)  time: 0.1702\n",
      "Epoch: [26]  [2200/5008]  eta: 0:08:23  lr: 0.000500  loss_labels: 0.0730 (0.0762)  loss: 0.0736 (0.0762)  time: 0.1711\n",
      "Epoch: [26]  [2300/5008]  eta: 0:08:04  lr: 0.000500  loss_labels: 0.0771 (0.0764)  loss: 0.0688 (0.0764)  time: 0.1695\n",
      "Epoch: [26]  [2400/5008]  eta: 0:07:45  lr: 0.000500  loss_labels: 0.0709 (0.0762)  loss: 0.0686 (0.0762)  time: 0.1699\n",
      "Epoch: [26]  [2500/5008]  eta: 0:07:26  lr: 0.000500  loss_labels: 0.0699 (0.0760)  loss: 0.0674 (0.0760)  time: 0.1692\n",
      "Epoch: [26]  [2600/5008]  eta: 0:07:08  lr: 0.000500  loss_labels: 0.0718 (0.0761)  loss: 0.0662 (0.0761)  time: 0.1710\n",
      "Epoch: [26]  [2700/5008]  eta: 0:06:49  lr: 0.000500  loss_labels: 0.0753 (0.0762)  loss: 0.0789 (0.0762)  time: 0.1695\n",
      "Epoch: [26]  [2800/5008]  eta: 0:06:31  lr: 0.000500  loss_labels: 0.0730 (0.0762)  loss: 0.0791 (0.0762)  time: 0.1683\n",
      "Epoch: [26]  [2900/5008]  eta: 0:06:12  lr: 0.000500  loss_labels: 0.0694 (0.0763)  loss: 0.0687 (0.0763)  time: 0.1685\n",
      "Epoch: [26]  [3000/5008]  eta: 0:05:54  lr: 0.000500  loss_labels: 0.0726 (0.0762)  loss: 0.0783 (0.0762)  time: 0.1679\n",
      "Epoch: [26]  [3100/5008]  eta: 0:05:36  lr: 0.000500  loss_labels: 0.0741 (0.0763)  loss: 0.0612 (0.0763)  time: 0.1695\n",
      "Epoch: [26]  [3200/5008]  eta: 0:05:18  lr: 0.000500  loss_labels: 0.0710 (0.0762)  loss: 0.0791 (0.0762)  time: 0.1698\n",
      "Epoch: [26]  [3300/5008]  eta: 0:05:00  lr: 0.000500  loss_labels: 0.0683 (0.0761)  loss: 0.0610 (0.0761)  time: 0.1724\n",
      "Epoch: [26]  [3400/5008]  eta: 0:04:42  lr: 0.000500  loss_labels: 0.0715 (0.0760)  loss: 0.0756 (0.0760)  time: 0.1726\n",
      "Epoch: [26]  [3500/5008]  eta: 0:04:25  lr: 0.000500  loss_labels: 0.0684 (0.0761)  loss: 0.0802 (0.0761)  time: 0.1721\n",
      "Epoch: [26]  [3600/5008]  eta: 0:04:07  lr: 0.000500  loss_labels: 0.0739 (0.0761)  loss: 0.0804 (0.0761)  time: 0.1717\n",
      "Epoch: [26]  [3700/5008]  eta: 0:03:49  lr: 0.000500  loss_labels: 0.0675 (0.0759)  loss: 0.0646 (0.0759)  time: 0.1718\n",
      "Epoch: [26]  [3800/5008]  eta: 0:03:31  lr: 0.000500  loss_labels: 0.0769 (0.0759)  loss: 0.0660 (0.0759)  time: 0.1716\n",
      "Epoch: [26]  [3900/5008]  eta: 0:03:14  lr: 0.000500  loss_labels: 0.0747 (0.0760)  loss: 0.0686 (0.0760)  time: 0.1712\n",
      "Epoch: [26]  [4000/5008]  eta: 0:02:56  lr: 0.000500  loss_labels: 0.0716 (0.0760)  loss: 0.0637 (0.0760)  time: 0.1762\n",
      "Epoch: [26]  [4100/5008]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.0754 (0.0760)  loss: 0.0856 (0.0760)  time: 0.1741\n",
      "Epoch: [26]  [4200/5008]  eta: 0:02:21  lr: 0.000500  loss_labels: 0.0675 (0.0758)  loss: 0.0712 (0.0758)  time: 0.1727\n",
      "Epoch: [26]  [4300/5008]  eta: 0:02:03  lr: 0.000500  loss_labels: 0.0670 (0.0756)  loss: 0.0690 (0.0756)  time: 0.1699\n",
      "Epoch: [26]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 0.0754 (0.0757)  loss: 0.0655 (0.0757)  time: 0.1691\n",
      "Epoch: [26]  [4500/5008]  eta: 0:01:28  lr: 0.000500  loss_labels: 0.0708 (0.0757)  loss: 0.0784 (0.0757)  time: 0.1691\n",
      "Epoch: [26]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.0692 (0.0757)  loss: 0.0705 (0.0757)  time: 0.1691\n",
      "Epoch: [26]  [4700/5008]  eta: 0:00:53  lr: 0.000500  loss_labels: 0.0710 (0.0757)  loss: 0.0846 (0.0757)  time: 0.1716\n",
      "Epoch: [26]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.0701 (0.0756)  loss: 0.0677 (0.0756)  time: 0.1707\n",
      "Epoch: [26]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0709 (0.0756)  loss: 0.0635 (0.0756)  time: 0.1710\n",
      "Epoch: [26]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0702 (0.0756)  loss: 0.0650 (0.0756)  time: 0.1717\n",
      "Epoch: [26]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0701 (0.0756)  loss: 0.0494 (0.0756)  time: 0.1715\n",
      "Epoch: [26] Total time: 0:14:33 (0.1745 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0701 (0.0756)  loss: 0.0494 (0.0756)\n",
      "Test:  [  0/565]  eta: 2:34:18  loss_labels: 1.1883 (1.1883)  loss: 1.1883 (1.1883)  time: 16.3860\n",
      "Test:  [100/565]  eta: 0:02:11  loss_labels: 1.4962 (1.6021)  loss: 1.7104 (1.6021)  time: 0.1272\n",
      "Test:  [200/565]  eta: 0:01:13  loss_labels: 1.2464 (1.4954)  loss: 1.3344 (1.4954)  time: 0.1214\n",
      "Test:  [300/565]  eta: 0:00:47  loss_labels: 1.1000 (1.4104)  loss: 1.0018 (1.4104)  time: 0.1244\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.4338 (1.4355)  loss: 1.3718 (1.4355)  time: 0.1074\n",
      "Test:  [500/565]  eta: 0:00:09  loss_labels: 1.2179 (1.4061)  loss: 1.2860 (1.4061)  time: 0.1130\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.2157 (1.3847)  loss: 1.0777 (1.3847)  time: 0.1212\n",
      "Test: Total time: 0:01:24 (0.1501 s / it)\n",
      "Averaged stats: loss_labels: 1.2157 (1.3847)  loss: 1.0777 (1.3847)\n",
      "acc: 0.7863651514053345\n",
      "top 1 and top 5 accuracies {'top1': 0.7863651470669695, 'top5': 0.8961114496205617, 'loss': tensor(0.0108, device='cuda:0')}\n",
      "Epoch: [27]  [   0/5008]  eta: 1 day, 5:42:00  lr: 0.000500  loss_labels: 0.1400 (0.1400)  loss: 0.1400 (0.1400)  time: 21.3500\n",
      "Epoch: [27]  [ 100/5008]  eta: 0:30:56  lr: 0.000500  loss_labels: 0.0738 (0.0761)  loss: 0.0648 (0.0761)  time: 0.1696\n",
      "Epoch: [27]  [ 200/5008]  eta: 0:22:00  lr: 0.000500  loss_labels: 0.0735 (0.0763)  loss: 0.0672 (0.0763)  time: 0.1710\n",
      "Epoch: [27]  [ 300/5008]  eta: 0:18:50  lr: 0.000500  loss_labels: 0.0665 (0.0753)  loss: 0.0737 (0.0753)  time: 0.1697\n",
      "Epoch: [27]  [ 400/5008]  eta: 0:17:03  lr: 0.000500  loss_labels: 0.0709 (0.0744)  loss: 0.0752 (0.0744)  time: 0.1675\n",
      "Epoch: [27]  [ 500/5008]  eta: 0:15:53  lr: 0.000500  loss_labels: 0.0716 (0.0747)  loss: 0.0602 (0.0747)  time: 0.1696\n",
      "Epoch: [27]  [ 600/5008]  eta: 0:15:03  lr: 0.000500  loss_labels: 0.0640 (0.0740)  loss: 0.0519 (0.0740)  time: 0.1725\n",
      "Epoch: [27]  [ 700/5008]  eta: 0:14:22  lr: 0.000500  loss_labels: 0.0710 (0.0742)  loss: 0.0819 (0.0742)  time: 0.1714\n",
      "Epoch: [27]  [ 800/5008]  eta: 0:13:48  lr: 0.000500  loss_labels: 0.0734 (0.0744)  loss: 0.0763 (0.0744)  time: 0.1746\n",
      "Epoch: [27]  [ 900/5008]  eta: 0:13:18  lr: 0.000500  loss_labels: 0.0665 (0.0739)  loss: 0.0682 (0.0739)  time: 0.1748\n",
      "Epoch: [27]  [1000/5008]  eta: 0:12:51  lr: 0.000500  loss_labels: 0.0687 (0.0739)  loss: 0.0687 (0.0739)  time: 0.1735\n",
      "Epoch: [27]  [1100/5008]  eta: 0:12:25  lr: 0.000500  loss_labels: 0.0660 (0.0735)  loss: 0.0606 (0.0735)  time: 0.1715\n",
      "Epoch: [27]  [1200/5008]  eta: 0:11:59  lr: 0.000500  loss_labels: 0.0650 (0.0733)  loss: 0.0592 (0.0733)  time: 0.1699\n",
      "Epoch: [27]  [1300/5008]  eta: 0:11:35  lr: 0.000500  loss_labels: 0.0745 (0.0736)  loss: 0.0690 (0.0736)  time: 0.1707\n",
      "Epoch: [27]  [1400/5008]  eta: 0:11:12  lr: 0.000500  loss_labels: 0.0695 (0.0738)  loss: 0.0813 (0.0738)  time: 0.1725\n",
      "Epoch: [27]  [1500/5008]  eta: 0:10:50  lr: 0.000500  loss_labels: 0.0728 (0.0739)  loss: 0.0687 (0.0739)  time: 0.1726\n",
      "Epoch: [27]  [1600/5008]  eta: 0:10:29  lr: 0.000500  loss_labels: 0.0621 (0.0737)  loss: 0.0621 (0.0737)  time: 0.1697\n",
      "Epoch: [27]  [1700/5008]  eta: 0:10:07  lr: 0.000500  loss_labels: 0.0746 (0.0739)  loss: 0.0797 (0.0739)  time: 0.1690\n",
      "Epoch: [27]  [1800/5008]  eta: 0:09:46  lr: 0.000500  loss_labels: 0.0635 (0.0736)  loss: 0.0628 (0.0736)  time: 0.1688\n",
      "Epoch: [27]  [1900/5008]  eta: 0:09:26  lr: 0.000500  loss_labels: 0.0652 (0.0735)  loss: 0.0647 (0.0735)  time: 0.1693\n",
      "Epoch: [27]  [2000/5008]  eta: 0:09:06  lr: 0.000500  loss_labels: 0.0655 (0.0734)  loss: 0.0583 (0.0734)  time: 0.1709\n",
      "Epoch: [27]  [2100/5008]  eta: 0:08:46  lr: 0.000500  loss_labels: 0.0660 (0.0731)  loss: 0.0678 (0.0731)  time: 0.1713\n",
      "Epoch: [27]  [2200/5008]  eta: 0:08:27  lr: 0.000500  loss_labels: 0.0731 (0.0732)  loss: 0.0857 (0.0732)  time: 0.1710\n",
      "Epoch: [27]  [2300/5008]  eta: 0:08:07  lr: 0.000500  loss_labels: 0.0662 (0.0731)  loss: 0.0719 (0.0731)  time: 0.1705\n",
      "Epoch: [27]  [2400/5008]  eta: 0:07:48  lr: 0.000500  loss_labels: 0.0707 (0.0731)  loss: 0.0725 (0.0731)  time: 0.1739\n",
      "Epoch: [27]  [2500/5008]  eta: 0:07:30  lr: 0.000500  loss_labels: 0.0675 (0.0730)  loss: 0.0620 (0.0730)  time: 0.1696\n",
      "Epoch: [27]  [2600/5008]  eta: 0:07:11  lr: 0.000500  loss_labels: 0.0655 (0.0730)  loss: 0.0553 (0.0730)  time: 0.1701\n",
      "Epoch: [27]  [2700/5008]  eta: 0:06:52  lr: 0.000500  loss_labels: 0.0668 (0.0729)  loss: 0.0651 (0.0729)  time: 0.1713\n",
      "Epoch: [27]  [2800/5008]  eta: 0:06:34  lr: 0.000500  loss_labels: 0.0710 (0.0730)  loss: 0.0669 (0.0730)  time: 0.1711\n",
      "Epoch: [27]  [2900/5008]  eta: 0:06:15  lr: 0.000500  loss_labels: 0.0710 (0.0731)  loss: 0.0686 (0.0731)  time: 0.1715\n",
      "Epoch: [27]  [3000/5008]  eta: 0:05:57  lr: 0.000500  loss_labels: 0.0759 (0.0732)  loss: 0.0816 (0.0732)  time: 0.1702\n",
      "Epoch: [27]  [3100/5008]  eta: 0:05:39  lr: 0.000500  loss_labels: 0.0749 (0.0733)  loss: 0.0611 (0.0733)  time: 0.1713\n",
      "Epoch: [27]  [3200/5008]  eta: 0:05:20  lr: 0.000500  loss_labels: 0.0654 (0.0731)  loss: 0.0672 (0.0731)  time: 0.1713\n",
      "Epoch: [27]  [3300/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.0645 (0.0731)  loss: 0.0631 (0.0731)  time: 0.1706\n",
      "Epoch: [27]  [3400/5008]  eta: 0:04:44  lr: 0.000500  loss_labels: 0.0674 (0.0731)  loss: 0.0707 (0.0731)  time: 0.1697\n",
      "Epoch: [27]  [3500/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.0685 (0.0732)  loss: 0.0929 (0.0732)  time: 0.1716\n",
      "Epoch: [27]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.0596 (0.0731)  loss: 0.0625 (0.0731)  time: 0.1730\n",
      "Epoch: [27]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 0.0639 (0.0729)  loss: 0.0757 (0.0729)  time: 0.1731\n",
      "Epoch: [27]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 0.0651 (0.0727)  loss: 0.0670 (0.0727)  time: 0.1723\n",
      "Epoch: [27]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.0734 (0.0727)  loss: 0.0537 (0.0727)  time: 0.1741\n",
      "Epoch: [27]  [4000/5008]  eta: 0:02:57  lr: 0.000500  loss_labels: 0.0650 (0.0727)  loss: 0.0598 (0.0727)  time: 0.1722\n",
      "Epoch: [27]  [4100/5008]  eta: 0:02:40  lr: 0.000500  loss_labels: 0.0716 (0.0727)  loss: 0.0789 (0.0727)  time: 0.1722\n",
      "Epoch: [27]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.0644 (0.0726)  loss: 0.0671 (0.0726)  time: 0.1737\n",
      "Epoch: [27]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.0665 (0.0725)  loss: 0.0687 (0.0725)  time: 0.1717\n",
      "Epoch: [27]  [4400/5008]  eta: 0:01:47  lr: 0.000500  loss_labels: 0.0717 (0.0726)  loss: 0.0662 (0.0726)  time: 0.1719\n",
      "Epoch: [27]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.0680 (0.0726)  loss: 0.0718 (0.0726)  time: 0.1717\n",
      "Epoch: [27]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.0677 (0.0725)  loss: 0.0618 (0.0725)  time: 0.1711\n",
      "Epoch: [27]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.0665 (0.0725)  loss: 0.0703 (0.0725)  time: 0.1707\n",
      "Epoch: [27]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.0607 (0.0724)  loss: 0.0740 (0.0724)  time: 0.1717\n",
      "Epoch: [27]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0673 (0.0723)  loss: 0.0669 (0.0723)  time: 0.1702\n",
      "Epoch: [27]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0615 (0.0723)  loss: 0.0612 (0.0723)  time: 0.1727\n",
      "Epoch: [27]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0630 (0.0723)  loss: 0.0654 (0.0723)  time: 0.1728\n",
      "Epoch: [27] Total time: 0:14:39 (0.1756 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0630 (0.0723)  loss: 0.0654 (0.0723)\n",
      "Test:  [  0/565]  eta: 2:37:57  loss_labels: 0.9442 (0.9442)  loss: 0.9442 (0.9442)  time: 16.7736\n",
      "Test:  [100/565]  eta: 0:02:13  loss_labels: 1.3789 (1.5004)  loss: 1.3705 (1.5004)  time: 0.1204\n",
      "Test:  [200/565]  eta: 0:01:14  loss_labels: 1.2136 (1.4286)  loss: 1.5052 (1.4286)  time: 0.1132\n",
      "Test:  [300/565]  eta: 0:00:46  loss_labels: 1.1122 (1.3630)  loss: 0.9695 (1.3630)  time: 0.1189\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.4228 (1.3805)  loss: 1.3649 (1.3805)  time: 0.1093\n",
      "Test:  [500/565]  eta: 0:00:09  loss_labels: 1.1079 (1.3510)  loss: 1.2821 (1.3510)  time: 0.1176\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1723 (1.3321)  loss: 1.0666 (1.3321)  time: 0.1236\n",
      "Test: Total time: 0:01:24 (0.1501 s / it)\n",
      "Averaged stats: loss_labels: 1.1723 (1.3321)  loss: 1.0666 (1.3321)\n",
      "acc: 0.7990915775299072\n",
      "top 1 and top 5 accuracies {'top1': 0.7990915637290201, 'top5': 0.9010136819365202, 'loss': tensor(0.0104, device='cuda:0')}\n",
      "Epoch: [28]  [   0/5008]  eta: 1 day, 4:59:58  lr: 0.000500  loss_labels: 0.0966 (0.0966)  loss: 0.0966 (0.0966)  time: 20.8464\n",
      "Epoch: [28]  [ 100/5008]  eta: 0:30:43  lr: 0.000500  loss_labels: 0.0708 (0.0735)  loss: 0.0594 (0.0735)  time: 0.1707\n",
      "Epoch: [28]  [ 200/5008]  eta: 0:21:58  lr: 0.000500  loss_labels: 0.0611 (0.0689)  loss: 0.0523 (0.0689)  time: 0.1716\n",
      "Epoch: [28]  [ 300/5008]  eta: 0:18:51  lr: 0.000500  loss_labels: 0.0662 (0.0708)  loss: 0.0660 (0.0708)  time: 0.1712\n",
      "Epoch: [28]  [ 400/5008]  eta: 0:17:08  lr: 0.000500  loss_labels: 0.0603 (0.0696)  loss: 0.0656 (0.0696)  time: 0.1708\n",
      "Epoch: [28]  [ 500/5008]  eta: 0:15:57  lr: 0.000500  loss_labels: 0.0598 (0.0683)  loss: 0.0525 (0.0683)  time: 0.1695\n",
      "Epoch: [28]  [ 600/5008]  eta: 0:15:04  lr: 0.000500  loss_labels: 0.0655 (0.0684)  loss: 0.0581 (0.0684)  time: 0.1679\n",
      "Epoch: [28]  [ 700/5008]  eta: 0:14:21  lr: 0.000500  loss_labels: 0.0634 (0.0684)  loss: 0.0600 (0.0684)  time: 0.1684\n",
      "Epoch: [28]  [ 800/5008]  eta: 0:13:45  lr: 0.000500  loss_labels: 0.0715 (0.0689)  loss: 0.0822 (0.0689)  time: 0.1702\n",
      "Epoch: [28]  [ 900/5008]  eta: 0:13:13  lr: 0.000500  loss_labels: 0.0625 (0.0686)  loss: 0.0730 (0.0686)  time: 0.1690\n",
      "Epoch: [28]  [1000/5008]  eta: 0:12:44  lr: 0.000500  loss_labels: 0.0680 (0.0685)  loss: 0.0635 (0.0685)  time: 0.1700\n",
      "Epoch: [28]  [1100/5008]  eta: 0:12:18  lr: 0.000500  loss_labels: 0.0631 (0.0682)  loss: 0.0592 (0.0682)  time: 0.1694\n",
      "Epoch: [28]  [1200/5008]  eta: 0:11:53  lr: 0.000500  loss_labels: 0.0600 (0.0683)  loss: 0.0619 (0.0683)  time: 0.1696\n",
      "Epoch: [28]  [1300/5008]  eta: 0:11:29  lr: 0.000500  loss_labels: 0.0585 (0.0679)  loss: 0.0584 (0.0679)  time: 0.1697\n",
      "Epoch: [28]  [1400/5008]  eta: 0:11:06  lr: 0.000500  loss_labels: 0.0659 (0.0679)  loss: 0.0666 (0.0679)  time: 0.1697\n",
      "Epoch: [28]  [1500/5008]  eta: 0:10:44  lr: 0.000500  loss_labels: 0.0678 (0.0679)  loss: 0.0568 (0.0679)  time: 0.1697\n",
      "Epoch: [28]  [1600/5008]  eta: 0:10:23  lr: 0.000500  loss_labels: 0.0604 (0.0680)  loss: 0.0556 (0.0680)  time: 0.1695\n",
      "Epoch: [28]  [1700/5008]  eta: 0:10:02  lr: 0.000500  loss_labels: 0.0654 (0.0681)  loss: 0.0714 (0.0681)  time: 0.1718\n",
      "Epoch: [28]  [1800/5008]  eta: 0:09:42  lr: 0.000500  loss_labels: 0.0597 (0.0681)  loss: 0.0522 (0.0681)  time: 0.1729\n",
      "Epoch: [28]  [1900/5008]  eta: 0:09:23  lr: 0.000500  loss_labels: 0.0585 (0.0679)  loss: 0.0585 (0.0679)  time: 0.1728\n",
      "Epoch: [28]  [2000/5008]  eta: 0:09:04  lr: 0.000500  loss_labels: 0.0657 (0.0680)  loss: 0.0669 (0.0680)  time: 0.1748\n",
      "Epoch: [28]  [2100/5008]  eta: 0:08:45  lr: 0.000500  loss_labels: 0.0617 (0.0679)  loss: 0.0751 (0.0679)  time: 0.1742\n",
      "Epoch: [28]  [2200/5008]  eta: 0:08:26  lr: 0.000500  loss_labels: 0.0600 (0.0678)  loss: 0.0597 (0.0678)  time: 0.1745\n",
      "Epoch: [28]  [2300/5008]  eta: 0:08:07  lr: 0.000500  loss_labels: 0.0663 (0.0679)  loss: 0.0629 (0.0679)  time: 0.1731\n",
      "Epoch: [28]  [2400/5008]  eta: 0:07:48  lr: 0.000500  loss_labels: 0.0635 (0.0679)  loss: 0.0771 (0.0679)  time: 0.1726\n",
      "Epoch: [28]  [2500/5008]  eta: 0:07:30  lr: 0.000500  loss_labels: 0.0604 (0.0677)  loss: 0.0637 (0.0677)  time: 0.1727\n",
      "Epoch: [28]  [2600/5008]  eta: 0:07:11  lr: 0.000500  loss_labels: 0.0615 (0.0677)  loss: 0.0613 (0.0677)  time: 0.1731\n",
      "Epoch: [28]  [2700/5008]  eta: 0:06:53  lr: 0.000500  loss_labels: 0.0626 (0.0677)  loss: 0.0591 (0.0677)  time: 0.1728\n",
      "Epoch: [28]  [2800/5008]  eta: 0:06:34  lr: 0.000500  loss_labels: 0.0682 (0.0678)  loss: 0.0657 (0.0678)  time: 0.1736\n",
      "Epoch: [28]  [2900/5008]  eta: 0:06:16  lr: 0.000500  loss_labels: 0.0650 (0.0678)  loss: 0.0613 (0.0678)  time: 0.1727\n",
      "Epoch: [28]  [3000/5008]  eta: 0:05:58  lr: 0.000500  loss_labels: 0.0631 (0.0678)  loss: 0.0616 (0.0678)  time: 0.1729\n",
      "Epoch: [28]  [3100/5008]  eta: 0:05:40  lr: 0.000500  loss_labels: 0.0679 (0.0679)  loss: 0.0540 (0.0679)  time: 0.1728\n",
      "Epoch: [28]  [3200/5008]  eta: 0:05:21  lr: 0.000500  loss_labels: 0.0553 (0.0677)  loss: 0.0619 (0.0677)  time: 0.1742\n",
      "Epoch: [28]  [3300/5008]  eta: 0:05:03  lr: 0.000500  loss_labels: 0.0642 (0.0677)  loss: 0.0551 (0.0677)  time: 0.1730\n",
      "Epoch: [28]  [3400/5008]  eta: 0:04:45  lr: 0.000500  loss_labels: 0.0637 (0.0677)  loss: 0.0714 (0.0677)  time: 0.1729\n",
      "Epoch: [28]  [3500/5008]  eta: 0:04:27  lr: 0.000500  loss_labels: 0.0682 (0.0679)  loss: 0.0744 (0.0679)  time: 0.1730\n",
      "Epoch: [28]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.0670 (0.0680)  loss: 0.0644 (0.0680)  time: 0.1731\n",
      "Epoch: [28]  [3700/5008]  eta: 0:03:52  lr: 0.000500  loss_labels: 0.0580 (0.0679)  loss: 0.0663 (0.0679)  time: 0.1735\n",
      "Epoch: [28]  [3800/5008]  eta: 0:03:34  lr: 0.000500  loss_labels: 0.0687 (0.0680)  loss: 0.0834 (0.0680)  time: 0.1729\n",
      "Epoch: [28]  [3900/5008]  eta: 0:03:16  lr: 0.000500  loss_labels: 0.0672 (0.0680)  loss: 0.0639 (0.0680)  time: 0.1737\n",
      "Epoch: [28]  [4000/5008]  eta: 0:02:58  lr: 0.000500  loss_labels: 0.0662 (0.0680)  loss: 0.0496 (0.0680)  time: 0.1730\n",
      "Epoch: [28]  [4100/5008]  eta: 0:02:40  lr: 0.000500  loss_labels: 0.0674 (0.0680)  loss: 0.0685 (0.0680)  time: 0.1745\n",
      "Epoch: [28]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.0588 (0.0679)  loss: 0.0639 (0.0679)  time: 0.1757\n",
      "Epoch: [28]  [4300/5008]  eta: 0:02:05  lr: 0.000500  loss_labels: 0.0611 (0.0679)  loss: 0.0752 (0.0679)  time: 0.1735\n",
      "Epoch: [28]  [4400/5008]  eta: 0:01:47  lr: 0.000500  loss_labels: 0.0688 (0.0679)  loss: 0.0688 (0.0679)  time: 0.1733\n",
      "Epoch: [28]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.0659 (0.0680)  loss: 0.0688 (0.0680)  time: 0.1715\n",
      "Epoch: [28]  [4600/5008]  eta: 0:01:12  lr: 0.000500  loss_labels: 0.0579 (0.0679)  loss: 0.0543 (0.0679)  time: 0.1727\n",
      "Epoch: [28]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.0574 (0.0677)  loss: 0.0686 (0.0677)  time: 0.1712\n",
      "Epoch: [28]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.0591 (0.0677)  loss: 0.0606 (0.0677)  time: 0.1718\n",
      "Epoch: [28]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.0605 (0.0676)  loss: 0.0623 (0.0676)  time: 0.1705\n",
      "Epoch: [28]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0602 (0.0675)  loss: 0.0484 (0.0675)  time: 0.1738\n",
      "Epoch: [28]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0599 (0.0675)  loss: 0.0440 (0.0675)  time: 0.1722\n",
      "Epoch: [28] Total time: 0:14:43 (0.1764 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0599 (0.0675)  loss: 0.0440 (0.0675)\n",
      "Test:  [  0/565]  eta: 2:36:12  loss_labels: 0.9852 (0.9852)  loss: 0.9852 (0.9852)  time: 16.5882\n",
      "Test:  [100/565]  eta: 0:02:11  loss_labels: 1.4591 (1.5711)  loss: 1.5904 (1.5711)  time: 0.1152\n",
      "Test:  [200/565]  eta: 0:01:14  loss_labels: 1.3165 (1.4693)  loss: 1.4285 (1.4693)  time: 0.1181\n",
      "Test:  [300/565]  eta: 0:00:46  loss_labels: 1.0906 (1.3835)  loss: 1.0118 (1.3835)  time: 0.1211\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.4230 (1.4074)  loss: 1.3665 (1.4074)  time: 0.1105\n",
      "Test:  [500/565]  eta: 0:00:09  loss_labels: 1.1969 (1.3769)  loss: 1.3805 (1.3769)  time: 0.1180\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1698 (1.3513)  loss: 1.0266 (1.3513)  time: 0.1190\n",
      "Test: Total time: 0:01:24 (0.1497 s / it)\n",
      "Averaged stats: loss_labels: 1.1698 (1.3513)  loss: 1.0266 (1.3513)\n",
      "acc: 0.7969866394996643\n",
      "top 1 and top 5 accuracies {'top1': 0.796986650418213, 'top5': 0.9005566941782529, 'loss': tensor(0.0106, device='cuda:0')}\n",
      "Epoch: [29]  [   0/5008]  eta: 1 day, 5:48:44  lr: 0.000500  loss_labels: 0.0829 (0.0829)  loss: 0.0829 (0.0829)  time: 21.4307\n",
      "Epoch: [29]  [ 100/5008]  eta: 0:31:07  lr: 0.000500  loss_labels: 0.0616 (0.0622)  loss: 0.0572 (0.0622)  time: 0.1697\n",
      "Epoch: [29]  [ 200/5008]  eta: 0:22:04  lr: 0.000500  loss_labels: 0.0644 (0.0643)  loss: 0.0596 (0.0643)  time: 0.1698\n",
      "Epoch: [29]  [ 300/5008]  eta: 0:18:52  lr: 0.000500  loss_labels: 0.0584 (0.0642)  loss: 0.0592 (0.0642)  time: 0.1696\n",
      "Epoch: [29]  [ 400/5008]  eta: 0:17:07  lr: 0.000500  loss_labels: 0.0594 (0.0633)  loss: 0.0647 (0.0633)  time: 0.1701\n",
      "Epoch: [29]  [ 500/5008]  eta: 0:15:57  lr: 0.000500  loss_labels: 0.0642 (0.0651)  loss: 0.0617 (0.0651)  time: 0.1701\n",
      "Epoch: [29]  [ 600/5008]  eta: 0:15:05  lr: 0.000500  loss_labels: 0.0651 (0.0659)  loss: 0.0679 (0.0659)  time: 0.1708\n",
      "Epoch: [29]  [ 700/5008]  eta: 0:14:23  lr: 0.000500  loss_labels: 0.0581 (0.0653)  loss: 0.0593 (0.0653)  time: 0.1710\n",
      "Epoch: [29]  [ 800/5008]  eta: 0:13:47  lr: 0.000500  loss_labels: 0.0646 (0.0660)  loss: 0.0759 (0.0660)  time: 0.1708\n",
      "Epoch: [29]  [ 900/5008]  eta: 0:13:16  lr: 0.000500  loss_labels: 0.0548 (0.0654)  loss: 0.0532 (0.0654)  time: 0.1730\n",
      "Epoch: [29]  [1000/5008]  eta: 0:12:48  lr: 0.000500  loss_labels: 0.0577 (0.0652)  loss: 0.0604 (0.0652)  time: 0.1703\n",
      "Epoch: [29]  [1100/5008]  eta: 0:12:21  lr: 0.000500  loss_labels: 0.0604 (0.0651)  loss: 0.0579 (0.0651)  time: 0.1701\n",
      "Epoch: [29]  [1200/5008]  eta: 0:11:56  lr: 0.000500  loss_labels: 0.0565 (0.0648)  loss: 0.0617 (0.0648)  time: 0.1691\n",
      "Epoch: [29]  [1300/5008]  eta: 0:11:32  lr: 0.000500  loss_labels: 0.0596 (0.0647)  loss: 0.0383 (0.0647)  time: 0.1699\n",
      "Epoch: [29]  [1400/5008]  eta: 0:11:08  lr: 0.000500  loss_labels: 0.0659 (0.0649)  loss: 0.0708 (0.0649)  time: 0.1702\n",
      "Epoch: [29]  [1500/5008]  eta: 0:10:46  lr: 0.000500  loss_labels: 0.0630 (0.0650)  loss: 0.0619 (0.0650)  time: 0.1692\n",
      "Epoch: [29]  [1600/5008]  eta: 0:10:25  lr: 0.000500  loss_labels: 0.0632 (0.0652)  loss: 0.0731 (0.0652)  time: 0.1710\n",
      "Epoch: [29]  [1700/5008]  eta: 0:10:04  lr: 0.000500  loss_labels: 0.0586 (0.0653)  loss: 0.0574 (0.0653)  time: 0.1709\n",
      "Epoch: [29]  [1800/5008]  eta: 0:09:44  lr: 0.000500  loss_labels: 0.0548 (0.0650)  loss: 0.0538 (0.0650)  time: 0.1732\n",
      "Epoch: [29]  [1900/5008]  eta: 0:09:24  lr: 0.000500  loss_labels: 0.0606 (0.0651)  loss: 0.0626 (0.0651)  time: 0.1709\n",
      "Epoch: [29]  [2000/5008]  eta: 0:09:04  lr: 0.000500  loss_labels: 0.0587 (0.0650)  loss: 0.0440 (0.0650)  time: 0.1714\n",
      "Epoch: [29]  [2100/5008]  eta: 0:08:45  lr: 0.000500  loss_labels: 0.0615 (0.0650)  loss: 0.0655 (0.0650)  time: 0.1711\n",
      "Epoch: [29]  [2200/5008]  eta: 0:08:26  lr: 0.000500  loss_labels: 0.0642 (0.0650)  loss: 0.0653 (0.0650)  time: 0.1719\n",
      "Epoch: [29]  [2300/5008]  eta: 0:08:06  lr: 0.000500  loss_labels: 0.0640 (0.0652)  loss: 0.0641 (0.0652)  time: 0.1705\n",
      "Epoch: [29]  [2400/5008]  eta: 0:07:48  lr: 0.000500  loss_labels: 0.0608 (0.0652)  loss: 0.0521 (0.0652)  time: 0.1714\n",
      "Epoch: [29]  [2500/5008]  eta: 0:07:29  lr: 0.000500  loss_labels: 0.0620 (0.0652)  loss: 0.0604 (0.0652)  time: 0.1713\n",
      "Epoch: [29]  [2600/5008]  eta: 0:07:10  lr: 0.000500  loss_labels: 0.0626 (0.0654)  loss: 0.0577 (0.0654)  time: 0.1717\n",
      "Epoch: [29]  [2700/5008]  eta: 0:06:52  lr: 0.000500  loss_labels: 0.0601 (0.0654)  loss: 0.0567 (0.0654)  time: 0.1713\n",
      "Epoch: [29]  [2800/5008]  eta: 0:06:33  lr: 0.000500  loss_labels: 0.0630 (0.0655)  loss: 0.0657 (0.0655)  time: 0.1732\n",
      "Epoch: [29]  [2900/5008]  eta: 0:06:15  lr: 0.000500  loss_labels: 0.0586 (0.0653)  loss: 0.0549 (0.0653)  time: 0.1711\n",
      "Epoch: [29]  [3000/5008]  eta: 0:05:57  lr: 0.000500  loss_labels: 0.0596 (0.0652)  loss: 0.0607 (0.0652)  time: 0.1710\n",
      "Epoch: [29]  [3100/5008]  eta: 0:05:39  lr: 0.000500  loss_labels: 0.0633 (0.0652)  loss: 0.0533 (0.0652)  time: 0.1704\n",
      "Epoch: [29]  [3200/5008]  eta: 0:05:20  lr: 0.000500  loss_labels: 0.0552 (0.0652)  loss: 0.0627 (0.0652)  time: 0.1717\n",
      "Epoch: [29]  [3300/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.0620 (0.0651)  loss: 0.0643 (0.0651)  time: 0.1713\n",
      "Epoch: [29]  [3400/5008]  eta: 0:04:44  lr: 0.000500  loss_labels: 0.0617 (0.0651)  loss: 0.0602 (0.0651)  time: 0.1703\n",
      "Epoch: [29]  [3500/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.0594 (0.0651)  loss: 0.0747 (0.0651)  time: 0.1717\n",
      "Epoch: [29]  [3600/5008]  eta: 0:04:08  lr: 0.000500  loss_labels: 0.0575 (0.0650)  loss: 0.0526 (0.0650)  time: 0.1688\n",
      "Epoch: [29]  [3700/5008]  eta: 0:03:50  lr: 0.000500  loss_labels: 0.0567 (0.0648)  loss: 0.0625 (0.0648)  time: 0.1690\n",
      "Epoch: [29]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 0.0515 (0.0646)  loss: 0.0468 (0.0646)  time: 0.1690\n",
      "Epoch: [29]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.0657 (0.0647)  loss: 0.0579 (0.0647)  time: 0.1711\n",
      "Epoch: [29]  [4000/5008]  eta: 0:02:59  lr: 0.000500  loss_labels: 0.0586 (0.0647)  loss: 0.0540 (0.0647)  time: 0.2323\n",
      "Epoch: [29]  [4100/5008]  eta: 0:02:41  lr: 0.000500  loss_labels: 0.0592 (0.0647)  loss: 0.0595 (0.0647)  time: 0.1713\n",
      "Epoch: [29]  [4200/5008]  eta: 0:02:23  lr: 0.000500  loss_labels: 0.0610 (0.0647)  loss: 0.0507 (0.0647)  time: 0.1714\n",
      "Epoch: [29]  [4300/5008]  eta: 0:02:05  lr: 0.000500  loss_labels: 0.0539 (0.0645)  loss: 0.0643 (0.0645)  time: 0.1709\n",
      "Epoch: [29]  [4400/5008]  eta: 0:01:48  lr: 0.000500  loss_labels: 0.0579 (0.0645)  loss: 0.0579 (0.0645)  time: 0.1704\n",
      "Epoch: [29]  [4500/5008]  eta: 0:01:30  lr: 0.000500  loss_labels: 0.0570 (0.0645)  loss: 0.0599 (0.0645)  time: 0.1702\n",
      "Epoch: [29]  [4600/5008]  eta: 0:01:12  lr: 0.000500  loss_labels: 0.0621 (0.0645)  loss: 0.0488 (0.0645)  time: 0.1711\n",
      "Epoch: [29]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.0569 (0.0644)  loss: 0.0524 (0.0644)  time: 0.1700\n",
      "Epoch: [29]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.0615 (0.0643)  loss: 0.0615 (0.0643)  time: 0.1696\n",
      "Epoch: [29]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.0589 (0.0643)  loss: 0.0619 (0.0643)  time: 0.1691\n",
      "Epoch: [29]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0542 (0.0642)  loss: 0.0534 (0.0642)  time: 0.1722\n",
      "Epoch: [29]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0542 (0.0642)  loss: 0.0524 (0.0642)  time: 0.1723\n",
      "Epoch: [29] Total time: 0:14:48 (0.1773 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0542 (0.0642)  loss: 0.0524 (0.0642)\n",
      "Test:  [  0/565]  eta: 3:30:20  loss_labels: 1.2640 (1.2640)  loss: 1.2640 (1.2640)  time: 22.3372\n",
      "Test:  [100/565]  eta: 0:02:49  loss_labels: 1.4546 (1.5878)  loss: 1.6878 (1.5878)  time: 0.1322\n",
      "Test:  [200/565]  eta: 0:01:30  loss_labels: 1.1830 (1.4943)  loss: 1.5256 (1.4943)  time: 0.1251\n",
      "Test:  [300/565]  eta: 0:00:55  loss_labels: 1.1025 (1.4069)  loss: 1.1025 (1.4069)  time: 0.1350\n",
      "Test:  [400/565]  eta: 0:00:31  loss_labels: 1.5334 (1.4420)  loss: 1.3551 (1.4420)  time: 0.1249\n",
      "Test:  [500/565]  eta: 0:00:11  loss_labels: 1.1533 (1.4075)  loss: 1.4099 (1.4075)  time: 0.1228\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.2342 (1.3827)  loss: 1.2662 (1.3827)  time: 0.1230\n",
      "Test: Total time: 0:01:37 (0.1722 s / it)\n",
      "Averaged stats: loss_labels: 1.2342 (1.3827)  loss: 1.2662 (1.3827)\n",
      "acc: 0.7957403063774109\n",
      "top 1 and top 5 accuracies {'top1': 0.795740320168393, 'top5': 0.9001966432171938, 'loss': tensor(0.0108, device='cuda:0')}\n",
      "Epoch: [30]  [   0/5008]  eta: 1 day, 6:23:19  lr: 0.000500  loss_labels: 0.0948 (0.0948)  loss: 0.0948 (0.0948)  time: 21.8450\n",
      "Epoch: [30]  [ 100/5008]  eta: 0:31:16  lr: 0.000500  loss_labels: 0.0556 (0.0599)  loss: 0.0548 (0.0599)  time: 0.1676\n",
      "Epoch: [30]  [ 200/5008]  eta: 0:22:04  lr: 0.000500  loss_labels: 0.0586 (0.0616)  loss: 0.0490 (0.0616)  time: 0.1680\n",
      "Epoch: [30]  [ 300/5008]  eta: 0:19:32  lr: 0.000500  loss_labels: 0.0591 (0.0620)  loss: 0.0642 (0.0620)  time: 0.1714\n",
      "Epoch: [30]  [ 400/5008]  eta: 0:17:36  lr: 0.000500  loss_labels: 0.0588 (0.0616)  loss: 0.0606 (0.0616)  time: 0.1687\n",
      "Epoch: [30]  [ 500/5008]  eta: 0:16:17  lr: 0.000500  loss_labels: 0.0545 (0.0612)  loss: 0.0456 (0.0612)  time: 0.1685\n",
      "Epoch: [30]  [ 600/5008]  eta: 0:15:20  lr: 0.000500  loss_labels: 0.0550 (0.0611)  loss: 0.0563 (0.0611)  time: 0.1687\n",
      "Epoch: [30]  [ 700/5008]  eta: 0:14:34  lr: 0.000500  loss_labels: 0.0572 (0.0613)  loss: 0.0624 (0.0613)  time: 0.1689\n",
      "Epoch: [30]  [ 800/5008]  eta: 0:13:56  lr: 0.000500  loss_labels: 0.0627 (0.0618)  loss: 0.0655 (0.0618)  time: 0.1679\n",
      "Epoch: [30]  [ 900/5008]  eta: 0:13:22  lr: 0.000500  loss_labels: 0.0531 (0.0611)  loss: 0.0604 (0.0611)  time: 0.1679\n",
      "Epoch: [30]  [1000/5008]  eta: 0:12:51  lr: 0.000500  loss_labels: 0.0557 (0.0612)  loss: 0.0533 (0.0612)  time: 0.1680\n",
      "Epoch: [30]  [1100/5008]  eta: 0:12:23  lr: 0.000500  loss_labels: 0.0589 (0.0613)  loss: 0.0620 (0.0613)  time: 0.1681\n",
      "Epoch: [30]  [1200/5008]  eta: 0:11:57  lr: 0.000500  loss_labels: 0.0540 (0.0613)  loss: 0.0550 (0.0613)  time: 0.1689\n",
      "Epoch: [30]  [1300/5008]  eta: 0:11:33  lr: 0.000500  loss_labels: 0.0542 (0.0611)  loss: 0.0428 (0.0611)  time: 0.1698\n",
      "Epoch: [30]  [1400/5008]  eta: 0:11:10  lr: 0.000500  loss_labels: 0.0578 (0.0611)  loss: 0.0550 (0.0611)  time: 0.1687\n",
      "Epoch: [30]  [1500/5008]  eta: 0:10:47  lr: 0.000500  loss_labels: 0.0605 (0.0614)  loss: 0.0605 (0.0614)  time: 0.1687\n",
      "Epoch: [30]  [1600/5008]  eta: 0:10:25  lr: 0.000500  loss_labels: 0.0571 (0.0616)  loss: 0.0523 (0.0616)  time: 0.1695\n",
      "Epoch: [30]  [1700/5008]  eta: 0:10:04  lr: 0.000500  loss_labels: 0.0572 (0.0616)  loss: 0.0657 (0.0616)  time: 0.1697\n",
      "Epoch: [30]  [1800/5008]  eta: 0:09:44  lr: 0.000500  loss_labels: 0.0542 (0.0615)  loss: 0.0470 (0.0615)  time: 0.1713\n",
      "Epoch: [30]  [1900/5008]  eta: 0:09:24  lr: 0.000500  loss_labels: 0.0570 (0.0615)  loss: 0.0517 (0.0615)  time: 0.1701\n",
      "Epoch: [30]  [2000/5008]  eta: 0:09:04  lr: 0.000500  loss_labels: 0.0572 (0.0616)  loss: 0.0352 (0.0616)  time: 0.1701\n",
      "Epoch: [30]  [2100/5008]  eta: 0:08:44  lr: 0.000500  loss_labels: 0.0552 (0.0615)  loss: 0.0631 (0.0615)  time: 0.1700\n",
      "Epoch: [30]  [2200/5008]  eta: 0:08:25  lr: 0.000500  loss_labels: 0.0571 (0.0615)  loss: 0.0585 (0.0615)  time: 0.1712\n",
      "Epoch: [30]  [2300/5008]  eta: 0:08:06  lr: 0.000500  loss_labels: 0.0591 (0.0614)  loss: 0.0496 (0.0614)  time: 0.1711\n",
      "Epoch: [30]  [2400/5008]  eta: 0:07:47  lr: 0.000500  loss_labels: 0.0589 (0.0615)  loss: 0.0623 (0.0615)  time: 0.1700\n",
      "Epoch: [30]  [2500/5008]  eta: 0:07:28  lr: 0.000500  loss_labels: 0.0563 (0.0614)  loss: 0.0568 (0.0614)  time: 0.1708\n",
      "Epoch: [30]  [2600/5008]  eta: 0:07:09  lr: 0.000500  loss_labels: 0.0599 (0.0613)  loss: 0.0466 (0.0613)  time: 0.1720\n",
      "Epoch: [30]  [2700/5008]  eta: 0:06:51  lr: 0.000500  loss_labels: 0.0563 (0.0613)  loss: 0.0518 (0.0613)  time: 0.1711\n",
      "Epoch: [30]  [2800/5008]  eta: 0:06:32  lr: 0.000500  loss_labels: 0.0571 (0.0614)  loss: 0.0534 (0.0614)  time: 0.1703\n",
      "Epoch: [30]  [2900/5008]  eta: 0:06:14  lr: 0.000500  loss_labels: 0.0565 (0.0614)  loss: 0.0570 (0.0614)  time: 0.1710\n",
      "Epoch: [30]  [3000/5008]  eta: 0:05:56  lr: 0.000500  loss_labels: 0.0592 (0.0614)  loss: 0.0629 (0.0614)  time: 0.1720\n",
      "Epoch: [30]  [3100/5008]  eta: 0:05:38  lr: 0.000500  loss_labels: 0.0608 (0.0615)  loss: 0.0490 (0.0615)  time: 0.1731\n",
      "Epoch: [30]  [3200/5008]  eta: 0:05:20  lr: 0.000500  loss_labels: 0.0638 (0.0616)  loss: 0.0614 (0.0616)  time: 0.1712\n",
      "Epoch: [30]  [3300/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.0519 (0.0615)  loss: 0.0472 (0.0615)  time: 0.1722\n",
      "Epoch: [30]  [3400/5008]  eta: 0:04:44  lr: 0.000500  loss_labels: 0.0571 (0.0614)  loss: 0.0477 (0.0614)  time: 0.1718\n",
      "Epoch: [30]  [3500/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.0587 (0.0615)  loss: 0.0663 (0.0615)  time: 0.1723\n",
      "Epoch: [30]  [3600/5008]  eta: 0:04:08  lr: 0.000500  loss_labels: 0.0566 (0.0615)  loss: 0.0658 (0.0615)  time: 0.1730\n",
      "Epoch: [30]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 0.0545 (0.0613)  loss: 0.0565 (0.0613)  time: 0.1734\n",
      "Epoch: [30]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 0.0534 (0.0612)  loss: 0.0447 (0.0612)  time: 0.1711\n",
      "Epoch: [30]  [3900/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.0571 (0.0612)  loss: 0.0642 (0.0612)  time: 0.1698\n",
      "Epoch: [30]  [4000/5008]  eta: 0:02:57  lr: 0.000500  loss_labels: 0.0574 (0.0612)  loss: 0.0568 (0.0612)  time: 0.1705\n",
      "Epoch: [30]  [4100/5008]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.0575 (0.0611)  loss: 0.0540 (0.0611)  time: 0.1723\n",
      "Epoch: [30]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.0570 (0.0611)  loss: 0.0521 (0.0611)  time: 0.1701\n",
      "Epoch: [30]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.0597 (0.0611)  loss: 0.0609 (0.0611)  time: 0.1718\n",
      "Epoch: [30]  [4400/5008]  eta: 0:01:46  lr: 0.000500  loss_labels: 0.0551 (0.0612)  loss: 0.0511 (0.0612)  time: 0.1688\n",
      "Epoch: [30]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.0605 (0.0613)  loss: 0.0639 (0.0613)  time: 0.1699\n",
      "Epoch: [30]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.0538 (0.0613)  loss: 0.0500 (0.0613)  time: 0.1713\n",
      "Epoch: [30]  [4700/5008]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.0511 (0.0612)  loss: 0.0498 (0.0612)  time: 0.1729\n",
      "Epoch: [30]  [4800/5008]  eta: 0:00:36  lr: 0.000500  loss_labels: 0.0587 (0.0612)  loss: 0.0527 (0.0612)  time: 0.1742\n",
      "Epoch: [30]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0547 (0.0611)  loss: 0.0550 (0.0611)  time: 0.1712\n",
      "Epoch: [30]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0592 (0.0612)  loss: 0.0539 (0.0612)  time: 0.1735\n",
      "Epoch: [30]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0554 (0.0612)  loss: 0.0477 (0.0612)  time: 0.1724\n",
      "Epoch: [30] Total time: 0:14:38 (0.1755 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0554 (0.0612)  loss: 0.0477 (0.0612)\n",
      "Test:  [  0/565]  eta: 3:06:54  loss_labels: 1.0718 (1.0718)  loss: 1.0718 (1.0718)  time: 19.8485\n",
      "Test:  [100/565]  eta: 0:02:31  loss_labels: 1.4365 (1.5013)  loss: 1.4952 (1.5013)  time: 0.1347\n",
      "Test:  [200/565]  eta: 0:01:24  loss_labels: 1.2305 (1.4130)  loss: 1.3954 (1.4130)  time: 0.1270\n",
      "Test:  [300/565]  eta: 0:00:52  loss_labels: 1.1348 (1.3426)  loss: 1.1348 (1.3426)  time: 0.1467\n",
      "Test:  [400/565]  eta: 0:00:30  loss_labels: 1.3492 (1.3680)  loss: 1.2628 (1.3680)  time: 0.1573\n",
      "Test:  [500/565]  eta: 0:00:11  loss_labels: 1.1234 (1.3391)  loss: 1.2739 (1.3391)  time: 0.1165\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1219 (1.3182)  loss: 1.0638 (1.3182)  time: 0.1395\n",
      "Test: Total time: 0:01:37 (0.1731 s / it)\n",
      "Averaged stats: loss_labels: 1.1219 (1.3182)  loss: 1.0638 (1.3182)\n",
      "acc: 0.8029136061668396\n",
      "top 1 and top 5 accuracies {'top1': 0.8029136431618014, 'top5': 0.9038386971694455, 'loss': tensor(0.0103, device='cuda:0')}\n",
      "Epoch: [31]  [   0/5008]  eta: 1 day, 8:47:54  lr: 0.000500  loss_labels: 0.0643 (0.0643)  loss: 0.0643 (0.0643)  time: 23.5772\n",
      "Epoch: [31]  [ 100/5008]  eta: 0:32:54  lr: 0.000500  loss_labels: 0.0519 (0.0573)  loss: 0.0559 (0.0573)  time: 0.1712\n",
      "Epoch: [31]  [ 200/5008]  eta: 0:23:00  lr: 0.000500  loss_labels: 0.0611 (0.0597)  loss: 0.0517 (0.0597)  time: 0.1701\n",
      "Epoch: [31]  [ 300/5008]  eta: 0:19:28  lr: 0.000500  loss_labels: 0.0564 (0.0601)  loss: 0.0648 (0.0601)  time: 0.1725\n",
      "Epoch: [31]  [ 400/5008]  eta: 0:17:35  lr: 0.000500  loss_labels: 0.0521 (0.0587)  loss: 0.0527 (0.0587)  time: 0.1720\n",
      "Epoch: [31]  [ 500/5008]  eta: 0:16:22  lr: 0.000500  loss_labels: 0.0512 (0.0578)  loss: 0.0455 (0.0578)  time: 0.1736\n",
      "Epoch: [31]  [ 600/5008]  eta: 0:15:28  lr: 0.000500  loss_labels: 0.0555 (0.0583)  loss: 0.0467 (0.0583)  time: 0.1744\n",
      "Epoch: [31]  [ 700/5008]  eta: 0:14:47  lr: 0.000500  loss_labels: 0.0544 (0.0587)  loss: 0.0619 (0.0587)  time: 0.1971\n",
      "Epoch: [31]  [ 800/5008]  eta: 0:14:10  lr: 0.000500  loss_labels: 0.0568 (0.0591)  loss: 0.0566 (0.0591)  time: 0.1739\n",
      "Epoch: [31]  [ 900/5008]  eta: 0:13:37  lr: 0.000500  loss_labels: 0.0569 (0.0591)  loss: 0.0504 (0.0591)  time: 0.1720\n",
      "Epoch: [31]  [1000/5008]  eta: 0:13:05  lr: 0.000500  loss_labels: 0.0515 (0.0591)  loss: 0.0515 (0.0591)  time: 0.1701\n",
      "Epoch: [31]  [1100/5008]  eta: 0:12:35  lr: 0.000500  loss_labels: 0.0546 (0.0591)  loss: 0.0565 (0.0591)  time: 0.1675\n",
      "Epoch: [31]  [1200/5008]  eta: 0:12:09  lr: 0.000500  loss_labels: 0.0552 (0.0591)  loss: 0.0587 (0.0591)  time: 0.1704\n",
      "Epoch: [31]  [1300/5008]  eta: 0:11:43  lr: 0.000500  loss_labels: 0.0511 (0.0588)  loss: 0.0599 (0.0588)  time: 0.1687\n",
      "Epoch: [31]  [1400/5008]  eta: 0:11:19  lr: 0.000500  loss_labels: 0.0572 (0.0588)  loss: 0.0530 (0.0588)  time: 0.1692\n",
      "Epoch: [31]  [1500/5008]  eta: 0:10:56  lr: 0.000500  loss_labels: 0.0568 (0.0591)  loss: 0.0531 (0.0591)  time: 0.1704\n",
      "Epoch: [31]  [1600/5008]  eta: 0:10:33  lr: 0.000500  loss_labels: 0.0539 (0.0590)  loss: 0.0485 (0.0590)  time: 0.1702\n",
      "Epoch: [31]  [1700/5008]  eta: 0:10:12  lr: 0.000500  loss_labels: 0.0546 (0.0591)  loss: 0.0546 (0.0591)  time: 0.1692\n",
      "Epoch: [31]  [1800/5008]  eta: 0:09:50  lr: 0.000500  loss_labels: 0.0509 (0.0589)  loss: 0.0503 (0.0589)  time: 0.1674\n",
      "Epoch: [31]  [1900/5008]  eta: 0:09:29  lr: 0.000500  loss_labels: 0.0548 (0.0588)  loss: 0.0475 (0.0588)  time: 0.1689\n",
      "Epoch: [31]  [2000/5008]  eta: 0:09:09  lr: 0.000500  loss_labels: 0.0589 (0.0590)  loss: 0.0527 (0.0590)  time: 0.1690\n",
      "Epoch: [31]  [2100/5008]  eta: 0:08:49  lr: 0.000500  loss_labels: 0.0593 (0.0592)  loss: 0.0618 (0.0592)  time: 0.1685\n",
      "Epoch: [31]  [2200/5008]  eta: 0:08:29  lr: 0.000500  loss_labels: 0.0532 (0.0592)  loss: 0.0510 (0.0592)  time: 0.1694\n",
      "Epoch: [31]  [2300/5008]  eta: 0:08:09  lr: 0.000500  loss_labels: 0.0541 (0.0592)  loss: 0.0541 (0.0592)  time: 0.1700\n",
      "Epoch: [31]  [2400/5008]  eta: 0:07:50  lr: 0.000500  loss_labels: 0.0536 (0.0591)  loss: 0.0569 (0.0591)  time: 0.1708\n",
      "Epoch: [31]  [2500/5008]  eta: 0:07:31  lr: 0.000500  loss_labels: 0.0548 (0.0591)  loss: 0.0570 (0.0591)  time: 0.1695\n",
      "Epoch: [31]  [2600/5008]  eta: 0:07:12  lr: 0.000500  loss_labels: 0.0560 (0.0591)  loss: 0.0583 (0.0591)  time: 0.1701\n",
      "Epoch: [31]  [2700/5008]  eta: 0:06:53  lr: 0.000500  loss_labels: 0.0553 (0.0592)  loss: 0.0491 (0.0592)  time: 0.1691\n",
      "Epoch: [31]  [2800/5008]  eta: 0:06:34  lr: 0.000500  loss_labels: 0.0607 (0.0593)  loss: 0.0578 (0.0593)  time: 0.1701\n",
      "Epoch: [31]  [2900/5008]  eta: 0:06:16  lr: 0.000500  loss_labels: 0.0510 (0.0592)  loss: 0.0494 (0.0592)  time: 0.1702\n",
      "Epoch: [31]  [3000/5008]  eta: 0:05:57  lr: 0.000500  loss_labels: 0.0552 (0.0591)  loss: 0.0547 (0.0591)  time: 0.1681\n",
      "Epoch: [31]  [3100/5008]  eta: 0:05:39  lr: 0.000500  loss_labels: 0.0529 (0.0590)  loss: 0.0487 (0.0590)  time: 0.1681\n",
      "Epoch: [31]  [3200/5008]  eta: 0:05:20  lr: 0.000500  loss_labels: 0.0574 (0.0590)  loss: 0.0631 (0.0590)  time: 0.1681\n",
      "Epoch: [31]  [3300/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.0529 (0.0589)  loss: 0.0639 (0.0589)  time: 0.1702\n",
      "Epoch: [31]  [3400/5008]  eta: 0:04:46  lr: 0.000500  loss_labels: 0.0507 (0.0587)  loss: 0.0495 (0.0587)  time: 0.1686\n",
      "Epoch: [31]  [3500/5008]  eta: 0:04:27  lr: 0.000500  loss_labels: 0.0503 (0.0587)  loss: 0.0651 (0.0587)  time: 0.1683\n",
      "Epoch: [31]  [3600/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.0553 (0.0587)  loss: 0.0523 (0.0587)  time: 0.1691\n",
      "Epoch: [31]  [3700/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 0.0562 (0.0586)  loss: 0.0605 (0.0586)  time: 0.1697\n",
      "Epoch: [31]  [3800/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 0.0557 (0.0586)  loss: 0.0541 (0.0586)  time: 0.1706\n",
      "Epoch: [31]  [3900/5008]  eta: 0:03:16  lr: 0.000500  loss_labels: 0.0536 (0.0586)  loss: 0.0531 (0.0586)  time: 0.1706\n",
      "Epoch: [31]  [4000/5008]  eta: 0:02:58  lr: 0.000500  loss_labels: 0.0562 (0.0586)  loss: 0.0501 (0.0586)  time: 0.1705\n",
      "Epoch: [31]  [4100/5008]  eta: 0:02:40  lr: 0.000500  loss_labels: 0.0619 (0.0587)  loss: 0.0580 (0.0587)  time: 0.1708\n",
      "Epoch: [31]  [4200/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.0483 (0.0586)  loss: 0.0593 (0.0586)  time: 0.1704\n",
      "Epoch: [31]  [4300/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.0501 (0.0585)  loss: 0.0467 (0.0585)  time: 0.1691\n",
      "Epoch: [31]  [4400/5008]  eta: 0:01:47  lr: 0.000500  loss_labels: 0.0547 (0.0585)  loss: 0.0485 (0.0585)  time: 0.1697\n",
      "Epoch: [31]  [4500/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.0587 (0.0586)  loss: 0.0600 (0.0586)  time: 0.1697\n",
      "Epoch: [31]  [4600/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 0.0473 (0.0584)  loss: 0.0469 (0.0584)  time: 0.1684\n",
      "Epoch: [31]  [4700/5008]  eta: 0:01:06  lr: 0.000500  loss_labels: 0.0510 (0.0584)  loss: 0.0510 (0.0584)  time: 2.7260\n",
      "Epoch: [31]  [4800/5008]  eta: 0:00:44  lr: 0.000500  loss_labels: 0.0526 (0.0584)  loss: 0.0454 (0.0584)  time: 0.1666\n",
      "Epoch: [31]  [4900/5008]  eta: 0:00:23  lr: 0.000500  loss_labels: 0.0560 (0.0584)  loss: 0.0528 (0.0584)  time: 0.1682\n",
      "Epoch: [31]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0563 (0.0583)  loss: 0.0418 (0.0583)  time: 0.1705\n",
      "Epoch: [31]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0554 (0.0583)  loss: 0.0369 (0.0583)  time: 0.1708\n",
      "Epoch: [31] Total time: 0:17:46 (0.2131 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0554 (0.0583)  loss: 0.0369 (0.0583)\n",
      "Test:  [  0/565]  eta: 3:19:45  loss_labels: 1.0630 (1.0630)  loss: 1.0630 (1.0630)  time: 21.2132\n",
      "Test:  [100/565]  eta: 0:02:36  loss_labels: 1.4905 (1.5876)  loss: 1.5218 (1.5876)  time: 0.1391\n",
      "Test:  [200/565]  eta: 0:01:28  loss_labels: 1.2522 (1.4941)  loss: 1.4909 (1.4941)  time: 0.1286\n",
      "Test:  [300/565]  eta: 0:00:54  loss_labels: 1.1452 (1.4130)  loss: 1.2025 (1.4130)  time: 0.1247\n",
      "Test:  [400/565]  eta: 0:00:30  loss_labels: 1.4137 (1.4383)  loss: 1.4534 (1.4383)  time: 0.1109\n",
      "Test:  [500/565]  eta: 0:00:11  loss_labels: 1.1469 (1.4071)  loss: 1.4162 (1.4071)  time: 0.1094\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1887 (1.3878)  loss: 1.0875 (1.3878)  time: 0.1364\n",
      "Test: Total time: 0:01:34 (0.1678 s / it)\n",
      "Averaged stats: loss_labels: 1.1887 (1.3878)  loss: 1.0875 (1.3878)\n",
      "acc: 0.7951033115386963\n",
      "top 1 and top 5 accuracies {'top1': 0.7951033069295962, 'top5': 0.9002797319005151, 'loss': tensor(0.0109, device='cuda:0')}\n",
      "Epoch: [32]  [   0/5008]  eta: 1 day, 7:35:07  lr: 0.000500  loss_labels: 0.0853 (0.0853)  loss: 0.0853 (0.0853)  time: 22.7052\n",
      "Epoch: [32]  [ 100/5008]  eta: 0:31:54  lr: 0.000500  loss_labels: 0.0547 (0.0569)  loss: 0.0484 (0.0569)  time: 0.1666\n",
      "Epoch: [32]  [ 200/5008]  eta: 0:22:23  lr: 0.000500  loss_labels: 0.0550 (0.0566)  loss: 0.0475 (0.0566)  time: 0.1687\n",
      "Epoch: [32]  [ 300/5008]  eta: 0:18:59  lr: 0.000500  loss_labels: 0.0520 (0.0572)  loss: 0.0659 (0.0572)  time: 0.1670\n",
      "Epoch: [32]  [ 400/5008]  eta: 0:17:09  lr: 0.000500  loss_labels: 0.0514 (0.0566)  loss: 0.0536 (0.0566)  time: 0.1678\n",
      "Epoch: [32]  [ 500/5008]  eta: 0:16:02  lr: 0.000500  loss_labels: 0.0469 (0.0555)  loss: 0.0329 (0.0555)  time: 0.1676\n",
      "Epoch: [32]  [ 600/5008]  eta: 0:15:23  lr: 0.000500  loss_labels: 0.0561 (0.0560)  loss: 0.0518 (0.0560)  time: 0.1682\n",
      "Epoch: [32]  [ 700/5008]  eta: 0:14:36  lr: 0.000500  loss_labels: 0.0542 (0.0562)  loss: 0.0569 (0.0562)  time: 0.1673\n",
      "Epoch: [32]  [ 800/5008]  eta: 0:13:56  lr: 0.000500  loss_labels: 0.0502 (0.0560)  loss: 0.0469 (0.0560)  time: 0.1668\n",
      "Epoch: [32]  [ 900/5008]  eta: 0:13:21  lr: 0.000500  loss_labels: 0.0525 (0.0556)  loss: 0.0463 (0.0556)  time: 0.1665\n",
      "Epoch: [32]  [1000/5008]  eta: 0:12:50  lr: 0.000500  loss_labels: 0.0527 (0.0557)  loss: 0.0495 (0.0557)  time: 0.1663\n",
      "Epoch: [32]  [1100/5008]  eta: 0:12:22  lr: 0.000500  loss_labels: 0.0558 (0.0561)  loss: 0.0565 (0.0561)  time: 0.1673\n",
      "Epoch: [32]  [1200/5008]  eta: 0:11:55  lr: 0.000500  loss_labels: 0.0545 (0.0560)  loss: 0.0609 (0.0560)  time: 0.1642\n",
      "Epoch: [32]  [1300/5008]  eta: 0:11:30  lr: 0.000500  loss_labels: 0.0486 (0.0556)  loss: 0.0394 (0.0556)  time: 0.1639\n",
      "Epoch: [32]  [1400/5008]  eta: 0:11:06  lr: 0.000500  loss_labels: 0.0519 (0.0556)  loss: 0.0566 (0.0556)  time: 0.1640\n",
      "Epoch: [32]  [1500/5008]  eta: 0:10:42  lr: 0.000500  loss_labels: 0.0516 (0.0557)  loss: 0.0533 (0.0557)  time: 0.1649\n",
      "Epoch: [32]  [1600/5008]  eta: 0:10:20  lr: 0.000500  loss_labels: 0.0524 (0.0558)  loss: 0.0579 (0.0558)  time: 0.1648\n",
      "Epoch: [32]  [1700/5008]  eta: 0:09:59  lr: 0.000500  loss_labels: 0.0500 (0.0559)  loss: 0.0535 (0.0559)  time: 0.1646\n",
      "Epoch: [32]  [1800/5008]  eta: 0:09:37  lr: 0.000500  loss_labels: 0.0545 (0.0560)  loss: 0.0473 (0.0560)  time: 0.1644\n",
      "Epoch: [32]  [1900/5008]  eta: 0:09:17  lr: 0.000500  loss_labels: 0.0554 (0.0561)  loss: 0.0554 (0.0561)  time: 0.1634\n",
      "Epoch: [32]  [2000/5008]  eta: 0:08:57  lr: 0.000500  loss_labels: 0.0535 (0.0562)  loss: 0.0493 (0.0562)  time: 0.1660\n",
      "Epoch: [32]  [2100/5008]  eta: 0:08:37  lr: 0.000500  loss_labels: 0.0527 (0.0560)  loss: 0.0549 (0.0560)  time: 0.1672\n",
      "Epoch: [32]  [2200/5008]  eta: 0:08:18  lr: 0.000500  loss_labels: 0.0534 (0.0561)  loss: 0.0495 (0.0561)  time: 0.1682\n",
      "Epoch: [32]  [2300/5008]  eta: 0:07:59  lr: 0.000500  loss_labels: 0.0512 (0.0561)  loss: 0.0521 (0.0561)  time: 0.1681\n",
      "Epoch: [32]  [2400/5008]  eta: 0:07:40  lr: 0.000500  loss_labels: 0.0548 (0.0560)  loss: 0.0478 (0.0560)  time: 0.1676\n",
      "Epoch: [32]  [2500/5008]  eta: 0:07:22  lr: 0.000500  loss_labels: 0.0525 (0.0561)  loss: 0.0469 (0.0561)  time: 0.1670\n",
      "Epoch: [32]  [2600/5008]  eta: 0:07:03  lr: 0.000500  loss_labels: 0.0495 (0.0559)  loss: 0.0365 (0.0559)  time: 0.1681\n",
      "Epoch: [32]  [2700/5008]  eta: 0:06:45  lr: 0.000500  loss_labels: 0.0506 (0.0559)  loss: 0.0383 (0.0559)  time: 0.1688\n",
      "Epoch: [32]  [2800/5008]  eta: 0:06:27  lr: 0.000500  loss_labels: 0.0590 (0.0561)  loss: 0.0541 (0.0561)  time: 0.1683\n",
      "Epoch: [32]  [2900/5008]  eta: 0:06:09  lr: 0.000500  loss_labels: 0.0521 (0.0562)  loss: 0.0477 (0.0562)  time: 0.1673\n",
      "Epoch: [32]  [3000/5008]  eta: 0:05:51  lr: 0.000500  loss_labels: 0.0590 (0.0563)  loss: 0.0556 (0.0563)  time: 0.1681\n",
      "Epoch: [32]  [3100/5008]  eta: 0:05:33  lr: 0.000500  loss_labels: 0.0496 (0.0562)  loss: 0.0442 (0.0562)  time: 0.1661\n",
      "Epoch: [32]  [3200/5008]  eta: 0:05:15  lr: 0.000500  loss_labels: 0.0494 (0.0561)  loss: 0.0466 (0.0561)  time: 0.1673\n",
      "Epoch: [32]  [3300/5008]  eta: 0:04:57  lr: 0.000500  loss_labels: 0.0507 (0.0560)  loss: 0.0585 (0.0560)  time: 0.1668\n",
      "Epoch: [32]  [3400/5008]  eta: 0:04:39  lr: 0.000500  loss_labels: 0.0508 (0.0559)  loss: 0.0508 (0.0559)  time: 0.1690\n",
      "Epoch: [32]  [3500/5008]  eta: 0:04:22  lr: 0.000500  loss_labels: 0.0582 (0.0561)  loss: 0.0673 (0.0561)  time: 0.1686\n",
      "Epoch: [32]  [3600/5008]  eta: 0:04:04  lr: 0.000500  loss_labels: 0.0533 (0.0561)  loss: 0.0447 (0.0561)  time: 0.1669\n",
      "Epoch: [32]  [3700/5008]  eta: 0:03:46  lr: 0.000500  loss_labels: 0.0491 (0.0560)  loss: 0.0509 (0.0560)  time: 0.1655\n",
      "Epoch: [32]  [3800/5008]  eta: 0:03:29  lr: 0.000500  loss_labels: 0.0515 (0.0559)  loss: 0.0477 (0.0559)  time: 0.1670\n",
      "Epoch: [32]  [3900/5008]  eta: 0:03:11  lr: 0.000500  loss_labels: 0.0474 (0.0558)  loss: 0.0464 (0.0558)  time: 0.1668\n",
      "Epoch: [32]  [4000/5008]  eta: 0:02:54  lr: 0.000500  loss_labels: 0.0456 (0.0557)  loss: 0.0391 (0.0557)  time: 0.1675\n",
      "Epoch: [32]  [4100/5008]  eta: 0:02:36  lr: 0.000500  loss_labels: 0.0498 (0.0557)  loss: 0.0498 (0.0557)  time: 0.1660\n",
      "Epoch: [32]  [4200/5008]  eta: 0:02:19  lr: 0.000500  loss_labels: 0.0477 (0.0556)  loss: 0.0606 (0.0556)  time: 0.1672\n",
      "Epoch: [32]  [4300/5008]  eta: 0:02:02  lr: 0.000500  loss_labels: 0.0483 (0.0556)  loss: 0.0478 (0.0556)  time: 0.1694\n",
      "Epoch: [32]  [4400/5008]  eta: 0:01:44  lr: 0.000500  loss_labels: 0.0554 (0.0556)  loss: 0.0565 (0.0556)  time: 0.1665\n",
      "Epoch: [32]  [4500/5008]  eta: 0:01:27  lr: 0.000500  loss_labels: 0.0478 (0.0555)  loss: 0.0477 (0.0555)  time: 0.1665\n",
      "Epoch: [32]  [4600/5008]  eta: 0:01:10  lr: 0.000500  loss_labels: 0.0495 (0.0554)  loss: 0.0564 (0.0554)  time: 0.1664\n",
      "Epoch: [32]  [4700/5008]  eta: 0:00:53  lr: 0.000500  loss_labels: 0.0515 (0.0555)  loss: 0.0439 (0.0555)  time: 0.1677\n",
      "Epoch: [32]  [4800/5008]  eta: 0:00:35  lr: 0.000500  loss_labels: 0.0522 (0.0555)  loss: 0.0529 (0.0555)  time: 0.1676\n",
      "Epoch: [32]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0546 (0.0555)  loss: 0.0549 (0.0555)  time: 0.1682\n",
      "Epoch: [32]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0569 (0.0555)  loss: 0.0633 (0.0555)  time: 0.1682\n",
      "Epoch: [32]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0569 (0.0556)  loss: 0.0547 (0.0556)  time: 0.1691\n",
      "Epoch: [32] Total time: 0:14:21 (0.1720 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0569 (0.0556)  loss: 0.0547 (0.0556)\n",
      "Test:  [  0/565]  eta: 2:37:05  loss_labels: 1.3230 (1.3230)  loss: 1.3230 (1.3230)  time: 16.6818\n",
      "Test:  [100/565]  eta: 0:02:15  loss_labels: 1.3738 (1.5421)  loss: 1.4400 (1.5421)  time: 0.1178\n",
      "Test:  [200/565]  eta: 0:01:16  loss_labels: 1.1766 (1.4469)  loss: 1.5694 (1.4469)  time: 0.1160\n",
      "Test:  [300/565]  eta: 0:00:47  loss_labels: 1.1244 (1.3742)  loss: 0.9652 (1.3742)  time: 0.1279\n",
      "Test:  [400/565]  eta: 0:00:27  loss_labels: 1.3672 (1.3979)  loss: 1.3639 (1.3979)  time: 0.1119\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.1471 (1.3586)  loss: 1.3003 (1.3586)  time: 0.1092\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1869 (1.3386)  loss: 1.0113 (1.3386)  time: 0.1332\n",
      "Test: Total time: 0:01:27 (0.1547 s / it)\n",
      "Averaged stats: loss_labels: 1.1869 (1.3386)  loss: 1.0113 (1.3386)\n",
      "acc: 0.8030659556388855\n",
      "top 1 and top 5 accuracies {'top1': 0.8030659724145571, 'top5': 0.9040879632194095, 'loss': tensor(0.0105, device='cuda:0')}\n",
      "Epoch: [33]  [   0/5008]  eta: 1 day, 5:14:26  lr: 0.000500  loss_labels: 0.0683 (0.0683)  loss: 0.0683 (0.0683)  time: 21.0196\n",
      "Epoch: [33]  [ 100/5008]  eta: 0:30:34  lr: 0.000500  loss_labels: 0.0502 (0.0533)  loss: 0.0461 (0.0533)  time: 0.1664\n",
      "Epoch: [33]  [ 200/5008]  eta: 0:21:42  lr: 0.000500  loss_labels: 0.0505 (0.0547)  loss: 0.0556 (0.0547)  time: 0.1667\n",
      "Epoch: [33]  [ 300/5008]  eta: 0:18:34  lr: 0.000500  loss_labels: 0.0497 (0.0545)  loss: 0.0457 (0.0545)  time: 0.1689\n",
      "Epoch: [33]  [ 400/5008]  eta: 0:16:51  lr: 0.000500  loss_labels: 0.0470 (0.0538)  loss: 0.0558 (0.0538)  time: 0.1660\n",
      "Epoch: [33]  [ 500/5008]  eta: 0:15:42  lr: 0.000500  loss_labels: 0.0507 (0.0541)  loss: 0.0417 (0.0541)  time: 0.1677\n",
      "Epoch: [33]  [ 600/5008]  eta: 0:14:51  lr: 0.000500  loss_labels: 0.0534 (0.0550)  loss: 0.0450 (0.0550)  time: 0.1673\n",
      "Epoch: [33]  [ 700/5008]  eta: 0:14:09  lr: 0.000500  loss_labels: 0.0512 (0.0549)  loss: 0.0535 (0.0549)  time: 0.1669\n",
      "Epoch: [33]  [ 800/5008]  eta: 0:13:34  lr: 0.000500  loss_labels: 0.0524 (0.0550)  loss: 0.0564 (0.0550)  time: 0.1671\n",
      "Epoch: [33]  [ 900/5008]  eta: 0:13:01  lr: 0.000500  loss_labels: 0.0479 (0.0547)  loss: 0.0434 (0.0547)  time: 0.1657\n",
      "Epoch: [33]  [1000/5008]  eta: 0:12:33  lr: 0.000500  loss_labels: 0.0479 (0.0542)  loss: 0.0515 (0.0542)  time: 0.1654\n",
      "Epoch: [33]  [1100/5008]  eta: 0:20:16  lr: 0.000500  loss_labels: 0.0487 (0.0540)  loss: 0.0487 (0.0540)  time: 0.1655\n",
      "Epoch: [33]  [1200/5008]  eta: 0:18:59  lr: 0.000500  loss_labels: 0.0505 (0.0538)  loss: 0.0512 (0.0538)  time: 0.1660\n",
      "Epoch: [33]  [1300/5008]  eta: 0:17:51  lr: 0.000500  loss_labels: 0.0457 (0.0536)  loss: 0.0421 (0.0536)  time: 0.1690\n",
      "Epoch: [33]  [1400/5008]  eta: 0:16:51  lr: 0.000500  loss_labels: 0.0517 (0.0535)  loss: 0.0510 (0.0535)  time: 0.1684\n",
      "Epoch: [33]  [1500/5008]  eta: 0:15:57  lr: 0.000500  loss_labels: 0.0504 (0.0534)  loss: 0.0477 (0.0534)  time: 0.1677\n",
      "Epoch: [33]  [1600/5008]  eta: 0:15:07  lr: 0.000500  loss_labels: 0.0489 (0.0533)  loss: 0.0480 (0.0533)  time: 0.1679\n",
      "Epoch: [33]  [1700/5008]  eta: 0:14:22  lr: 0.000500  loss_labels: 0.0531 (0.0534)  loss: 0.0601 (0.0534)  time: 0.1684\n",
      "Epoch: [33]  [1800/5008]  eta: 0:13:39  lr: 0.000500  loss_labels: 0.0430 (0.0532)  loss: 0.0497 (0.0532)  time: 0.1694\n",
      "Epoch: [33]  [1900/5008]  eta: 0:13:00  lr: 0.000500  loss_labels: 0.0478 (0.0530)  loss: 0.0488 (0.0530)  time: 0.1690\n",
      "Epoch: [33]  [2000/5008]  eta: 0:12:22  lr: 0.000500  loss_labels: 0.0539 (0.0532)  loss: 0.0536 (0.0532)  time: 0.1673\n",
      "Epoch: [33]  [2100/5008]  eta: 0:11:46  lr: 0.000500  loss_labels: 0.0517 (0.0532)  loss: 0.0553 (0.0532)  time: 0.1673\n",
      "Epoch: [33]  [2200/5008]  eta: 0:11:12  lr: 0.000500  loss_labels: 0.0498 (0.0531)  loss: 0.0558 (0.0531)  time: 0.1669\n",
      "Epoch: [33]  [2300/5008]  eta: 0:10:40  lr: 0.000500  loss_labels: 0.0485 (0.0532)  loss: 0.0483 (0.0532)  time: 0.1689\n",
      "Epoch: [33]  [2400/5008]  eta: 0:10:09  lr: 0.000500  loss_labels: 0.0541 (0.0533)  loss: 0.0510 (0.0533)  time: 0.1692\n",
      "Epoch: [33]  [2500/5008]  eta: 0:09:39  lr: 0.000500  loss_labels: 0.0499 (0.0533)  loss: 0.0494 (0.0533)  time: 0.1680\n",
      "Epoch: [33]  [2600/5008]  eta: 0:09:10  lr: 0.000500  loss_labels: 0.0486 (0.0532)  loss: 0.0518 (0.0532)  time: 0.1678\n",
      "Epoch: [33]  [2700/5008]  eta: 0:08:42  lr: 0.000500  loss_labels: 0.0501 (0.0532)  loss: 0.0546 (0.0532)  time: 0.1705\n",
      "Epoch: [33]  [2800/5008]  eta: 0:08:15  lr: 0.000500  loss_labels: 0.0549 (0.0534)  loss: 0.0475 (0.0534)  time: 0.1690\n",
      "Epoch: [33]  [2900/5008]  eta: 0:07:48  lr: 0.000500  loss_labels: 0.0526 (0.0536)  loss: 0.0472 (0.0536)  time: 0.1696\n",
      "Epoch: [33]  [3000/5008]  eta: 0:07:23  lr: 0.000500  loss_labels: 0.0506 (0.0536)  loss: 0.0580 (0.0536)  time: 0.1682\n",
      "Epoch: [33]  [3100/5008]  eta: 0:06:57  lr: 0.000500  loss_labels: 0.0457 (0.0535)  loss: 0.0464 (0.0535)  time: 0.1684\n",
      "Epoch: [33]  [3200/5008]  eta: 0:06:33  lr: 0.000500  loss_labels: 0.0499 (0.0536)  loss: 0.0598 (0.0536)  time: 0.1684\n",
      "Epoch: [33]  [3300/5008]  eta: 0:06:08  lr: 0.000500  loss_labels: 0.0489 (0.0536)  loss: 0.0421 (0.0536)  time: 0.1682\n",
      "Epoch: [33]  [3400/5008]  eta: 0:05:44  lr: 0.000500  loss_labels: 0.0499 (0.0535)  loss: 0.0522 (0.0535)  time: 0.1688\n",
      "Epoch: [33]  [3500/5008]  eta: 0:05:21  lr: 0.000500  loss_labels: 0.0547 (0.0536)  loss: 0.0595 (0.0536)  time: 0.1681\n",
      "Epoch: [33]  [3600/5008]  eta: 0:04:58  lr: 0.000500  loss_labels: 0.0510 (0.0536)  loss: 0.0526 (0.0536)  time: 0.1686\n",
      "Epoch: [33]  [3700/5008]  eta: 0:04:35  lr: 0.000500  loss_labels: 0.0440 (0.0534)  loss: 0.0501 (0.0534)  time: 0.1695\n",
      "Epoch: [33]  [3800/5008]  eta: 0:04:13  lr: 0.000500  loss_labels: 0.0523 (0.0534)  loss: 0.0417 (0.0534)  time: 0.1701\n",
      "Epoch: [33]  [3900/5008]  eta: 0:03:51  lr: 0.000500  loss_labels: 0.0534 (0.0535)  loss: 0.0434 (0.0535)  time: 0.1717\n",
      "Epoch: [33]  [4000/5008]  eta: 0:03:29  lr: 0.000500  loss_labels: 0.0496 (0.0535)  loss: 0.0478 (0.0535)  time: 0.1699\n",
      "Epoch: [33]  [4100/5008]  eta: 0:03:07  lr: 0.000500  loss_labels: 0.0496 (0.0535)  loss: 0.0493 (0.0535)  time: 0.1694\n",
      "Epoch: [33]  [4200/5008]  eta: 0:02:46  lr: 0.000500  loss_labels: 0.0460 (0.0534)  loss: 0.0405 (0.0534)  time: 0.1692\n",
      "Epoch: [33]  [4300/5008]  eta: 0:02:25  lr: 0.000500  loss_labels: 0.0471 (0.0533)  loss: 0.0417 (0.0533)  time: 0.1686\n",
      "Epoch: [33]  [4400/5008]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.0490 (0.0533)  loss: 0.0401 (0.0533)  time: 0.1689\n",
      "Epoch: [33]  [4500/5008]  eta: 0:01:43  lr: 0.000500  loss_labels: 0.0448 (0.0533)  loss: 0.0403 (0.0533)  time: 0.1682\n",
      "Epoch: [33]  [4600/5008]  eta: 0:01:22  lr: 0.000500  loss_labels: 0.0513 (0.0533)  loss: 0.0513 (0.0533)  time: 0.1687\n",
      "Epoch: [33]  [4700/5008]  eta: 0:01:02  lr: 0.000500  loss_labels: 0.0471 (0.0532)  loss: 0.0402 (0.0532)  time: 0.1692\n",
      "Epoch: [33]  [4800/5008]  eta: 0:00:41  lr: 0.000500  loss_labels: 0.0485 (0.0532)  loss: 0.0650 (0.0532)  time: 0.1691\n",
      "Epoch: [33]  [4900/5008]  eta: 0:00:21  lr: 0.000500  loss_labels: 0.0475 (0.0532)  loss: 0.0424 (0.0532)  time: 0.1683\n",
      "Epoch: [33]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0463 (0.0532)  loss: 0.0574 (0.0532)  time: 0.1687\n",
      "Epoch: [33]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0460 (0.0532)  loss: 0.0416 (0.0532)  time: 0.1688\n",
      "Epoch: [33] Total time: 0:16:42 (0.2001 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0460 (0.0532)  loss: 0.0416 (0.0532)\n",
      "Test:  [  0/565]  eta: 2:37:03  loss_labels: 1.1882 (1.1882)  loss: 1.1882 (1.1882)  time: 16.6795\n",
      "Test:  [100/565]  eta: 0:02:15  loss_labels: 1.4943 (1.5812)  loss: 1.5469 (1.5812)  time: 0.1199\n",
      "Test:  [200/565]  eta: 0:01:15  loss_labels: 1.2332 (1.4915)  loss: 1.6118 (1.4915)  time: 0.1155\n",
      "Test:  [300/565]  eta: 0:00:46  loss_labels: 1.1633 (1.4055)  loss: 1.0313 (1.4055)  time: 0.1149\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.3981 (1.4341)  loss: 1.3756 (1.4341)  time: 0.1065\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.1389 (1.4023)  loss: 1.3787 (1.4023)  time: 0.1258\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1804 (1.3810)  loss: 1.1342 (1.3810)  time: 0.1243\n",
      "Test: Total time: 0:01:25 (0.1521 s / it)\n",
      "Averaged stats: loss_labels: 1.1804 (1.3810)  loss: 1.1342 (1.3810)\n",
      "acc: 0.8017227053642273\n",
      "top 1 and top 5 accuracies {'top1': 0.801722705367529, 'top5': 0.9023707970974353, 'loss': tensor(0.0108, device='cuda:0')}\n",
      "Epoch: [34]  [   0/5008]  eta: 1 day, 5:54:47  lr: 0.000500  loss_labels: 0.0909 (0.0909)  loss: 0.0909 (0.0909)  time: 21.5030\n",
      "Epoch: [34]  [ 100/5008]  eta: 0:30:58  lr: 0.000500  loss_labels: 0.0466 (0.0505)  loss: 0.0386 (0.0505)  time: 0.1675\n",
      "Epoch: [34]  [ 200/5008]  eta: 0:21:56  lr: 0.000500  loss_labels: 0.0437 (0.0501)  loss: 0.0443 (0.0501)  time: 0.1678\n",
      "Epoch: [34]  [ 300/5008]  eta: 0:18:43  lr: 0.000500  loss_labels: 0.0471 (0.0512)  loss: 0.0539 (0.0512)  time: 0.1686\n",
      "Epoch: [34]  [ 400/5008]  eta: 0:16:58  lr: 0.000500  loss_labels: 0.0499 (0.0510)  loss: 0.0514 (0.0510)  time: 0.1677\n",
      "Epoch: [34]  [ 500/5008]  eta: 0:15:46  lr: 0.000500  loss_labels: 0.0494 (0.0511)  loss: 0.0464 (0.0511)  time: 0.1690\n",
      "Epoch: [34]  [ 600/5008]  eta: 0:14:53  lr: 0.000500  loss_labels: 0.0508 (0.0511)  loss: 0.0473 (0.0511)  time: 0.1657\n",
      "Epoch: [34]  [ 700/5008]  eta: 0:14:10  lr: 0.000500  loss_labels: 0.0496 (0.0511)  loss: 0.0579 (0.0511)  time: 0.1686\n",
      "Epoch: [34]  [ 800/5008]  eta: 0:13:34  lr: 0.000500  loss_labels: 0.0480 (0.0512)  loss: 0.0466 (0.0512)  time: 0.1663\n",
      "Epoch: [34]  [ 900/5008]  eta: 0:13:03  lr: 0.000500  loss_labels: 0.0451 (0.0512)  loss: 0.0436 (0.0512)  time: 0.1667\n",
      "Epoch: [34]  [1000/5008]  eta: 0:12:33  lr: 0.000500  loss_labels: 0.0470 (0.0511)  loss: 0.0483 (0.0511)  time: 0.1646\n",
      "Epoch: [34]  [1100/5008]  eta: 0:12:06  lr: 0.000500  loss_labels: 0.0448 (0.0508)  loss: 0.0435 (0.0508)  time: 0.1646\n",
      "Epoch: [34]  [1200/5008]  eta: 0:11:41  lr: 0.000500  loss_labels: 0.0452 (0.0507)  loss: 0.0351 (0.0507)  time: 0.1641\n",
      "Epoch: [34]  [1300/5008]  eta: 0:11:17  lr: 0.000500  loss_labels: 0.0482 (0.0507)  loss: 0.0520 (0.0507)  time: 0.1652\n",
      "Epoch: [34]  [1400/5008]  eta: 0:10:54  lr: 0.000500  loss_labels: 0.0507 (0.0508)  loss: 0.0446 (0.0508)  time: 0.1661\n",
      "Epoch: [34]  [1500/5008]  eta: 0:10:32  lr: 0.000500  loss_labels: 0.0566 (0.0513)  loss: 0.0509 (0.0513)  time: 0.1662\n",
      "Epoch: [34]  [1600/5008]  eta: 0:10:11  lr: 0.000500  loss_labels: 0.0444 (0.0512)  loss: 0.0547 (0.0512)  time: 0.1661\n",
      "Epoch: [34]  [1700/5008]  eta: 0:09:51  lr: 0.000500  loss_labels: 0.0463 (0.0512)  loss: 0.0415 (0.0512)  time: 0.1668\n",
      "Epoch: [34]  [1800/5008]  eta: 0:09:31  lr: 0.000500  loss_labels: 0.0463 (0.0512)  loss: 0.0436 (0.0512)  time: 0.1659\n",
      "Epoch: [34]  [1900/5008]  eta: 0:09:11  lr: 0.000500  loss_labels: 0.0457 (0.0511)  loss: 0.0399 (0.0511)  time: 0.1662\n",
      "Epoch: [34]  [2000/5008]  eta: 0:08:52  lr: 0.000500  loss_labels: 0.0427 (0.0508)  loss: 0.0388 (0.0508)  time: 0.1677\n",
      "Epoch: [34]  [2100/5008]  eta: 0:08:32  lr: 0.000500  loss_labels: 0.0470 (0.0509)  loss: 0.0508 (0.0509)  time: 0.1657\n",
      "Epoch: [34]  [2200/5008]  eta: 0:08:13  lr: 0.000500  loss_labels: 0.0499 (0.0510)  loss: 0.0524 (0.0510)  time: 0.1664\n",
      "Epoch: [34]  [2300/5008]  eta: 0:07:55  lr: 0.000500  loss_labels: 0.0497 (0.0511)  loss: 0.0430 (0.0511)  time: 0.1661\n",
      "Epoch: [34]  [2400/5008]  eta: 0:07:36  lr: 0.000500  loss_labels: 0.0475 (0.0511)  loss: 0.0501 (0.0511)  time: 0.1673\n",
      "Epoch: [34]  [2500/5008]  eta: 0:07:18  lr: 0.000500  loss_labels: 0.0500 (0.0512)  loss: 0.0502 (0.0512)  time: 0.1671\n",
      "Epoch: [34]  [2600/5008]  eta: 0:07:00  lr: 0.000500  loss_labels: 0.0488 (0.0512)  loss: 0.0427 (0.0512)  time: 0.1667\n",
      "Epoch: [34]  [2700/5008]  eta: 0:06:42  lr: 0.000500  loss_labels: 0.0491 (0.0512)  loss: 0.0443 (0.0512)  time: 0.1653\n",
      "Epoch: [34]  [2800/5008]  eta: 0:06:23  lr: 0.000500  loss_labels: 0.0506 (0.0513)  loss: 0.0506 (0.0513)  time: 0.1663\n",
      "Epoch: [34]  [2900/5008]  eta: 0:06:06  lr: 0.000500  loss_labels: 0.0459 (0.0513)  loss: 0.0446 (0.0513)  time: 0.1668\n",
      "Epoch: [34]  [3000/5008]  eta: 0:05:48  lr: 0.000500  loss_labels: 0.0448 (0.0512)  loss: 0.0448 (0.0512)  time: 0.1659\n",
      "Epoch: [34]  [3100/5008]  eta: 0:05:30  lr: 0.000500  loss_labels: 0.0461 (0.0512)  loss: 0.0361 (0.0512)  time: 0.1676\n",
      "Epoch: [34]  [3200/5008]  eta: 0:05:12  lr: 0.000500  loss_labels: 0.0445 (0.0511)  loss: 0.0495 (0.0511)  time: 0.1646\n",
      "Epoch: [34]  [3300/5008]  eta: 0:04:55  lr: 0.000500  loss_labels: 0.0505 (0.0513)  loss: 0.0420 (0.0513)  time: 0.1658\n",
      "Epoch: [34]  [3400/5008]  eta: 0:04:37  lr: 0.000500  loss_labels: 0.0433 (0.0511)  loss: 0.0443 (0.0511)  time: 0.1643\n",
      "Epoch: [34]  [3500/5008]  eta: 0:04:19  lr: 0.000500  loss_labels: 0.0506 (0.0512)  loss: 0.0558 (0.0512)  time: 0.1650\n",
      "Epoch: [34]  [3600/5008]  eta: 0:04:02  lr: 0.000500  loss_labels: 0.0492 (0.0512)  loss: 0.0448 (0.0512)  time: 0.1652\n",
      "Epoch: [34]  [3700/5008]  eta: 0:03:44  lr: 0.000500  loss_labels: 0.0434 (0.0511)  loss: 0.0441 (0.0511)  time: 0.1681\n",
      "Epoch: [34]  [3800/5008]  eta: 0:03:27  lr: 0.000500  loss_labels: 0.0502 (0.0512)  loss: 0.0502 (0.0512)  time: 0.1677\n",
      "Epoch: [34]  [3900/5008]  eta: 0:03:10  lr: 0.000500  loss_labels: 0.0435 (0.0511)  loss: 0.0368 (0.0511)  time: 0.1653\n",
      "Epoch: [34]  [4000/5008]  eta: 0:02:52  lr: 0.000500  loss_labels: 0.0465 (0.0510)  loss: 0.0408 (0.0510)  time: 0.1646\n",
      "Epoch: [34]  [4100/5008]  eta: 0:02:35  lr: 0.000500  loss_labels: 0.0491 (0.0511)  loss: 0.0485 (0.0511)  time: 0.1647\n",
      "Epoch: [34]  [4200/5008]  eta: 0:02:18  lr: 0.000500  loss_labels: 0.0427 (0.0510)  loss: 0.0414 (0.0510)  time: 0.1659\n",
      "Epoch: [34]  [4300/5008]  eta: 0:02:01  lr: 0.000500  loss_labels: 0.0454 (0.0509)  loss: 0.0511 (0.0509)  time: 0.1648\n",
      "Epoch: [34]  [4400/5008]  eta: 0:01:43  lr: 0.000500  loss_labels: 0.0490 (0.0510)  loss: 0.0579 (0.0510)  time: 0.1646\n",
      "Epoch: [34]  [4500/5008]  eta: 0:01:26  lr: 0.000500  loss_labels: 0.0448 (0.0509)  loss: 0.0594 (0.0509)  time: 0.1664\n",
      "Epoch: [34]  [4600/5008]  eta: 0:01:09  lr: 0.000500  loss_labels: 0.0461 (0.0508)  loss: 0.0449 (0.0508)  time: 0.1656\n",
      "Epoch: [34]  [4700/5008]  eta: 0:00:52  lr: 0.000500  loss_labels: 0.0468 (0.0508)  loss: 0.0474 (0.0508)  time: 0.1664\n",
      "Epoch: [34]  [4800/5008]  eta: 0:00:35  lr: 0.000500  loss_labels: 0.0496 (0.0509)  loss: 0.0491 (0.0509)  time: 0.1667\n",
      "Epoch: [34]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0402 (0.0508)  loss: 0.0402 (0.0508)  time: 0.1681\n",
      "Epoch: [34]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0501 (0.0508)  loss: 0.0453 (0.0508)  time: 0.1680\n",
      "Epoch: [34]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0484 (0.0508)  loss: 0.0406 (0.0508)  time: 0.1673\n",
      "Epoch: [34] Total time: 0:14:14 (0.1706 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0484 (0.0508)  loss: 0.0406 (0.0508)\n",
      "Test:  [  0/565]  eta: 2:35:16  loss_labels: 1.0752 (1.0752)  loss: 1.0752 (1.0752)  time: 16.4897\n",
      "Test:  [100/565]  eta: 0:02:09  loss_labels: 1.5102 (1.5762)  loss: 1.6028 (1.5762)  time: 0.1108\n",
      "Test:  [200/565]  eta: 0:01:12  loss_labels: 1.1979 (1.4912)  loss: 1.5495 (1.4912)  time: 0.1108\n",
      "Test:  [300/565]  eta: 0:00:45  loss_labels: 1.1216 (1.4032)  loss: 1.0477 (1.4032)  time: 0.1195\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.4067 (1.4342)  loss: 1.2884 (1.4342)  time: 0.1033\n",
      "Test:  [500/565]  eta: 0:00:09  loss_labels: 1.1650 (1.4036)  loss: 1.4720 (1.4036)  time: 0.1086\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.2097 (1.3774)  loss: 1.2097 (1.3774)  time: 0.1205\n",
      "Test: Total time: 0:01:22 (0.1467 s / it)\n",
      "Averaged stats: loss_labels: 1.2097 (1.3774)  loss: 1.2097 (1.3774)\n",
      "acc: 0.805433988571167\n",
      "top 1 and top 5 accuracies {'top1': 0.805433999889215, 'top5': 0.9054312302664377, 'loss': tensor(0.0108, device='cuda:0')}\n",
      "Epoch: [35]  [   0/5008]  eta: 1 day, 4:41:02  lr: 0.000500  loss_labels: 0.0745 (0.0745)  loss: 0.0745 (0.0745)  time: 20.6195\n",
      "Epoch: [35]  [ 100/5008]  eta: 0:30:11  lr: 0.000500  loss_labels: 0.0465 (0.0509)  loss: 0.0508 (0.0509)  time: 0.1664\n",
      "Epoch: [35]  [ 200/5008]  eta: 0:21:30  lr: 0.000500  loss_labels: 0.0483 (0.0523)  loss: 0.0546 (0.0523)  time: 0.1670\n",
      "Epoch: [35]  [ 300/5008]  eta: 0:18:25  lr: 0.000500  loss_labels: 0.0449 (0.0516)  loss: 0.0497 (0.0516)  time: 0.1665\n",
      "Epoch: [35]  [ 400/5008]  eta: 0:16:43  lr: 0.000500  loss_labels: 0.0442 (0.0510)  loss: 0.0494 (0.0510)  time: 0.1678\n",
      "Epoch: [35]  [ 500/5008]  eta: 0:15:36  lr: 0.000500  loss_labels: 0.0514 (0.0517)  loss: 0.0397 (0.0517)  time: 0.1678\n",
      "Epoch: [35]  [ 600/5008]  eta: 0:14:47  lr: 0.000500  loss_labels: 0.0506 (0.0519)  loss: 0.0368 (0.0519)  time: 0.1688\n",
      "Epoch: [35]  [ 700/5008]  eta: 0:14:06  lr: 0.000500  loss_labels: 0.0440 (0.0516)  loss: 0.0468 (0.0516)  time: 0.1683\n",
      "Epoch: [35]  [ 800/5008]  eta: 0:13:32  lr: 0.000500  loss_labels: 0.0461 (0.0515)  loss: 0.0542 (0.0515)  time: 0.1683\n",
      "Epoch: [35]  [ 900/5008]  eta: 0:13:02  lr: 0.000500  loss_labels: 0.0434 (0.0509)  loss: 0.0465 (0.0509)  time: 0.1680\n",
      "Epoch: [35]  [1000/5008]  eta: 0:12:34  lr: 0.000500  loss_labels: 0.0482 (0.0508)  loss: 0.0469 (0.0508)  time: 0.1685\n",
      "Epoch: [35]  [1100/5008]  eta: 0:12:08  lr: 0.000500  loss_labels: 0.0470 (0.0507)  loss: 0.0521 (0.0507)  time: 0.1683\n",
      "Epoch: [35]  [1200/5008]  eta: 0:11:44  lr: 0.000500  loss_labels: 0.0458 (0.0508)  loss: 0.0388 (0.0508)  time: 0.1681\n",
      "Epoch: [35]  [1300/5008]  eta: 0:11:20  lr: 0.000500  loss_labels: 0.0444 (0.0505)  loss: 0.0390 (0.0505)  time: 0.1689\n",
      "Epoch: [35]  [1400/5008]  eta: 0:10:58  lr: 0.000500  loss_labels: 0.0455 (0.0505)  loss: 0.0438 (0.0505)  time: 0.1694\n",
      "Epoch: [35]  [1500/5008]  eta: 0:10:36  lr: 0.000500  loss_labels: 0.0469 (0.0506)  loss: 0.0495 (0.0506)  time: 0.1686\n",
      "Epoch: [35]  [1600/5008]  eta: 0:10:15  lr: 0.000500  loss_labels: 0.0430 (0.0504)  loss: 0.0472 (0.0504)  time: 0.1679\n",
      "Epoch: [35]  [1700/5008]  eta: 0:09:55  lr: 0.000500  loss_labels: 0.0440 (0.0503)  loss: 0.0379 (0.0503)  time: 0.1670\n",
      "Epoch: [35]  [1800/5008]  eta: 0:09:34  lr: 0.000500  loss_labels: 0.0468 (0.0503)  loss: 0.0452 (0.0503)  time: 0.1653\n",
      "Epoch: [35]  [1900/5008]  eta: 0:09:14  lr: 0.000500  loss_labels: 0.0443 (0.0502)  loss: 0.0522 (0.0502)  time: 0.1653\n",
      "Epoch: [35]  [2000/5008]  eta: 0:08:55  lr: 0.000500  loss_labels: 0.0442 (0.0500)  loss: 0.0308 (0.0500)  time: 0.1662\n",
      "Epoch: [35]  [2100/5008]  eta: 0:08:35  lr: 0.000500  loss_labels: 0.0481 (0.0501)  loss: 0.0395 (0.0501)  time: 0.1662\n",
      "Epoch: [35]  [2200/5008]  eta: 0:08:16  lr: 0.000500  loss_labels: 0.0456 (0.0500)  loss: 0.0440 (0.0500)  time: 0.1663\n",
      "Epoch: [35]  [2300/5008]  eta: 0:07:57  lr: 0.000500  loss_labels: 0.0472 (0.0501)  loss: 0.0453 (0.0501)  time: 0.1668\n",
      "Epoch: [35]  [2400/5008]  eta: 0:07:39  lr: 0.000500  loss_labels: 0.0445 (0.0500)  loss: 0.0472 (0.0500)  time: 0.1663\n",
      "Epoch: [35]  [2500/5008]  eta: 0:07:20  lr: 0.000500  loss_labels: 0.0434 (0.0501)  loss: 0.0377 (0.0501)  time: 0.1672\n",
      "Epoch: [35]  [2600/5008]  eta: 0:07:02  lr: 0.000500  loss_labels: 0.0465 (0.0501)  loss: 0.0515 (0.0501)  time: 0.1667\n",
      "Epoch: [35]  [2700/5008]  eta: 0:06:43  lr: 0.000500  loss_labels: 0.0431 (0.0500)  loss: 0.0386 (0.0500)  time: 0.1656\n",
      "Epoch: [35]  [2800/5008]  eta: 0:06:25  lr: 0.000500  loss_labels: 0.0425 (0.0500)  loss: 0.0425 (0.0500)  time: 0.1662\n",
      "Epoch: [35]  [2900/5008]  eta: 0:06:07  lr: 0.000500  loss_labels: 0.0482 (0.0500)  loss: 0.0404 (0.0500)  time: 0.1650\n",
      "Epoch: [35]  [3000/5008]  eta: 0:05:49  lr: 0.000500  loss_labels: 0.0438 (0.0499)  loss: 0.0438 (0.0499)  time: 0.1648\n",
      "Epoch: [35]  [3100/5008]  eta: 0:05:31  lr: 0.000500  loss_labels: 0.0433 (0.0498)  loss: 0.0349 (0.0498)  time: 0.1673\n",
      "Epoch: [35]  [3200/5008]  eta: 0:05:13  lr: 0.000500  loss_labels: 0.0473 (0.0499)  loss: 0.0502 (0.0499)  time: 0.1669\n",
      "Epoch: [35]  [3300/5008]  eta: 0:04:56  lr: 0.000500  loss_labels: 0.0444 (0.0498)  loss: 0.0382 (0.0498)  time: 0.1664\n",
      "Epoch: [35]  [3400/5008]  eta: 0:04:38  lr: 0.000500  loss_labels: 0.0434 (0.0498)  loss: 0.0376 (0.0498)  time: 0.1670\n",
      "Epoch: [35]  [3500/5008]  eta: 0:04:20  lr: 0.000500  loss_labels: 0.0442 (0.0498)  loss: 0.0438 (0.0498)  time: 0.1662\n",
      "Epoch: [35]  [3600/5008]  eta: 0:04:03  lr: 0.000500  loss_labels: 0.0432 (0.0497)  loss: 0.0500 (0.0497)  time: 0.1660\n",
      "Epoch: [35]  [3700/5008]  eta: 0:03:45  lr: 0.000500  loss_labels: 0.0421 (0.0496)  loss: 0.0386 (0.0496)  time: 0.1663\n",
      "Epoch: [35]  [3800/5008]  eta: 0:03:28  lr: 0.000500  loss_labels: 0.0459 (0.0496)  loss: 0.0422 (0.0496)  time: 0.1660\n",
      "Epoch: [35]  [3900/5008]  eta: 0:03:10  lr: 0.000500  loss_labels: 0.0468 (0.0496)  loss: 0.0401 (0.0496)  time: 0.1642\n",
      "Epoch: [35]  [4000/5008]  eta: 0:02:53  lr: 0.000500  loss_labels: 0.0462 (0.0495)  loss: 0.0435 (0.0495)  time: 0.1665\n",
      "Epoch: [35]  [4100/5008]  eta: 0:02:36  lr: 0.000500  loss_labels: 0.0447 (0.0495)  loss: 0.0537 (0.0495)  time: 0.1666\n",
      "Epoch: [35]  [4200/5008]  eta: 0:02:18  lr: 0.000500  loss_labels: 0.0415 (0.0495)  loss: 0.0431 (0.0495)  time: 0.1649\n",
      "Epoch: [35]  [4300/5008]  eta: 0:02:01  lr: 0.000500  loss_labels: 0.0476 (0.0495)  loss: 0.0530 (0.0495)  time: 0.1652\n",
      "Epoch: [35]  [4400/5008]  eta: 0:01:44  lr: 0.000500  loss_labels: 0.0462 (0.0495)  loss: 0.0487 (0.0495)  time: 0.1647\n",
      "Epoch: [35]  [4500/5008]  eta: 0:01:27  lr: 0.000500  loss_labels: 0.0430 (0.0495)  loss: 0.0560 (0.0495)  time: 0.1644\n",
      "Epoch: [35]  [4600/5008]  eta: 0:01:09  lr: 0.000500  loss_labels: 0.0424 (0.0494)  loss: 0.0401 (0.0494)  time: 0.1641\n",
      "Epoch: [35]  [4700/5008]  eta: 0:00:52  lr: 0.000500  loss_labels: 0.0455 (0.0494)  loss: 0.0471 (0.0494)  time: 0.1645\n",
      "Epoch: [35]  [4800/5008]  eta: 0:00:35  lr: 0.000500  loss_labels: 0.0420 (0.0494)  loss: 0.0397 (0.0494)  time: 0.1648\n",
      "Epoch: [35]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0437 (0.0493)  loss: 0.0412 (0.0493)  time: 0.1657\n",
      "Epoch: [35]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0464 (0.0493)  loss: 0.0464 (0.0493)  time: 0.1660\n",
      "Epoch: [35]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0438 (0.0493)  loss: 0.0384 (0.0493)  time: 0.1665\n",
      "Epoch: [35] Total time: 0:14:15 (0.1708 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0438 (0.0493)  loss: 0.0384 (0.0493)\n",
      "Test:  [  0/565]  eta: 2:33:52  loss_labels: 0.9781 (0.9781)  loss: 0.9781 (0.9781)  time: 16.3408\n",
      "Test:  [100/565]  eta: 0:02:10  loss_labels: 1.4789 (1.6328)  loss: 1.6907 (1.6328)  time: 0.1139\n",
      "Test:  [200/565]  eta: 0:01:13  loss_labels: 1.2990 (1.5375)  loss: 1.5302 (1.5375)  time: 0.1140\n",
      "Test:  [300/565]  eta: 0:00:46  loss_labels: 1.2502 (1.4662)  loss: 1.0556 (1.4662)  time: 0.1310\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.5124 (1.4854)  loss: 1.5162 (1.4854)  time: 0.1116\n",
      "Test:  [500/565]  eta: 0:00:09  loss_labels: 1.1560 (1.4457)  loss: 1.3878 (1.4457)  time: 0.1198\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.1797 (1.4244)  loss: 1.1507 (1.4244)  time: 0.1226\n",
      "Test: Total time: 0:01:25 (0.1514 s / it)\n",
      "Averaged stats: loss_labels: 1.1797 (1.4244)  loss: 1.1507 (1.4244)\n",
      "acc: 0.7999778389930725\n",
      "top 1 and top 5 accuracies {'top1': 0.799977843017781, 'top5': 0.9019415055669417, 'loss': tensor(0.0111, device='cuda:0')}\n",
      "Epoch: [36]  [   0/5008]  eta: 1 day, 5:20:52  lr: 0.000500  loss_labels: 0.0816 (0.0816)  loss: 0.0816 (0.0816)  time: 21.0967\n",
      "Epoch: [36]  [ 100/5008]  eta: 0:30:26  lr: 0.000500  loss_labels: 0.0446 (0.0504)  loss: 0.0423 (0.0504)  time: 0.1650\n",
      "Epoch: [36]  [ 200/5008]  eta: 0:21:35  lr: 0.000500  loss_labels: 0.0416 (0.0485)  loss: 0.0318 (0.0485)  time: 0.1668\n",
      "Epoch: [36]  [ 300/5008]  eta: 0:18:27  lr: 0.000500  loss_labels: 0.0457 (0.0490)  loss: 0.0456 (0.0490)  time: 0.1649\n",
      "Epoch: [36]  [ 400/5008]  eta: 0:16:42  lr: 0.000500  loss_labels: 0.0432 (0.0482)  loss: 0.0433 (0.0482)  time: 0.1648\n",
      "Epoch: [36]  [ 500/5008]  eta: 0:15:32  lr: 0.000500  loss_labels: 0.0450 (0.0481)  loss: 0.0399 (0.0481)  time: 0.1638\n",
      "Epoch: [36]  [ 600/5008]  eta: 0:14:41  lr: 0.000500  loss_labels: 0.0454 (0.0478)  loss: 0.0407 (0.0478)  time: 0.1649\n",
      "Epoch: [36]  [ 700/5008]  eta: 0:13:59  lr: 0.000500  loss_labels: 0.0422 (0.0476)  loss: 0.0378 (0.0476)  time: 0.1650\n",
      "Epoch: [36]  [ 800/5008]  eta: 0:13:24  lr: 0.000500  loss_labels: 0.0472 (0.0481)  loss: 0.0546 (0.0481)  time: 0.1649\n",
      "Epoch: [36]  [ 900/5008]  eta: 0:12:53  lr: 0.000500  loss_labels: 0.0403 (0.0475)  loss: 0.0318 (0.0475)  time: 0.1649\n",
      "Epoch: [36]  [1000/5008]  eta: 0:12:25  lr: 0.000500  loss_labels: 0.0390 (0.0470)  loss: 0.0343 (0.0470)  time: 0.1685\n",
      "Epoch: [36]  [1100/5008]  eta: 0:12:00  lr: 0.000500  loss_labels: 0.0466 (0.0473)  loss: 0.0482 (0.0473)  time: 0.1672\n",
      "Epoch: [36]  [1200/5008]  eta: 0:11:36  lr: 0.000500  loss_labels: 0.0392 (0.0471)  loss: 0.0424 (0.0471)  time: 0.1670\n",
      "Epoch: [36]  [1300/5008]  eta: 0:11:13  lr: 0.000500  loss_labels: 0.0438 (0.0471)  loss: 0.0432 (0.0471)  time: 0.1674\n",
      "Epoch: [36]  [1400/5008]  eta: 0:10:51  lr: 0.000500  loss_labels: 0.0466 (0.0472)  loss: 0.0441 (0.0472)  time: 0.1672\n",
      "Epoch: [36]  [1500/5008]  eta: 0:10:30  lr: 0.000500  loss_labels: 0.0434 (0.0473)  loss: 0.0419 (0.0473)  time: 0.1675\n",
      "Epoch: [36]  [1600/5008]  eta: 0:10:10  lr: 0.000500  loss_labels: 0.0423 (0.0474)  loss: 0.0432 (0.0474)  time: 0.1694\n",
      "Epoch: [36]  [1700/5008]  eta: 0:09:50  lr: 0.000500  loss_labels: 0.0431 (0.0474)  loss: 0.0461 (0.0474)  time: 0.1673\n",
      "Epoch: [36]  [1800/5008]  eta: 0:09:30  lr: 0.000500  loss_labels: 0.0423 (0.0473)  loss: 0.0379 (0.0473)  time: 0.1669\n",
      "Epoch: [36]  [1900/5008]  eta: 0:09:11  lr: 0.000500  loss_labels: 0.0437 (0.0474)  loss: 0.0404 (0.0474)  time: 0.1671\n",
      "Epoch: [36]  [2000/5008]  eta: 0:08:51  lr: 0.000500  loss_labels: 0.0421 (0.0474)  loss: 0.0282 (0.0474)  time: 0.1673\n",
      "Epoch: [36]  [2100/5008]  eta: 0:08:32  lr: 0.000500  loss_labels: 0.0434 (0.0475)  loss: 0.0482 (0.0475)  time: 0.1664\n",
      "Epoch: [36]  [2200/5008]  eta: 0:08:13  lr: 0.000500  loss_labels: 0.0443 (0.0475)  loss: 0.0372 (0.0475)  time: 0.1651\n",
      "Epoch: [36]  [2300/5008]  eta: 0:07:55  lr: 0.000500  loss_labels: 0.0410 (0.0474)  loss: 0.0344 (0.0474)  time: 0.1650\n",
      "Epoch: [36]  [2400/5008]  eta: 0:07:36  lr: 0.000500  loss_labels: 0.0441 (0.0475)  loss: 0.0441 (0.0475)  time: 0.1668\n",
      "Epoch: [36]  [2500/5008]  eta: 0:07:18  lr: 0.000500  loss_labels: 0.0406 (0.0473)  loss: 0.0398 (0.0473)  time: 0.1659\n",
      "Epoch: [36]  [2600/5008]  eta: 0:06:59  lr: 0.000500  loss_labels: 0.0429 (0.0473)  loss: 0.0323 (0.0473)  time: 0.1656\n",
      "Epoch: [36]  [2700/5008]  eta: 0:06:41  lr: 0.000500  loss_labels: 0.0411 (0.0472)  loss: 0.0422 (0.0472)  time: 0.1660\n",
      "Epoch: [36]  [2800/5008]  eta: 0:06:23  lr: 0.000500  loss_labels: 0.0440 (0.0472)  loss: 0.0506 (0.0472)  time: 0.1664\n",
      "Epoch: [36]  [2900/5008]  eta: 0:06:05  lr: 0.000500  loss_labels: 0.0433 (0.0472)  loss: 0.0361 (0.0472)  time: 0.1665\n",
      "Epoch: [36]  [3000/5008]  eta: 0:05:47  lr: 0.000500  loss_labels: 0.0428 (0.0472)  loss: 0.0465 (0.0472)  time: 0.1664\n",
      "Epoch: [36]  [3100/5008]  eta: 0:05:30  lr: 0.000500  loss_labels: 0.0446 (0.0472)  loss: 0.0317 (0.0472)  time: 0.1671\n",
      "Epoch: [36]  [3200/5008]  eta: 0:05:12  lr: 0.000500  loss_labels: 0.0410 (0.0472)  loss: 0.0415 (0.0472)  time: 0.1658\n",
      "Epoch: [36]  [3300/5008]  eta: 0:04:54  lr: 0.000500  loss_labels: 0.0378 (0.0471)  loss: 0.0380 (0.0471)  time: 0.1663\n",
      "Epoch: [36]  [3400/5008]  eta: 0:04:37  lr: 0.000500  loss_labels: 0.0425 (0.0471)  loss: 0.0508 (0.0471)  time: 0.1659\n",
      "Epoch: [36]  [3500/5008]  eta: 0:04:19  lr: 0.000500  loss_labels: 0.0428 (0.0471)  loss: 0.0408 (0.0471)  time: 0.1664\n",
      "Epoch: [36]  [3600/5008]  eta: 0:04:02  lr: 0.000500  loss_labels: 0.0410 (0.0470)  loss: 0.0385 (0.0470)  time: 0.1661\n",
      "Epoch: [36]  [3700/5008]  eta: 0:03:44  lr: 0.000500  loss_labels: 0.0457 (0.0471)  loss: 0.0494 (0.0471)  time: 0.1673\n",
      "Epoch: [36]  [3800/5008]  eta: 0:03:27  lr: 0.000500  loss_labels: 0.0480 (0.0471)  loss: 0.0419 (0.0471)  time: 0.1667\n",
      "Epoch: [36]  [3900/5008]  eta: 0:03:10  lr: 0.000500  loss_labels: 0.0420 (0.0471)  loss: 0.0393 (0.0471)  time: 0.1681\n",
      "Epoch: [36]  [4000/5008]  eta: 0:02:53  lr: 0.000500  loss_labels: 0.0447 (0.0471)  loss: 0.0446 (0.0471)  time: 0.1673\n",
      "Epoch: [36]  [4100/5008]  eta: 0:02:35  lr: 0.000500  loss_labels: 0.0438 (0.0471)  loss: 0.0424 (0.0471)  time: 0.1674\n",
      "Epoch: [36]  [4200/5008]  eta: 0:02:18  lr: 0.000500  loss_labels: 0.0478 (0.0472)  loss: 0.0396 (0.0472)  time: 0.1668\n",
      "Epoch: [36]  [4300/5008]  eta: 0:02:01  lr: 0.000500  loss_labels: 0.0426 (0.0472)  loss: 0.0477 (0.0472)  time: 0.1654\n",
      "Epoch: [36]  [4400/5008]  eta: 0:01:44  lr: 0.000500  loss_labels: 0.0447 (0.0471)  loss: 0.0435 (0.0471)  time: 0.1657\n",
      "Epoch: [36]  [4500/5008]  eta: 0:01:26  lr: 0.000500  loss_labels: 0.0395 (0.0471)  loss: 0.0476 (0.0471)  time: 0.1654\n",
      "Epoch: [36]  [4600/5008]  eta: 0:01:09  lr: 0.000500  loss_labels: 0.0420 (0.0471)  loss: 0.0383 (0.0471)  time: 0.1674\n",
      "Epoch: [36]  [4700/5008]  eta: 0:00:52  lr: 0.000500  loss_labels: 0.0445 (0.0471)  loss: 0.0593 (0.0471)  time: 0.1670\n",
      "Epoch: [36]  [4800/5008]  eta: 0:00:35  lr: 0.000500  loss_labels: 0.0467 (0.0471)  loss: 0.0425 (0.0471)  time: 0.1658\n",
      "Epoch: [36]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0456 (0.0471)  loss: 0.0458 (0.0471)  time: 0.1666\n",
      "Epoch: [36]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0413 (0.0471)  loss: 0.0375 (0.0471)  time: 0.1678\n",
      "Epoch: [36]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0410 (0.0471)  loss: 0.0382 (0.0471)  time: 0.1671\n",
      "Epoch: [36] Total time: 0:14:15 (0.1708 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0410 (0.0471)  loss: 0.0382 (0.0471)\n",
      "Test:  [  0/565]  eta: 2:22:16  loss_labels: 1.4247 (1.4247)  loss: 1.4247 (1.4247)  time: 15.1089\n",
      "Test:  [100/565]  eta: 0:02:07  loss_labels: 1.4990 (1.6216)  loss: 1.4901 (1.6216)  time: 0.1174\n",
      "Test:  [200/565]  eta: 0:01:12  loss_labels: 1.2417 (1.5254)  loss: 1.6037 (1.5254)  time: 0.1181\n",
      "Test:  [300/565]  eta: 0:00:45  loss_labels: 1.1528 (1.4448)  loss: 1.0061 (1.4448)  time: 0.1131\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.4776 (1.4710)  loss: 1.5069 (1.4710)  time: 0.1130\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.1789 (1.4319)  loss: 1.3087 (1.4319)  time: 0.1222\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.2103 (1.4084)  loss: 1.1807 (1.4084)  time: 0.1222\n",
      "Test: Total time: 0:01:25 (0.1522 s / it)\n",
      "Averaged stats: loss_labels: 1.2103 (1.4084)  loss: 1.1807 (1.4084)\n",
      "acc: 0.8037444949150085\n",
      "top 1 and top 5 accuracies {'top1': 0.8037445299950147, 'top5': 0.9032709245000831, 'loss': tensor(0.0110, device='cuda:0')}\n",
      "Epoch: [37]  [   0/5008]  eta: 1 day, 3:20:42  lr: 0.000500  loss_labels: 0.1065 (0.1065)  loss: 0.1065 (0.1065)  time: 19.6570\n",
      "Epoch: [37]  [ 100/5008]  eta: 0:29:17  lr: 0.000500  loss_labels: 0.0404 (0.0442)  loss: 0.0404 (0.0442)  time: 0.1640\n",
      "Epoch: [37]  [ 200/5008]  eta: 0:20:57  lr: 0.000500  loss_labels: 0.0444 (0.0462)  loss: 0.0445 (0.0462)  time: 0.1651\n",
      "Epoch: [37]  [ 300/5008]  eta: 0:18:00  lr: 0.000500  loss_labels: 0.0403 (0.0456)  loss: 0.0436 (0.0456)  time: 0.1639\n",
      "Epoch: [37]  [ 400/5008]  eta: 0:16:24  lr: 0.000500  loss_labels: 0.0409 (0.0452)  loss: 0.0361 (0.0452)  time: 0.1685\n",
      "Epoch: [37]  [ 500/5008]  eta: 0:15:20  lr: 0.000500  loss_labels: 0.0435 (0.0455)  loss: 0.0461 (0.0455)  time: 0.1659\n",
      "Epoch: [37]  [ 600/5008]  eta: 0:14:32  lr: 0.000500  loss_labels: 0.0397 (0.0448)  loss: 0.0382 (0.0448)  time: 0.1669\n",
      "Epoch: [37]  [ 700/5008]  eta: 0:13:53  lr: 0.000500  loss_labels: 0.0367 (0.0445)  loss: 0.0456 (0.0445)  time: 0.1663\n",
      "Epoch: [37]  [ 800/5008]  eta: 0:13:19  lr: 0.000500  loss_labels: 0.0431 (0.0448)  loss: 0.0497 (0.0448)  time: 0.1653\n",
      "Epoch: [37]  [ 900/5008]  eta: 0:12:49  lr: 0.000500  loss_labels: 0.0478 (0.0453)  loss: 0.0484 (0.0453)  time: 0.1661\n",
      "Epoch: [37]  [1000/5008]  eta: 0:12:21  lr: 0.000500  loss_labels: 0.0449 (0.0455)  loss: 0.0492 (0.0455)  time: 0.1639\n",
      "Epoch: [37]  [1100/5008]  eta: 0:11:55  lr: 0.000500  loss_labels: 0.0409 (0.0454)  loss: 0.0448 (0.0454)  time: 0.1633\n",
      "Epoch: [37]  [1200/5008]  eta: 0:11:31  lr: 0.000500  loss_labels: 0.0430 (0.0456)  loss: 0.0543 (0.0456)  time: 0.1642\n",
      "Epoch: [37]  [1300/5008]  eta: 0:11:08  lr: 0.000500  loss_labels: 0.0412 (0.0456)  loss: 0.0421 (0.0456)  time: 0.1672\n",
      "Epoch: [37]  [1400/5008]  eta: 0:10:47  lr: 0.000500  loss_labels: 0.0428 (0.0456)  loss: 0.0545 (0.0456)  time: 0.1663\n",
      "Epoch: [37]  [1500/5008]  eta: 0:10:26  lr: 0.000500  loss_labels: 0.0419 (0.0456)  loss: 0.0358 (0.0456)  time: 0.1668\n",
      "Epoch: [37]  [1600/5008]  eta: 0:10:06  lr: 0.000500  loss_labels: 0.0423 (0.0457)  loss: 0.0366 (0.0457)  time: 0.1662\n",
      "Epoch: [37]  [1700/5008]  eta: 0:09:46  lr: 0.000500  loss_labels: 0.0420 (0.0458)  loss: 0.0428 (0.0458)  time: 0.1669\n",
      "Epoch: [37]  [1800/5008]  eta: 0:09:26  lr: 0.000500  loss_labels: 0.0394 (0.0457)  loss: 0.0323 (0.0457)  time: 0.1672\n",
      "Epoch: [37]  [1900/5008]  eta: 0:09:07  lr: 0.000500  loss_labels: 0.0380 (0.0455)  loss: 0.0386 (0.0455)  time: 0.1656\n",
      "Epoch: [37]  [2000/5008]  eta: 0:08:48  lr: 0.000500  loss_labels: 0.0405 (0.0454)  loss: 0.0280 (0.0454)  time: 0.1657\n",
      "Epoch: [37]  [2100/5008]  eta: 0:08:29  lr: 0.000500  loss_labels: 0.0452 (0.0455)  loss: 0.0420 (0.0455)  time: 0.1671\n",
      "Epoch: [37]  [2200/5008]  eta: 0:08:10  lr: 0.000500  loss_labels: 0.0423 (0.0455)  loss: 0.0498 (0.0455)  time: 0.1667\n",
      "Epoch: [37]  [2300/5008]  eta: 0:07:52  lr: 0.000500  loss_labels: 0.0411 (0.0454)  loss: 0.0419 (0.0454)  time: 0.1661\n",
      "Epoch: [37]  [2400/5008]  eta: 0:07:33  lr: 0.000500  loss_labels: 0.0378 (0.0453)  loss: 0.0353 (0.0453)  time: 0.1668\n",
      "Epoch: [37]  [2500/5008]  eta: 0:07:15  lr: 0.000500  loss_labels: 0.0423 (0.0453)  loss: 0.0365 (0.0453)  time: 0.1660\n",
      "Epoch: [37]  [2600/5008]  eta: 0:06:57  lr: 0.000500  loss_labels: 0.0388 (0.0452)  loss: 0.0314 (0.0452)  time: 0.1665\n",
      "Epoch: [37]  [2700/5008]  eta: 0:06:39  lr: 0.000500  loss_labels: 0.0483 (0.0453)  loss: 0.0514 (0.0453)  time: 0.1663\n",
      "Epoch: [37]  [2800/5008]  eta: 0:06:21  lr: 0.000500  loss_labels: 0.0420 (0.0454)  loss: 0.0441 (0.0454)  time: 0.1679\n",
      "Epoch: [37]  [2900/5008]  eta: 0:06:04  lr: 0.000500  loss_labels: 0.0440 (0.0455)  loss: 0.0518 (0.0455)  time: 0.1665\n",
      "Epoch: [37]  [3000/5008]  eta: 0:05:46  lr: 0.000500  loss_labels: 0.0451 (0.0455)  loss: 0.0361 (0.0455)  time: 0.1663\n",
      "Epoch: [37]  [3100/5008]  eta: 0:05:28  lr: 0.000500  loss_labels: 0.0396 (0.0454)  loss: 0.0352 (0.0454)  time: 0.1654\n",
      "Epoch: [37]  [3200/5008]  eta: 0:05:11  lr: 0.000500  loss_labels: 0.0431 (0.0455)  loss: 0.0427 (0.0455)  time: 0.1663\n",
      "Epoch: [37]  [3300/5008]  eta: 0:04:53  lr: 0.000500  loss_labels: 0.0409 (0.0455)  loss: 0.0417 (0.0455)  time: 0.1662\n",
      "Epoch: [37]  [3400/5008]  eta: 0:04:36  lr: 0.000500  loss_labels: 0.0396 (0.0455)  loss: 0.0431 (0.0455)  time: 0.1667\n",
      "Epoch: [37]  [3500/5008]  eta: 0:04:18  lr: 0.000500  loss_labels: 0.0415 (0.0455)  loss: 0.0421 (0.0455)  time: 0.1665\n",
      "Epoch: [37]  [3600/5008]  eta: 0:04:01  lr: 0.000500  loss_labels: 0.0432 (0.0454)  loss: 0.0443 (0.0454)  time: 0.1661\n",
      "Epoch: [37]  [3700/5008]  eta: 0:03:44  lr: 0.000500  loss_labels: 0.0414 (0.0454)  loss: 0.0411 (0.0454)  time: 0.1682\n",
      "Epoch: [37]  [3800/5008]  eta: 0:03:26  lr: 0.000500  loss_labels: 0.0419 (0.0454)  loss: 0.0387 (0.0454)  time: 0.1677\n",
      "Epoch: [37]  [3900/5008]  eta: 0:03:09  lr: 0.000500  loss_labels: 0.0452 (0.0455)  loss: 0.0427 (0.0455)  time: 0.1673\n",
      "Epoch: [37]  [4000/5008]  eta: 0:02:52  lr: 0.000500  loss_labels: 0.0387 (0.0454)  loss: 0.0368 (0.0454)  time: 0.1683\n",
      "Epoch: [37]  [4100/5008]  eta: 0:02:35  lr: 0.000500  loss_labels: 0.0436 (0.0455)  loss: 0.0421 (0.0455)  time: 0.1685\n",
      "Epoch: [37]  [4200/5008]  eta: 0:02:18  lr: 0.000500  loss_labels: 0.0395 (0.0454)  loss: 0.0356 (0.0454)  time: 0.1676\n",
      "Epoch: [37]  [4300/5008]  eta: 0:02:00  lr: 0.000500  loss_labels: 0.0418 (0.0455)  loss: 0.0431 (0.0455)  time: 0.1662\n",
      "Epoch: [37]  [4400/5008]  eta: 0:01:43  lr: 0.000500  loss_labels: 0.0379 (0.0455)  loss: 0.0344 (0.0455)  time: 0.1661\n",
      "Epoch: [37]  [4500/5008]  eta: 0:01:26  lr: 0.000500  loss_labels: 0.0417 (0.0455)  loss: 0.0443 (0.0455)  time: 0.1656\n",
      "Epoch: [37]  [4600/5008]  eta: 0:01:09  lr: 0.000500  loss_labels: 0.0399 (0.0454)  loss: 0.0331 (0.0454)  time: 0.1655\n",
      "Epoch: [37]  [4700/5008]  eta: 0:00:52  lr: 0.000500  loss_labels: 0.0407 (0.0454)  loss: 0.0367 (0.0454)  time: 0.1658\n",
      "Epoch: [37]  [4800/5008]  eta: 0:00:35  lr: 0.000500  loss_labels: 0.0405 (0.0453)  loss: 0.0389 (0.0453)  time: 0.1659\n",
      "Epoch: [37]  [4900/5008]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.0357 (0.0453)  loss: 0.0295 (0.0453)  time: 0.1666\n",
      "Epoch: [37]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0418 (0.0452)  loss: 0.0418 (0.0452)  time: 0.1668\n",
      "Epoch: [37]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0412 (0.0452)  loss: 0.0378 (0.0452)  time: 0.1668\n",
      "Epoch: [37] Total time: 0:14:13 (0.1704 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0412 (0.0452)  loss: 0.0378 (0.0452)\n",
      "Test:  [  0/565]  eta: 2:36:20  loss_labels: 1.1438 (1.1438)  loss: 1.1438 (1.1438)  time: 16.6032\n",
      "Test:  [100/565]  eta: 0:02:13  loss_labels: 1.5755 (1.6417)  loss: 1.7185 (1.6417)  time: 0.1159\n",
      "Test:  [200/565]  eta: 0:01:14  loss_labels: 1.3550 (1.5473)  loss: 1.5829 (1.5473)  time: 0.1144\n",
      "Test:  [300/565]  eta: 0:00:47  loss_labels: 1.1618 (1.4651)  loss: 1.0878 (1.4651)  time: 0.1282\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 1.5366 (1.4864)  loss: 1.5395 (1.4864)  time: 0.1119\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.1938 (1.4468)  loss: 1.3517 (1.4468)  time: 0.1085\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.2206 (1.4256)  loss: 1.2613 (1.4256)  time: 0.1565\n",
      "Test: Total time: 0:01:26 (0.1530 s / it)\n",
      "Averaged stats: loss_labels: 1.2206 (1.4256)  loss: 1.2613 (1.4256)\n",
      "acc: 0.8029966950416565\n",
      "top 1 and top 5 accuracies {'top1': 0.8029967318451227, 'top5': 0.9038940896249931, 'loss': tensor(0.0112, device='cuda:0')}\n",
      "Epoch: [38]  [   0/5008]  eta: 1 day, 5:30:51  lr: 0.000500  loss_labels: 0.0525 (0.0525)  loss: 0.0525 (0.0525)  time: 21.2163\n",
      "Epoch: [38]  [ 100/5008]  eta: 0:30:38  lr: 0.000500  loss_labels: 0.0429 (0.0429)  loss: 0.0356 (0.0429)  time: 0.1656\n",
      "Epoch: [38]  [ 200/5008]  eta: 0:21:42  lr: 0.000500  loss_labels: 0.0428 (0.0441)  loss: 0.0361 (0.0441)  time: 0.1663\n",
      "Epoch: [38]  [ 300/5008]  eta: 0:18:31  lr: 0.000500  loss_labels: 0.0404 (0.0445)  loss: 0.0397 (0.0445)  time: 0.1659\n",
      "Epoch: [38]  [ 400/5008]  eta: 0:16:47  lr: 0.000500  loss_labels: 0.0389 (0.0439)  loss: 0.0389 (0.0439)  time: 0.1665\n",
      "Epoch: [38]  [ 500/5008]  eta: 0:15:39  lr: 0.000500  loss_labels: 0.0412 (0.0438)  loss: 0.0364 (0.0438)  time: 0.1675\n",
      "Epoch: [38]  [ 600/5008]  eta: 0:14:47  lr: 0.000500  loss_labels: 0.0425 (0.0441)  loss: 0.0584 (0.0441)  time: 0.1669\n",
      "Epoch: [38]  [ 700/5008]  eta: 0:14:07  lr: 0.000500  loss_labels: 0.0384 (0.0441)  loss: 0.0432 (0.0441)  time: 0.1680\n",
      "Epoch: [38]  [ 800/5008]  eta: 0:13:31  lr: 0.000500  loss_labels: 0.0415 (0.0442)  loss: 0.0380 (0.0442)  time: 0.1666\n",
      "Epoch: [38]  [ 900/5008]  eta: 0:13:00  lr: 0.000500  loss_labels: 0.0386 (0.0440)  loss: 0.0360 (0.0440)  time: 0.1663\n",
      "Epoch: [38]  [1000/5008]  eta: 0:12:32  lr: 0.000500  loss_labels: 0.0364 (0.0441)  loss: 0.0364 (0.0441)  time: 0.1664\n",
      "Epoch: [38]  [1100/5008]  eta: 0:12:06  lr: 0.000500  loss_labels: 0.0413 (0.0443)  loss: 0.0408 (0.0443)  time: 0.1675\n",
      "Epoch: [38]  [1200/5008]  eta: 0:11:41  lr: 0.000500  loss_labels: 0.0410 (0.0443)  loss: 0.0386 (0.0443)  time: 0.1669\n",
      "Epoch: [38]  [1300/5008]  eta: 0:11:18  lr: 0.000500  loss_labels: 0.0393 (0.0444)  loss: 0.0339 (0.0444)  time: 0.1672\n",
      "Epoch: [38]  [1400/5008]  eta: 0:10:55  lr: 0.000500  loss_labels: 0.0409 (0.0444)  loss: 0.0437 (0.0444)  time: 0.1669\n",
      "Epoch: [38]  [1500/5008]  eta: 0:10:34  lr: 0.000500  loss_labels: 0.0405 (0.0444)  loss: 0.0366 (0.0444)  time: 0.1666\n",
      "Epoch: [38]  [1600/5008]  eta: 0:10:13  lr: 0.000500  loss_labels: 0.0381 (0.0443)  loss: 0.0440 (0.0443)  time: 0.1671\n",
      "Epoch: [38]  [1700/5008]  eta: 0:09:52  lr: 0.000500  loss_labels: 0.0429 (0.0443)  loss: 0.0260 (0.0443)  time: 0.1678\n",
      "Epoch: [38]  [1800/5008]  eta: 0:09:32  lr: 0.000500  loss_labels: 0.0393 (0.0442)  loss: 0.0331 (0.0442)  time: 0.1678\n",
      "Epoch: [38]  [1900/5008]  eta: 0:09:12  lr: 0.000500  loss_labels: 0.0397 (0.0442)  loss: 0.0390 (0.0442)  time: 0.1656\n",
      "Epoch: [38]  [2000/5008]  eta: 0:08:53  lr: 0.000500  loss_labels: 0.0380 (0.0440)  loss: 0.0320 (0.0440)  time: 0.1648\n",
      "Epoch: [38]  [2100/5008]  eta: 0:08:48  lr: 0.000500  loss_labels: 0.0428 (0.0441)  loss: 0.0443 (0.0441)  time: 0.6824\n",
      "Epoch: [38]  [2200/5008]  eta: 0:09:45  lr: 0.000500  loss_labels: 0.0369 (0.0440)  loss: 0.0321 (0.0440)  time: 0.1644\n",
      "Epoch: [38]  [2300/5008]  eta: 0:09:19  lr: 0.000500  loss_labels: 0.0397 (0.0440)  loss: 0.0385 (0.0440)  time: 0.1639\n",
      "Epoch: [38]  [2400/5008]  eta: 0:08:54  lr: 0.000500  loss_labels: 0.0371 (0.0439)  loss: 0.0299 (0.0439)  time: 0.1641\n",
      "Epoch: [38]  [2500/5008]  eta: 0:08:29  lr: 0.000500  loss_labels: 0.0395 (0.0439)  loss: 0.0359 (0.0439)  time: 0.1635\n",
      "Epoch: [38]  [2600/5008]  eta: 0:08:05  lr: 0.000500  loss_labels: 0.0433 (0.0440)  loss: 0.0360 (0.0440)  time: 0.1653\n",
      "Epoch: [38]  [2700/5008]  eta: 0:07:42  lr: 0.000500  loss_labels: 0.0392 (0.0441)  loss: 0.0465 (0.0441)  time: 0.1642\n",
      "Epoch: [38]  [2800/5008]  eta: 0:07:19  lr: 0.000500  loss_labels: 0.0411 (0.0441)  loss: 0.0411 (0.0441)  time: 0.1640\n",
      "Epoch: [38]  [2900/5008]  eta: 0:06:57  lr: 0.000500  loss_labels: 0.0402 (0.0441)  loss: 0.0300 (0.0441)  time: 0.1647\n",
      "Epoch: [38]  [3000/5008]  eta: 0:06:35  lr: 0.000500  loss_labels: 0.0386 (0.0439)  loss: 0.0386 (0.0439)  time: 0.1646\n",
      "Epoch: [38]  [3100/5008]  eta: 0:06:13  lr: 0.000500  loss_labels: 0.0408 (0.0439)  loss: 0.0408 (0.0439)  time: 0.1642\n",
      "Epoch: [38]  [3200/5008]  eta: 0:05:52  lr: 0.000500  loss_labels: 0.0409 (0.0438)  loss: 0.0419 (0.0438)  time: 0.1645\n",
      "Epoch: [38]  [3300/5008]  eta: 0:05:31  lr: 0.000500  loss_labels: 0.0418 (0.0438)  loss: 0.0417 (0.0438)  time: 0.1648\n",
      "Epoch: [38]  [3400/5008]  eta: 0:05:10  lr: 0.000500  loss_labels: 0.0412 (0.0439)  loss: 0.0448 (0.0439)  time: 0.1638\n",
      "Epoch: [38]  [3500/5008]  eta: 0:04:49  lr: 0.000500  loss_labels: 0.0443 (0.0439)  loss: 0.0505 (0.0439)  time: 0.1642\n",
      "Epoch: [38]  [3600/5008]  eta: 0:04:29  lr: 0.000500  loss_labels: 0.0390 (0.0438)  loss: 0.0390 (0.0438)  time: 0.1644\n",
      "Epoch: [38]  [3700/5008]  eta: 0:04:09  lr: 0.000500  loss_labels: 0.0375 (0.0438)  loss: 0.0345 (0.0438)  time: 0.1650\n",
      "Epoch: [38]  [3800/5008]  eta: 0:03:49  lr: 0.000500  loss_labels: 0.0398 (0.0437)  loss: 0.0301 (0.0437)  time: 0.1640\n",
      "Epoch: [38]  [3900/5008]  eta: 0:03:29  lr: 0.000500  loss_labels: 0.0401 (0.0437)  loss: 0.0308 (0.0437)  time: 0.1644\n",
      "Epoch: [38]  [4000/5008]  eta: 0:03:10  lr: 0.000500  loss_labels: 0.0366 (0.0436)  loss: 0.0310 (0.0436)  time: 0.1641\n",
      "Epoch: [38]  [4100/5008]  eta: 0:02:50  lr: 0.000500  loss_labels: 0.0404 (0.0436)  loss: 0.0407 (0.0436)  time: 0.1651\n",
      "Epoch: [38]  [4200/5008]  eta: 0:02:31  lr: 0.000500  loss_labels: 0.0382 (0.0436)  loss: 0.0403 (0.0436)  time: 0.1651\n",
      "Epoch: [38]  [4300/5008]  eta: 0:02:12  lr: 0.000500  loss_labels: 0.0402 (0.0435)  loss: 0.0341 (0.0435)  time: 0.1648\n",
      "Epoch: [38]  [4400/5008]  eta: 0:01:53  lr: 0.000500  loss_labels: 0.0407 (0.0436)  loss: 0.0509 (0.0436)  time: 0.1656\n",
      "Epoch: [38]  [4500/5008]  eta: 0:01:34  lr: 0.000500  loss_labels: 0.0388 (0.0436)  loss: 0.0382 (0.0436)  time: 0.1658\n",
      "Epoch: [38]  [4600/5008]  eta: 0:01:15  lr: 0.000500  loss_labels: 0.0408 (0.0436)  loss: 0.0388 (0.0436)  time: 0.1660\n",
      "Epoch: [38]  [4700/5008]  eta: 0:00:57  lr: 0.000500  loss_labels: 0.0396 (0.0436)  loss: 0.0396 (0.0436)  time: 0.1656\n",
      "Epoch: [38]  [4800/5008]  eta: 0:00:38  lr: 0.000500  loss_labels: 0.0366 (0.0436)  loss: 0.0326 (0.0436)  time: 0.1665\n",
      "Epoch: [38]  [4900/5008]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.0413 (0.0436)  loss: 0.0350 (0.0436)  time: 0.1674\n",
      "Epoch: [38]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.0427 (0.0436)  loss: 0.0321 (0.0436)  time: 0.1688\n",
      "Epoch: [38]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.0429 (0.0436)  loss: 0.0339 (0.0436)  time: 0.1677\n",
      "Epoch: [38] Total time: 0:15:22 (0.1843 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.0429 (0.0436)  loss: 0.0339 (0.0436)\n",
      "Test:  [  0/565]  eta: 2:33:49  loss_labels: 1.4679 (1.4679)  loss: 1.4679 (1.4679)  time: 16.3361\n",
      "Test:  [100/565]  eta: 0:02:11  loss_labels: 1.6381 (1.7054)  loss: 1.6906 (1.7054)  time: 0.1159\n",
      "Test:  [200/565]  eta: 0:01:14  loss_labels: 1.3330 (1.5973)  loss: 1.6738 (1.5973)  time: 0.1143\n",
      "Test:  [300/565]  eta: 0:00:46  loss_labels: 1.2051 (1.5120)  loss: 1.1920 (1.5120)  time: 0.1321\n",
      "Test:  [400/565]  eta: 0:00:27  loss_labels: 1.4294 (1.5363)  loss: 1.4294 (1.5363)  time: 0.1152\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.2742 (1.4995)  loss: 1.4740 (1.4995)  time: 0.1132\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.2952 (1.4774)  loss: 1.4720 (1.4774)  time: 0.1329\n",
      "Test: Total time: 0:01:27 (0.1540 s / it)\n",
      "Averaged stats: loss_labels: 1.2952 (1.4774)  loss: 1.4720 (1.4774)\n",
      "acc: 0.7998809218406677\n",
      "top 1 and top 5 accuracies {'top1': 0.7998809062205727, 'top5': 0.901789176314186, 'loss': tensor(0.0116, device='cuda:0')}\n",
      "Epoch: [39]  [   0/5008]  eta: 1 day, 5:53:01  lr: 0.000500  loss_labels: 0.0507 (0.0507)  loss: 0.0507 (0.0507)  time: 21.4820\n",
      "Epoch: [39]  [ 100/5008]  eta: 0:30:54  lr: 0.000500  loss_labels: 0.0382 (0.0422)  loss: 0.0382 (0.0422)  time: 0.1670\n",
      "Epoch: [39]  [ 200/5008]  eta: 0:21:51  lr: 0.000500  loss_labels: 0.0407 (0.0434)  loss: 0.0555 (0.0434)  time: 0.1685\n",
      "Epoch: [39]  [ 300/5008]  eta: 0:18:41  lr: 0.000500  loss_labels: 0.0400 (0.0433)  loss: 0.0471 (0.0433)  time: 0.1690\n",
      "Epoch: [39]  [ 400/5008]  eta: 0:16:56  lr: 0.000500  loss_labels: 0.0399 (0.0432)  loss: 0.0427 (0.0432)  time: 0.1659\n",
      "Epoch: [39]  [ 500/5008]  eta: 0:15:45  lr: 0.000500  loss_labels: 0.0408 (0.0432)  loss: 0.0360 (0.0432)  time: 0.1666\n",
      "Epoch: [39]  [ 600/5008]  eta: 0:14:52  lr: 0.000500  loss_labels: 0.0404 (0.0428)  loss: 0.0355 (0.0428)  time: 0.1667\n",
      "Epoch: [39]  [ 700/5008]  eta: 0:14:10  lr: 0.000500  loss_labels: 0.0391 (0.0427)  loss: 0.0334 (0.0427)  time: 0.1656\n",
      "Epoch: [39]  [ 800/5008]  eta: 0:13:34  lr: 0.000500  loss_labels: 0.0409 (0.0428)  loss: 0.0466 (0.0428)  time: 0.1672\n",
      "Epoch: [39]  [ 900/5008]  eta: 0:13:03  lr: 0.000500  loss_labels: 0.0378 (0.0427)  loss: 0.0384 (0.0427)  time: 0.1675\n",
      "Epoch: [39]  [1000/5008]  eta: 0:12:34  lr: 0.000500  loss_labels: 0.0433 (0.0429)  loss: 0.0449 (0.0429)  time: 0.1672\n",
      "Epoch: [39]  [1100/5008]  eta: 0:12:08  lr: 0.000500  loss_labels: 0.0357 (0.0424)  loss: 0.0330 (0.0424)  time: 0.1667\n",
      "Epoch: [39]  [1200/5008]  eta: 0:11:43  lr: 0.000500  loss_labels: 0.0412 (0.0425)  loss: 0.0452 (0.0425)  time: 0.1681\n",
      "Epoch: [39]  [1300/5008]  eta: 0:11:20  lr: 0.000500  loss_labels: 0.0356 (0.0422)  loss: 0.0456 (0.0422)  time: 0.1671\n",
      "Epoch: [39]  [1400/5008]  eta: 0:10:57  lr: 0.000500  loss_labels: 0.0393 (0.0421)  loss: 0.0392 (0.0421)  time: 0.1677\n",
      "Epoch: [39]  [1500/5008]  eta: 0:10:36  lr: 0.000500  loss_labels: 0.0391 (0.0421)  loss: 0.0323 (0.0421)  time: 0.1667\n",
      "Epoch: [39]  [1600/5008]  eta: 0:10:15  lr: 0.000500  loss_labels: 0.0349 (0.0421)  loss: 0.0412 (0.0421)  time: 0.1670\n",
      "Epoch: [39]  [1700/5008]  eta: 0:09:54  lr: 0.000500  loss_labels: 0.0378 (0.0421)  loss: 0.0362 (0.0421)  time: 0.1682\n",
      "Epoch: [39]  [1800/5008]  eta: 0:09:34  lr: 0.000500  loss_labels: 0.0385 (0.0420)  loss: 0.0397 (0.0420)  time: 0.1675\n",
      "Epoch: [39]  [1900/5008]  eta: 0:09:14  lr: 0.000500  loss_labels: 0.0426 (0.0422)  loss: 0.0459 (0.0422)  time: 0.1689\n",
      "Epoch: [39]  [2000/5008]  eta: 0:08:55  lr: 0.000500  loss_labels: 0.0371 (0.0420)  loss: 0.0284 (0.0420)  time: 0.1679\n",
      "Epoch: [39]  [2100/5008]  eta: 0:08:36  lr: 0.000500  loss_labels: 0.0370 (0.0419)  loss: 0.0388 (0.0419)  time: 0.1677\n",
      "Epoch: [39]  [2200/5008]  eta: 0:08:17  lr: 0.000500  loss_labels: 0.0349 (0.0419)  loss: 0.0308 (0.0419)  time: 0.1664\n",
      "Epoch: [39]  [2300/5008]  eta: 0:07:58  lr: 0.000500  loss_labels: 0.0363 (0.0418)  loss: 0.0367 (0.0418)  time: 0.1670\n",
      "Epoch: [39]  [2400/5008]  eta: 0:07:39  lr: 0.000500  loss_labels: 0.0359 (0.0417)  loss: 0.0433 (0.0417)  time: 0.1672\n",
      "Epoch: [39]  [2500/5008]  eta: 0:07:21  lr: 0.000500  loss_labels: 0.0400 (0.0417)  loss: 0.0371 (0.0417)  time: 0.1674\n",
      "Epoch: [39]  [2600/5008]  eta: 0:07:02  lr: 0.000500  loss_labels: 0.0394 (0.0418)  loss: 0.0394 (0.0418)  time: 0.1669\n",
      "Epoch: [39]  [2700/5008]  eta: 0:06:44  lr: 0.000500  loss_labels: 0.0353 (0.0417)  loss: 0.0301 (0.0417)  time: 0.1687\n",
      "Epoch: [39]  [2800/5008]  eta: 0:06:26  lr: 0.000500  loss_labels: 0.0408 (0.0418)  loss: 0.0426 (0.0418)  time: 0.1660\n",
      "Epoch: [39]  [2900/5008]  eta: 0:06:08  lr: 0.000500  loss_labels: 0.0416 (0.0419)  loss: 0.0393 (0.0419)  time: 0.1665\n",
      "Epoch: [39]  [3000/5008]  eta: 0:05:50  lr: 0.000500  loss_labels: 0.0378 (0.0418)  loss: 0.0441 (0.0418)  time: 0.1673\n",
      "Epoch: [39]  [3100/5008]  eta: 0:05:32  lr: 0.000500  loss_labels: 0.0375 (0.0418)  loss: 0.0417 (0.0418)  time: 0.1676\n",
      "Epoch: [39]  [3200/5008]  eta: 0:05:14  lr: 0.000500  loss_labels: 0.0388 (0.0418)  loss: 0.0420 (0.0418)  time: 0.1686\n",
      "Epoch: [39]  [3300/5008]  eta: 0:04:56  lr: 0.000500  loss_labels: 0.0395 (0.0418)  loss: 0.0362 (0.0418)  time: 0.1662\n",
      "Epoch: [39]  [3400/5008]  eta: 0:04:39  lr: 0.000500  loss_labels: 0.0384 (0.0418)  loss: 0.0384 (0.0418)  time: 0.1671\n",
      "Epoch: [39]  [3500/5008]  eta: 0:04:21  lr: 0.000500  loss_labels: 0.0430 (0.0419)  loss: 0.0431 (0.0419)  time: 0.1668\n",
      "Epoch: [39]  [3600/5008]  eta: 0:04:03  lr: 0.000500  loss_labels: 0.0347 (0.0419)  loss: 0.0401 (0.0419)  time: 0.1680\n",
      "Epoch: [39]  [3700/5008]  eta: 0:03:46  lr: 0.000500  loss_labels: 0.0344 (0.0418)  loss: 0.0344 (0.0418)  time: 0.1659\n",
      "Epoch: [39]  [3800/5008]  eta: 0:03:28  lr: 0.000500  loss_labels: 0.0392 (0.0419)  loss: 0.0361 (0.0419)  time: 0.1669\n",
      "Epoch: [39]  [3900/5008]  eta: 0:03:11  lr: 0.000500  loss_labels: 0.0434 (0.0420)  loss: 0.0300 (0.0420)  time: 0.1679\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://10.198.24.98:8913/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "!python main.py --save_model 1 --dataset 'vggface2' --wandb_p 'vggface2' --wandb_r 'blt_bl'  --model 'blt_bl' --epochs 100 --lr .0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 1): env://\n",
      "| distributed init (rank 0): env://\n",
      "cuda\n",
      "\n",
      "train_params ['conv_input.weight', 'conv_input.bias', 'norm_input.weight', 'norm_input.bias', 'norm_0.weight', 'norm_0.bias', 'norm_0_0.weight', 'norm_0_0.bias', 'conv_0_0.weight', 'conv_0_0.bias', 'norm_0_1.weight', 'norm_0_1.bias', 'conv_0_1.weight', 'conv_0_1.bias', 'norm_1.weight', 'norm_1.bias', 'norm_1_1.weight', 'norm_1_1.bias', 'conv_1_1.weight', 'conv_1_1.bias', 'norm_1_2.weight', 'norm_1_2.bias', 'conv_1_2.weight', 'conv_1_2.bias', 'norm_2.weight', 'norm_2.bias', 'norm_2_2.weight', 'norm_2_2.bias', 'conv_2_2.weight', 'conv_2_2.bias', 'norm_2_3.weight', 'norm_2_3.bias', 'conv_2_3.weight', 'conv_2_3.bias', 'norm_3.weight', 'norm_3.bias', 'norm_3_3.weight', 'norm_3_3.bias', 'conv_3_3.weight', 'conv_3_3.bias', 'read_out.linear.weight', 'read_out.linear.bias']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhosseinadeli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/engram/nklab/hossein/recurrent_models/BLT_models/wandb/run-20240528_212708-lyzej8x0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mblt_bl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hosseinadeli/imagenet\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/hosseinadeli/imagenet/runs/lyzej8x0\u001b[0m\n",
      "Start training\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch: [0]  [   0/5004]  eta: 3 days, 14:35:04  lr: 0.001000  loss_labels: 6.9735 (6.9735)  loss: 6.9735 (6.9735)  time: 62.2910\n",
      "Epoch: [0]  [ 100/5004]  eta: 3:28:42  lr: 0.001000  loss_labels: 6.9465 (6.9534)  loss: 6.9269 (6.9534)  time: 1.8904\n",
      "Epoch: [0]  [ 200/5004]  eta: 2:36:54  lr: 0.001000  loss_labels: 6.9254 (6.9399)  loss: 6.9205 (6.9399)  time: 1.1214\n",
      "Epoch: [0]  [ 300/5004]  eta: 2:05:41  lr: 0.001000  loss_labels: 6.9200 (6.9335)  loss: 6.9164 (6.9335)  time: 0.6759\n",
      "Epoch: [0]  [ 400/5004]  eta: 1:45:21  lr: 0.001000  loss_labels: 6.9108 (6.9277)  loss: 6.8966 (6.9277)  time: 0.6566\n",
      "Epoch: [0]  [ 500/5004]  eta: 1:31:31  lr: 0.001000  loss_labels: 6.8577 (6.9137)  loss: 6.8352 (6.9137)  time: 0.5003\n",
      "Epoch: [0]  [ 600/5004]  eta: 1:20:41  lr: 0.001000  loss_labels: 6.7776 (6.8915)  loss: 6.7376 (6.8915)  time: 0.4443\n",
      "Epoch: [0]  [ 700/5004]  eta: 1:11:35  lr: 0.001000  loss_labels: 6.7122 (6.8657)  loss: 6.6778 (6.8657)  time: 0.3602\n",
      "Epoch: [0]  [ 800/5004]  eta: 1:04:38  lr: 0.001000  loss_labels: 6.6445 (6.8380)  loss: 6.5994 (6.8380)  time: 0.4671\n",
      "Epoch: [0]  [ 900/5004]  eta: 0:59:03  lr: 0.001000  loss_labels: 6.5557 (6.8061)  loss: 6.5221 (6.8061)  time: 0.4116\n",
      "Epoch: [0]  [1000/5004]  eta: 0:53:59  lr: 0.001000  loss_labels: 6.4694 (6.7724)  loss: 6.4374 (6.7724)  time: 0.3235\n",
      "Epoch: [0]  [1100/5004]  eta: 0:50:02  lr: 0.001000  loss_labels: 6.3930 (6.7374)  loss: 6.3254 (6.7374)  time: 0.3590\n",
      "Epoch: [0]  [1200/5004]  eta: 0:46:20  lr: 0.001000  loss_labels: 6.3013 (6.7009)  loss: 6.2534 (6.7009)  time: 0.3357\n",
      "Epoch: [0]  [1300/5004]  eta: 0:43:17  lr: 0.001000  loss_labels: 6.1874 (6.6619)  loss: 6.1601 (6.6619)  time: 0.3929\n",
      "Epoch: [0]  [1400/5004]  eta: 0:40:48  lr: 0.001000  loss_labels: 6.0868 (6.6208)  loss: 6.0366 (6.6208)  time: 0.4336\n",
      "Epoch: [0]  [1500/5004]  eta: 0:38:46  lr: 0.001000  loss_labels: 6.0053 (6.5798)  loss: 5.9640 (6.5798)  time: 0.6161\n",
      "Epoch: [0]  [1600/5004]  eta: 0:39:01  lr: 0.001000  loss_labels: 5.9309 (6.5392)  loss: 5.8678 (6.5392)  time: 1.0540\n",
      "Epoch: [0]  [1700/5004]  eta: 0:38:56  lr: 0.001000  loss_labels: 5.8257 (6.4975)  loss: 5.7838 (6.4975)  time: 0.9939\n",
      "Epoch: [0]  [1800/5004]  eta: 0:38:34  lr: 0.001000  loss_labels: 5.7580 (6.4567)  loss: 5.7184 (6.4567)  time: 1.0221\n",
      "Epoch: [0]  [1900/5004]  eta: 0:38:05  lr: 0.001000  loss_labels: 5.6901 (6.4166)  loss: 5.6629 (6.4166)  time: 0.9323\n",
      "Epoch: [0]  [2000/5004]  eta: 0:37:43  lr: 0.001000  loss_labels: 5.6359 (6.3779)  loss: 5.6079 (6.3779)  time: 0.9149\n",
      "Epoch: [0]  [2100/5004]  eta: 0:37:06  lr: 0.001000  loss_labels: 5.5804 (6.3396)  loss: 5.5884 (6.3396)  time: 1.0356\n",
      "Epoch: [0]  [2200/5004]  eta: 0:36:08  lr: 0.001000  loss_labels: 5.4964 (6.3015)  loss: 5.4735 (6.3015)  time: 0.9100\n",
      "Epoch: [0]  [2300/5004]  eta: 0:35:22  lr: 0.001000  loss_labels: 5.4580 (6.2646)  loss: 5.4614 (6.2646)  time: 1.0556\n",
      "Epoch: [0]  [2400/5004]  eta: 0:34:39  lr: 0.001000  loss_labels: 5.4307 (6.2293)  loss: 5.3592 (6.2293)  time: 1.0672\n",
      "Epoch: [0]  [2500/5004]  eta: 0:34:45  lr: 0.001000  loss_labels: 5.3406 (6.1939)  loss: 5.3338 (6.1939)  time: 4.0296\n",
      "Epoch: [0]  [2600/5004]  eta: 0:33:34  lr: 0.001000  loss_labels: 5.2965 (6.1591)  loss: 5.2759 (6.1591)  time: 0.9670\n",
      "Epoch: [0]  [2700/5004]  eta: 0:32:23  lr: 0.001000  loss_labels: 5.2614 (6.1261)  loss: 5.2817 (6.1261)  time: 0.9435\n",
      "Epoch: [0]  [2800/5004]  eta: 0:31:17  lr: 0.001000  loss_labels: 5.2078 (6.0934)  loss: 5.1780 (6.0934)  time: 1.0716\n",
      "Epoch: [0]  [2900/5004]  eta: 0:30:05  lr: 0.001000  loss_labels: 5.1630 (6.0618)  loss: 5.1751 (6.0618)  time: 0.9094\n",
      "Epoch: [0]  [3000/5004]  eta: 0:29:03  lr: 0.001000  loss_labels: 5.1443 (6.0315)  loss: 5.0919 (6.0315)  time: 1.2187\n",
      "Epoch: [0]  [3100/5004]  eta: 0:27:59  lr: 0.001000  loss_labels: 5.0802 (6.0009)  loss: 5.0977 (6.0009)  time: 1.3085\n",
      "Epoch: [0]  [3200/5004]  eta: 0:26:52  lr: 0.001000  loss_labels: 5.0523 (5.9714)  loss: 5.0435 (5.9714)  time: 1.2337\n",
      "Epoch: [0]  [3300/5004]  eta: 0:25:37  lr: 0.001000  loss_labels: 5.0405 (5.9430)  loss: 4.9911 (5.9430)  time: 1.1294\n",
      "Epoch: [0]  [3400/5004]  eta: 0:24:19  lr: 0.001000  loss_labels: 4.9819 (5.9151)  loss: 5.0082 (5.9151)  time: 1.2303\n",
      "Epoch: [0]  [3500/5004]  eta: 0:23:02  lr: 0.001000  loss_labels: 4.9461 (5.8878)  loss: 4.9367 (5.8878)  time: 1.3257\n",
      "Epoch: [0]  [3600/5004]  eta: 0:21:48  lr: 0.001000  loss_labels: 4.9412 (5.8614)  loss: 4.9355 (5.8614)  time: 1.7037\n",
      "Epoch: [0]  [3700/5004]  eta: 0:20:41  lr: 0.001000  loss_labels: 4.9059 (5.8357)  loss: 4.8875 (5.8357)  time: 1.8987\n",
      "Epoch: [0]  [3800/5004]  eta: 0:19:34  lr: 0.001000  loss_labels: 4.8407 (5.8098)  loss: 4.8600 (5.8098)  time: 1.7958\n",
      "Epoch: [0]  [3900/5004]  eta: 0:18:17  lr: 0.001000  loss_labels: 4.8310 (5.7852)  loss: 4.8310 (5.7852)  time: 1.9396\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://10.198.24.98:8913/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "!python main.py --save_model 0 --dataset 'imagenet' --wandb_p 'imagenet' --wandb_r 'blt_bl'  --model 'blt_bl' --epochs 100 --lr .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 12510, 25020, 37530, 50040, 62550])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3378191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Define lists of values\n",
    "# num_layers_value = [2, 4, 5, 7]\n",
    "# num_layers_value = np.arange(1, 6)\n",
    "# seeds = [1, 2, 3]\n",
    "# seed = 0\n",
    "\n",
    "# face vggface2 recon  # \"face detr dino\"\n",
    "##SBATCH --nodelist=ax01  --num_ids 1000 \n",
    "#--wandb_p 'imagenet classification' --wandb_r 'cornet_z'\n",
    "##SBATCH --nodelist=ax19\n",
    "##SBATCH --exclude=ax[03-15]\n",
    "# python main.py --save_model 1 --wandb_p 'NSD' --wandb_r 'faces_dinov2'  --objective 'nsd' --readout_res 'faces' --task_arch 'transformer' --backbone_arch 'dinov2' --batch_size 32\n",
    "\n",
    "#python main.py --save_model 1 --wandb_p 'imagenet classification' --wandb_r 'cornet_rt_8steps'  --objective 'classification' --dataset 'imagenet' --task_arch 'cornet_rt' --batch_size 512\n",
    "\n",
    "#python main.py --save_model 1 --wandb_p 'imagenet_vggface2' --wandb_r 'cornet_rt_8steps'  --objective 'classification' --dataset 'imagenet_vggface2' --run 2 --task_arch 'cornet_rt' --batch_size 384\n",
    "\n",
    "#!python main.py --save_model 1 --wandb_p 'NSD' --wandb_r 'faces_dinov2'  --objective 'nsd' --readout_res 'faces' --task_arch 'transformer' --backbone_arch 'dinov2' --batch_size 32 --run 2\n",
    "\n",
    "#python main.py --save_model 1 --resume '/engram/nklab/hossein/recurrent_models/transformer_brain/results_new/classification_cornet_rt_imagenet_vggface2/run_1/checkpoint.pth' --wandb_p 'imagenet_vggface2' --wandb_r 'cornet_rt_8steps'  --objective 'classification' --dataset 'imagenet_vggface2' --run 1 --task_arch 'cornet_rt' --batch_size 768\n",
    "\n",
    "#python main.py --save_model 1 --distributed 1 --wandb_p 'imagenet_vggface2' --wandb_r 'b3lt3_4l_8t'  --objective 'classification' --dataset 'imagenet_vggface2' --run 1 --task_arch 'blt_b3lt3' --batch_size 512\n",
    "\n",
    "#python main.py --save_model 1 --distributed 1 --wandb_p 'imagenet_vggface2' --wandb_r 'cornet_rt_8steps_ddp'  --objective 'classification' --dataset 'imagenet_vggface2' --run 3 --task_arch 'cornet_rt' --batch_size 128\n",
    "\n",
    "#python main.py --save_model 1 --distributed 1 --wandb_p 'imagenet_vggface2' --wandb_r 'blt_5l_9t'  --objective 'classification' --dataset 'imagenet_vggface2' --run 1 --task_arch 'blt_blt' --recurrent_steps 9 --batch_size 512\n",
    "\n",
    "#python main.py --save_model 1 --wandb_p 'imagenet_vggface2' --wandb_r 'cornet_rt_8steps_ddp'  --objective 'classification' --dataset 'imagenet_vggface2' --run 4 --task_arch 'cornet_rt' --batch_size 128\n",
    "\n",
    "script = f'''#!/bin/sh\n",
    "#\n",
    "#\n",
    "#SBATCH --account=nklab\n",
    "#SBATCH --job-name=bl  # The job name.\n",
    "#SBATCH --gres=gpu:2\n",
    "#SBATCH --nodelist=ax17\n",
    "#SBATCH --cpus-per-task=24\n",
    "\n",
    "ml load anaconda3-2019.03\n",
    "\n",
    "cd /engram/nklab/hossein/recurrent_models/BLT_models/\n",
    "\n",
    "conda activate pytorch\n",
    "\n",
    "python main.py --save_model 0 --dataset 'imagenet' --wandb_p 'imagenet' --wandb_r 'blt_bl'  --model 'blt_bl' --epochs 100 --lr .001\n",
    "\n",
    "'''\n",
    "\n",
    "bash_script_path = \"/engram/nklab/hossein/batch_scripts/imagenet_nb.sh\"\n",
    "os.chdir('/engram/nklab/hossein/batch_scripts/')\n",
    "\n",
    "with open(bash_script_path, \"w+\") as bash_script_file:\n",
    "    bash_script_file.write(script)\n",
    "\n",
    "subprocess.run(['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
