{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/BLT_models\n"
     ]
    }
   ],
   "source": [
    "data_path = '/share/data/imagenet-pytorch'\n",
    "\n",
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/BLT_models/')\n",
    "!pwd\n",
    "\n",
    "#--wandb_p 'vggface2' --wandb_r 'blt_bl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 1): env://\n",
      "| distributed init (rank 0): env://\n",
      "cuda\n",
      "100%|██████████████████████████████████████| 3890/3890 [00:04<00:00, 790.13it/s]\n",
      "100%|██████████████████████████████████████| 3890/3890 [00:04<00:00, 790.29it/s]\n",
      "100%|█████████████████████████████████████| 3890/3890 [00:01<00:00, 2185.38it/s]\n",
      "100%|█████████████████████████████████████| 3890/3890 [00:01<00:00, 2203.82it/s]\n",
      "Number of model parameters: 13530866\n",
      "blt(\n",
      "  (conv_input): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "  (non_lin_input): ReLU(inplace=True)\n",
      "  (norm_input): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (non_lin_0): ReLU(inplace=True)\n",
      "  (norm_0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (output_0): Identity()\n",
      "  (conv_0_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_0_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv_0_2): Conv2d(64, 128, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))\n",
      "  (conv_0_3): Conv2d(64, 128, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))\n",
      "  (non_lin_1): ReLU(inplace=True)\n",
      "  (norm_1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (output_1): Identity()\n",
      "  (conv_1_0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_1_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_1_2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv_1_3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv_1_4): Conv2d(64, 256, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2))\n",
      "  (non_lin_2): ReLU(inplace=True)\n",
      "  (norm_2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (output_2): Identity()\n",
      "  (conv_2_0): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2), output_padding=(3, 3))\n",
      "  (conv_2_1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_2_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_2_4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv_2_5): Conv2d(128, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (non_lin_3): ReLU(inplace=True)\n",
      "  (norm_3): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (output_3): Identity()\n",
      "  (conv_3_0): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2), output_padding=(3, 3))\n",
      "  (conv_3_1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_3_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_3_4): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv_3_5): Conv2d(128, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (non_lin_4): ReLU(inplace=True)\n",
      "  (norm_4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (output_4): Identity()\n",
      "  (conv_4_1): ConvTranspose2d(256, 64, kernel_size=(5, 5), stride=(4, 4), padding=(2, 2), output_padding=(3, 3))\n",
      "  (conv_4_2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_4_3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_4_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_4_5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (non_lin_5): ReLU(inplace=True)\n",
      "  (norm_5): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (output_5): Identity()\n",
      "  (conv_5_2): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_5_3): ConvTranspose2d(512, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_5_4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_5_5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (read_out): Sequential(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=512, out_features=3890, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['conv_input.weight', 'conv_input.bias', 'norm_input.weight', 'norm_input.bias', 'norm_0.weight', 'norm_0.bias', 'conv_0_0.weight', 'conv_0_0.bias', 'conv_0_1.weight', 'conv_0_1.bias', 'conv_0_2.weight', 'conv_0_2.bias', 'conv_0_3.weight', 'conv_0_3.bias', 'norm_1.weight', 'norm_1.bias', 'conv_1_0.weight', 'conv_1_0.bias', 'conv_1_1.weight', 'conv_1_1.bias', 'conv_1_2.weight', 'conv_1_2.bias', 'conv_1_3.weight', 'conv_1_3.bias', 'conv_1_4.weight', 'conv_1_4.bias', 'norm_2.weight', 'norm_2.bias', 'conv_2_0.weight', 'conv_2_0.bias', 'conv_2_1.weight', 'conv_2_1.bias', 'conv_2_2.weight', 'conv_2_2.bias', 'conv_2_3.weight', 'conv_2_3.bias', 'conv_2_4.weight', 'conv_2_4.bias', 'conv_2_5.weight', 'conv_2_5.bias', 'norm_3.weight', 'norm_3.bias', 'conv_3_0.weight', 'conv_3_0.bias', 'conv_3_1.weight', 'conv_3_1.bias', 'conv_3_2.weight', 'conv_3_2.bias', 'conv_3_3.weight', 'conv_3_3.bias', 'conv_3_4.weight', 'conv_3_4.bias', 'conv_3_5.weight', 'conv_3_5.bias', 'norm_4.weight', 'norm_4.bias', 'conv_4_1.weight', 'conv_4_1.bias', 'conv_4_2.weight', 'conv_4_2.bias', 'conv_4_3.weight', 'conv_4_3.bias', 'conv_4_4.weight', 'conv_4_4.bias', 'conv_4_5.weight', 'conv_4_5.bias', 'norm_5.weight', 'norm_5.bias', 'conv_5_2.weight', 'conv_5_2.bias', 'conv_5_3.weight', 'conv_5_3.bias', 'conv_5_4.weight', 'conv_5_4.bias', 'conv_5_5.weight', 'conv_5_5.bias', 'read_out.linear.weight', 'read_out.linear.bias']\n",
      "Start training\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch: [0]  [   0/5008]  eta: 1 day, 4:08:07  lr: 0.004000  loss_labels: 8.2866 (8.2866)  loss: 8.2866 (8.2866)  time: 20.2251\n",
      "Epoch: [0]  [ 100/5008]  eta: 0:35:48  lr: 0.004000  loss_labels: 8.2655 (8.3520)  loss: 8.2557 (8.3520)  time: 0.2396\n",
      "Epoch: [0]  [ 200/5008]  eta: 0:27:12  lr: 0.004000  loss_labels: 8.2514 (8.3016)  loss: 8.2490 (8.3016)  time: 0.2399\n",
      "Epoch: [0]  [ 300/5008]  eta: 0:24:02  lr: 0.004000  loss_labels: 8.2437 (8.2827)  loss: 8.2400 (8.2827)  time: 0.2392\n",
      "Epoch: [0]  [ 400/5008]  eta: 0:22:15  lr: 0.004000  loss_labels: 8.2374 (8.2710)  loss: 8.2303 (8.2710)  time: 0.2385\n",
      "Epoch: [0]  [ 500/5008]  eta: 0:21:01  lr: 0.004000  loss_labels: 8.2221 (8.2612)  loss: 8.2108 (8.2612)  time: 0.2407\n",
      "Epoch: [0]  [ 600/5008]  eta: 0:20:03  lr: 0.004000  loss_labels: 8.2028 (8.2511)  loss: 8.1720 (8.2511)  time: 0.2388\n",
      "Epoch: [0]  [ 700/5008]  eta: 0:19:16  lr: 0.004000  loss_labels: 8.1530 (8.2365)  loss: 8.1205 (8.2365)  time: 0.2447\n",
      "Epoch: [0]  [ 800/5008]  eta: 0:18:35  lr: 0.004000  loss_labels: 8.0368 (8.2111)  loss: 7.9608 (8.2111)  time: 0.2405\n",
      "Epoch: [0]  [ 900/5008]  eta: 0:17:57  lr: 0.004000  loss_labels: 7.9110 (8.1773)  loss: 7.8402 (8.1773)  time: 0.2393\n",
      "Epoch: [0]  [1000/5008]  eta: 0:17:22  lr: 0.004000  loss_labels: 7.7324 (8.1326)  loss: 7.6430 (8.1326)  time: 0.2397\n",
      "Epoch: [0]  [1100/5008]  eta: 0:16:49  lr: 0.004000  loss_labels: 7.5453 (8.0794)  loss: 7.4662 (8.0794)  time: 0.2397\n",
      "Epoch: [0]  [1200/5008]  eta: 0:16:18  lr: 0.004000  loss_labels: 7.3911 (8.0212)  loss: 7.3620 (8.0212)  time: 0.2409\n",
      "Epoch: [0]  [1300/5008]  eta: 0:15:47  lr: 0.004000  loss_labels: 7.2803 (7.9649)  loss: 7.2658 (7.9649)  time: 0.2414\n",
      "Epoch: [0]  [1400/5008]  eta: 0:15:19  lr: 0.004000  loss_labels: 7.1723 (7.9081)  loss: 7.1155 (7.9081)  time: 0.2430\n",
      "Epoch: [0]  [1500/5008]  eta: 0:14:50  lr: 0.004000  loss_labels: 7.0519 (7.8505)  loss: 6.9964 (7.8505)  time: 0.2407\n",
      "Epoch: [0]  [1600/5008]  eta: 0:14:22  lr: 0.004000  loss_labels: 6.9492 (7.7946)  loss: 6.9110 (7.7946)  time: 0.2402\n",
      "Epoch: [0]  [1700/5008]  eta: 0:13:54  lr: 0.004000  loss_labels: 6.8410 (7.7390)  loss: 6.8102 (7.7390)  time: 0.2419\n",
      "Epoch: [0]  [1800/5008]  eta: 0:13:27  lr: 0.004000  loss_labels: 6.7252 (7.6831)  loss: 6.6676 (7.6831)  time: 0.2411\n",
      "Epoch: [0]  [1900/5008]  eta: 0:13:00  lr: 0.004000  loss_labels: 6.6250 (7.6273)  loss: 6.5875 (7.6273)  time: 0.2405\n",
      "Epoch: [0]  [2000/5008]  eta: 0:12:34  lr: 0.004000  loss_labels: 6.4885 (7.5702)  loss: 6.4124 (7.5702)  time: 0.2411\n",
      "Epoch: [0]  [2100/5008]  eta: 0:12:07  lr: 0.004000  loss_labels: 6.3648 (7.5124)  loss: 6.2514 (7.5124)  time: 0.2417\n",
      "Epoch: [0]  [2200/5008]  eta: 0:11:43  lr: 0.004000  loss_labels: 6.1973 (7.4531)  loss: 6.1549 (7.4531)  time: 0.2393\n",
      "Epoch: [0]  [2300/5008]  eta: 0:11:16  lr: 0.004000  loss_labels: 6.0969 (7.3947)  loss: 6.0375 (7.3947)  time: 0.2401\n",
      "Epoch: [0]  [2400/5008]  eta: 0:10:53  lr: 0.004000  loss_labels: 5.9831 (7.3360)  loss: 5.8969 (7.3360)  time: 0.2391\n",
      "Epoch: [0]  [2500/5008]  eta: 0:10:27  lr: 0.004000  loss_labels: 5.8650 (7.2773)  loss: 5.8125 (7.2773)  time: 0.2392\n",
      "Epoch: [0]  [2600/5008]  eta: 0:10:01  lr: 0.004000  loss_labels: 5.7087 (7.2176)  loss: 5.6603 (7.2176)  time: 0.2398\n",
      "Epoch: [0]  [2700/5008]  eta: 0:09:35  lr: 0.004000  loss_labels: 5.6083 (7.1577)  loss: 5.5063 (7.1577)  time: 0.2394\n",
      "Epoch: [0]  [2800/5008]  eta: 0:09:10  lr: 0.004000  loss_labels: 5.4843 (7.0980)  loss: 5.4214 (7.0980)  time: 0.2422\n",
      "Epoch: [0]  [2900/5008]  eta: 0:08:44  lr: 0.004000  loss_labels: 5.3112 (7.0371)  loss: 5.2604 (7.0371)  time: 0.2407\n",
      "Epoch: [0]  [3000/5008]  eta: 0:08:19  lr: 0.004000  loss_labels: 5.2445 (6.9782)  loss: 5.1913 (6.9782)  time: 0.2399\n",
      "Epoch: [0]  [3100/5008]  eta: 0:07:53  lr: 0.004000  loss_labels: 5.1476 (6.9189)  loss: 5.0444 (6.9189)  time: 0.2402\n",
      "Epoch: [0]  [3200/5008]  eta: 0:07:28  lr: 0.004000  loss_labels: 5.0263 (6.8594)  loss: 4.9942 (6.8594)  time: 0.2405\n",
      "Epoch: [0]  [3300/5008]  eta: 0:07:03  lr: 0.004000  loss_labels: 4.9077 (6.8004)  loss: 4.8571 (6.8004)  time: 0.2405\n",
      "Epoch: [0]  [3400/5008]  eta: 0:06:38  lr: 0.004000  loss_labels: 4.8002 (6.7416)  loss: 4.7401 (6.7416)  time: 0.2411\n",
      "Epoch: [0]  [3500/5008]  eta: 0:06:13  lr: 0.004000  loss_labels: 4.7091 (6.6836)  loss: 4.7098 (6.6836)  time: 0.2400\n",
      "Epoch: [0]  [3600/5008]  eta: 0:05:47  lr: 0.004000  loss_labels: 4.6376 (6.6269)  loss: 4.6378 (6.6269)  time: 0.2399\n",
      "Epoch: [0]  [3700/5008]  eta: 0:05:23  lr: 0.004000  loss_labels: 4.5633 (6.5706)  loss: 4.4624 (6.5706)  time: 0.2396\n",
      "Epoch: [0]  [3800/5008]  eta: 0:04:58  lr: 0.004000  loss_labels: 4.4526 (6.5146)  loss: 4.3166 (6.5146)  time: 0.2405\n",
      "Epoch: [0]  [3900/5008]  eta: 0:04:33  lr: 0.004000  loss_labels: 4.3577 (6.4592)  loss: 4.3275 (6.4592)  time: 0.2391\n",
      "Epoch: [0]  [4000/5008]  eta: 0:04:08  lr: 0.004000  loss_labels: 4.2415 (6.4040)  loss: 4.2161 (6.4040)  time: 0.2397\n",
      "Epoch: [0]  [4100/5008]  eta: 0:03:43  lr: 0.004000  loss_labels: 4.1782 (6.3496)  loss: 4.1782 (6.3496)  time: 0.2407\n",
      "Epoch: [0]  [4200/5008]  eta: 0:03:18  lr: 0.004000  loss_labels: 4.0820 (6.2962)  loss: 4.0156 (6.2962)  time: 0.2398\n",
      "Epoch: [0]  [4300/5008]  eta: 0:02:54  lr: 0.004000  loss_labels: 3.9965 (6.2431)  loss: 3.9287 (6.2431)  time: 0.2414\n",
      "Epoch: [0]  [4400/5008]  eta: 0:02:29  lr: 0.004000  loss_labels: 3.9394 (6.1911)  loss: 3.9165 (6.1911)  time: 0.2415\n",
      "Epoch: [0]  [4500/5008]  eta: 0:02:04  lr: 0.004000  loss_labels: 3.8678 (6.1390)  loss: 3.9012 (6.1390)  time: 0.2413\n",
      "Epoch: [0]  [4600/5008]  eta: 0:01:40  lr: 0.004000  loss_labels: 3.7265 (6.0868)  loss: 3.7191 (6.0868)  time: 0.2402\n",
      "Epoch: [0]  [4700/5008]  eta: 0:01:15  lr: 0.004000  loss_labels: 3.6775 (6.0355)  loss: 3.6683 (6.0355)  time: 0.2419\n",
      "Epoch: [0]  [4800/5008]  eta: 0:00:51  lr: 0.004000  loss_labels: 3.5764 (5.9848)  loss: 3.6339 (5.9848)  time: 0.2437\n",
      "Epoch: [0]  [4900/5008]  eta: 0:00:26  lr: 0.004000  loss_labels: 3.5419 (5.9350)  loss: 3.4777 (5.9350)  time: 0.2416\n",
      "Epoch: [0]  [5000/5008]  eta: 0:00:01  lr: 0.004000  loss_labels: 3.4976 (5.8861)  loss: 3.5006 (5.8861)  time: 0.2406\n",
      "Epoch: [0]  [5007/5008]  eta: 0:00:00  lr: 0.004000  loss_labels: 3.4977 (5.8828)  loss: 3.4977 (5.8828)  time: 0.2411\n",
      "Epoch: [0] Total time: 0:20:29 (0.2455 s / it)\n",
      "Averaged stats: lr: 0.004000  loss_labels: 3.4977 (5.8828)  loss: 3.4977 (5.8828)\n",
      "Test:  [  0/565]  eta: 2:23:24  loss_labels: 3.7167 (3.7167)  loss: 3.7167 (3.7167)  time: 15.2286\n",
      "Test:  [100/565]  eta: 0:02:06  loss_labels: 4.0747 (4.0650)  loss: 4.2121 (4.0650)  time: 0.1200\n",
      "Test:  [200/565]  eta: 0:01:13  loss_labels: 3.9801 (4.0402)  loss: 4.1022 (4.0402)  time: 0.1553\n",
      "Test:  [300/565]  eta: 0:00:46  loss_labels: 3.9308 (3.9944)  loss: 3.7372 (3.9944)  time: 0.1193\n",
      "Test:  [400/565]  eta: 0:00:26  loss_labels: 4.0761 (4.0164)  loss: 3.8784 (4.0164)  time: 0.1225\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 3.8446 (3.9817)  loss: 4.2208 (3.9817)  time: 0.1250\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 3.9077 (3.9716)  loss: 3.9264 (3.9716)  time: 0.1288\n",
      "Test: Total time: 0:01:25 (0.1522 s / it)\n",
      "Averaged stats: loss_labels: 3.9077 (3.9716)  loss: 3.9264 (3.9716)\n",
      "acc: 0.26952582597732544\n",
      "top 1 and top 5 accuracies {'top1': 0.26952584058051293, 'top5': 0.4748518251814103, 'loss': tensor(0.0311, device='cuda:0')}\n",
      "Epoch: [1]  [   0/5008]  eta: 1 day, 3:37:42  lr: 0.004000  loss_labels: 3.4689 (3.4689)  loss: 3.4689 (3.4689)  time: 19.8608\n",
      "Epoch: [1]  [ 100/5008]  eta: 0:35:20  lr: 0.004000  loss_labels: 3.3984 (3.4058)  loss: 3.2789 (3.4058)  time: 0.2380\n",
      "Epoch: [1]  [ 200/5008]  eta: 0:26:56  lr: 0.004000  loss_labels: 3.3527 (3.3682)  loss: 3.4515 (3.3682)  time: 0.2392\n",
      "Epoch: [1]  [ 300/5008]  eta: 0:23:50  lr: 0.004000  loss_labels: 3.2937 (3.3398)  loss: 3.3378 (3.3398)  time: 0.2393\n",
      "Epoch: [1]  [ 400/5008]  eta: 0:22:06  lr: 0.004000  loss_labels: 3.1745 (3.3058)  loss: 3.1151 (3.3058)  time: 0.2399\n",
      "Epoch: [1]  [ 500/5008]  eta: 0:21:08  lr: 0.004000  loss_labels: 3.1962 (3.2829)  loss: 3.1470 (3.2829)  time: 0.2739\n",
      "Epoch: [1]  [ 600/5008]  eta: 0:20:14  lr: 0.004000  loss_labels: 3.0733 (3.2487)  loss: 2.9950 (3.2487)  time: 0.2402\n",
      "Epoch: [1]  [ 700/5008]  eta: 0:19:28  lr: 0.004000  loss_labels: 3.0438 (3.2172)  loss: 2.9586 (3.2172)  time: 0.2421\n",
      "Epoch: [1]  [ 800/5008]  eta: 0:18:45  lr: 0.004000  loss_labels: 2.9541 (3.1853)  loss: 2.9697 (3.1853)  time: 0.2409\n",
      "Epoch: [1]  [ 900/5008]  eta: 0:18:27  lr: 0.004000  loss_labels: 2.9167 (3.1576)  loss: 2.8914 (3.1576)  time: 0.3910\n",
      "Epoch: [1]  [1000/5008]  eta: 0:18:51  lr: 0.004000  loss_labels: 2.8577 (3.1287)  loss: 2.8323 (3.1287)  time: 0.3922\n",
      "Epoch: [1]  [1100/5008]  eta: 0:19:02  lr: 0.004000  loss_labels: 2.8254 (3.1011)  loss: 2.8010 (3.1011)  time: 0.3872\n",
      "Epoch: [1]  [1200/5008]  eta: 0:19:05  lr: 0.004000  loss_labels: 2.7834 (3.0743)  loss: 2.7517 (3.0743)  time: 0.3882\n",
      "Epoch: [1]  [1300/5008]  eta: 0:19:03  lr: 0.004000  loss_labels: 2.7371 (3.0476)  loss: 2.7189 (3.0476)  time: 0.3959\n",
      "Epoch: [1]  [1400/5008]  eta: 0:18:53  lr: 0.004000  loss_labels: 2.6864 (3.0220)  loss: 2.6279 (3.0220)  time: 0.3869\n",
      "Epoch: [1]  [1500/5008]  eta: 0:18:40  lr: 0.004000  loss_labels: 2.6455 (2.9977)  loss: 2.6281 (2.9977)  time: 0.3980\n",
      "Epoch: [1]  [1600/5008]  eta: 0:18:24  lr: 0.004000  loss_labels: 2.6123 (2.9738)  loss: 2.5776 (2.9738)  time: 0.3909\n",
      "Epoch: [1]  [1700/5008]  eta: 0:18:04  lr: 0.004000  loss_labels: 2.5669 (2.9499)  loss: 2.5320 (2.9499)  time: 0.3899\n",
      "Epoch: [1]  [1800/5008]  eta: 0:17:42  lr: 0.004000  loss_labels: 2.5078 (2.9256)  loss: 2.5146 (2.9256)  time: 0.3834\n",
      "Epoch: [1]  [1900/5008]  eta: 0:17:18  lr: 0.004000  loss_labels: 2.4761 (2.9021)  loss: 2.4511 (2.9021)  time: 0.3851\n",
      "Epoch: [1]  [2000/5008]  eta: 0:16:53  lr: 0.004000  loss_labels: 2.4423 (2.8793)  loss: 2.3832 (2.8793)  time: 0.3868\n",
      "Epoch: [1]  [2100/5008]  eta: 0:16:17  lr: 0.004000  loss_labels: 2.3846 (2.8561)  loss: 2.3562 (2.8561)  time: 0.2408\n",
      "Epoch: [1]  [2200/5008]  eta: 0:15:32  lr: 0.004000  loss_labels: 2.3261 (2.8328)  loss: 2.3316 (2.8328)  time: 0.2429\n",
      "Epoch: [1]  [2300/5008]  eta: 0:15:05  lr: 0.004000  loss_labels: 2.3424 (2.8120)  loss: 2.3330 (2.8120)  time: 0.4129\n",
      "Epoch: [1]  [2400/5008]  eta: 0:14:40  lr: 0.004000  loss_labels: 2.2808 (2.7911)  loss: 2.2849 (2.7911)  time: 0.4023\n",
      "Epoch: [1]  [2500/5008]  eta: 0:14:12  lr: 0.004000  loss_labels: 2.2828 (2.7707)  loss: 2.2828 (2.7707)  time: 0.3995\n",
      "Epoch: [1]  [2600/5008]  eta: 0:13:44  lr: 0.004000  loss_labels: 2.2307 (2.7503)  loss: 2.2153 (2.7503)  time: 0.3973\n",
      "Epoch: [1]  [2700/5008]  eta: 0:13:15  lr: 0.004000  loss_labels: 2.1815 (2.7294)  loss: 2.1789 (2.7294)  time: 0.4034\n",
      "Epoch: [1]  [2800/5008]  eta: 0:12:45  lr: 0.004000  loss_labels: 2.1235 (2.7090)  loss: 2.1112 (2.7090)  time: 0.3953\n",
      "Epoch: [1]  [2900/5008]  eta: 0:12:15  lr: 0.004000  loss_labels: 2.1320 (2.6895)  loss: 2.0734 (2.6895)  time: 0.4093\n",
      "Epoch: [1]  [3000/5008]  eta: 0:11:44  lr: 0.004000  loss_labels: 2.1215 (2.6712)  loss: 2.1394 (2.6712)  time: 0.4125\n",
      "Epoch: [1]  [3100/5008]  eta: 0:11:12  lr: 0.004000  loss_labels: 2.0979 (2.6525)  loss: 2.0130 (2.6525)  time: 0.3910\n",
      "Epoch: [1]  [3200/5008]  eta: 0:10:39  lr: 0.004000  loss_labels: 2.0332 (2.6336)  loss: 2.0834 (2.6336)  time: 0.4081\n",
      "Epoch: [1]  [3300/5008]  eta: 0:10:06  lr: 0.004000  loss_labels: 2.0440 (2.6157)  loss: 2.0259 (2.6157)  time: 0.4089\n",
      "Epoch: [1]  [3400/5008]  eta: 0:09:32  lr: 0.004000  loss_labels: 1.9835 (2.5969)  loss: 1.9609 (2.5969)  time: 0.3978\n",
      "Epoch: [1]  [3500/5008]  eta: 0:08:59  lr: 0.004000  loss_labels: 1.9868 (2.5794)  loss: 2.0064 (2.5794)  time: 0.3931\n",
      "Epoch: [1]  [3600/5008]  eta: 0:08:25  lr: 0.004000  loss_labels: 1.9577 (2.5623)  loss: 1.9154 (2.5623)  time: 0.4047\n",
      "Epoch: [1]  [3700/5008]  eta: 0:07:50  lr: 0.004000  loss_labels: 1.9163 (2.5449)  loss: 1.8951 (2.5449)  time: 0.3940\n",
      "Epoch: [1]  [3800/5008]  eta: 0:07:15  lr: 0.004000  loss_labels: 1.9087 (2.5280)  loss: 1.8523 (2.5280)  time: 0.4113\n",
      "Epoch: [1]  [3900/5008]  eta: 0:06:40  lr: 0.004000  loss_labels: 1.8656 (2.5115)  loss: 1.8172 (2.5115)  time: 0.4031\n",
      "Epoch: [1]  [4000/5008]  eta: 0:06:03  lr: 0.004000  loss_labels: 1.8294 (2.4951)  loss: 1.8108 (2.4951)  time: 0.2401\n",
      "Epoch: [1]  [4100/5008]  eta: 0:05:25  lr: 0.004000  loss_labels: 1.8205 (2.4784)  loss: 1.8144 (2.4784)  time: 0.2415\n",
      "Epoch: [1]  [4200/5008]  eta: 0:04:48  lr: 0.004000  loss_labels: 1.7806 (2.4620)  loss: 1.8330 (2.4620)  time: 0.4182\n",
      "Epoch: [1]  [4300/5008]  eta: 0:04:13  lr: 0.004000  loss_labels: 1.7769 (2.4460)  loss: 1.7585 (2.4460)  time: 0.4084\n",
      "Epoch: [1]  [4400/5008]  eta: 0:03:38  lr: 0.004000  loss_labels: 1.7648 (2.4307)  loss: 1.7738 (2.4307)  time: 0.4064\n",
      "Epoch: [1]  [4500/5008]  eta: 0:03:03  lr: 0.004000  loss_labels: 1.7449 (2.4155)  loss: 1.8514 (2.4155)  time: 0.4161\n",
      "Epoch: [1]  [4600/5008]  eta: 0:02:27  lr: 0.004000  loss_labels: 1.7182 (2.4002)  loss: 1.7836 (2.4002)  time: 0.4002\n",
      "Epoch: [1]  [4700/5008]  eta: 0:01:51  lr: 0.004000  loss_labels: 1.6747 (2.3849)  loss: 1.6820 (2.3849)  time: 0.3953\n",
      "Epoch: [1]  [4800/5008]  eta: 0:01:15  lr: 0.004000  loss_labels: 1.6651 (2.3700)  loss: 1.6845 (2.3700)  time: 0.3970\n",
      "Epoch: [1]  [4900/5008]  eta: 0:00:39  lr: 0.004000  loss_labels: 1.6654 (2.3554)  loss: 1.6027 (2.3554)  time: 0.4008\n",
      "Epoch: [1]  [5000/5008]  eta: 0:00:02  lr: 0.004000  loss_labels: 1.6449 (2.3412)  loss: 1.6508 (2.3412)  time: 0.4051\n",
      "Epoch: [1]  [5007/5008]  eta: 0:00:00  lr: 0.004000  loss_labels: 1.6449 (2.3403)  loss: 1.6508 (2.3403)  time: 0.4058\n",
      "Epoch: [1] Total time: 0:30:26 (0.3648 s / it)\n",
      "Averaged stats: lr: 0.004000  loss_labels: 1.6449 (2.3403)  loss: 1.6508 (2.3403)\n",
      "Test:  [  0/565]  eta: 2:51:12  loss_labels: 2.2264 (2.2264)  loss: 2.2264 (2.2264)  time: 18.1822\n",
      "Test:  [100/565]  eta: 0:02:51  loss_labels: 2.3789 (2.4246)  loss: 2.5039 (2.4246)  time: 0.1874\n",
      "Test:  [200/565]  eta: 0:01:41  loss_labels: 2.2292 (2.3841)  loss: 2.4256 (2.3841)  time: 0.1830\n",
      "Test:  [300/565]  eta: 0:01:05  loss_labels: 2.1649 (2.3279)  loss: 2.0129 (2.3279)  time: 0.1831\n",
      "Test:  [400/565]  eta: 0:00:38  loss_labels: 2.3757 (2.3593)  loss: 2.2904 (2.3593)  time: 0.1933\n",
      "Test:  [500/565]  eta: 0:00:14  loss_labels: 2.1368 (2.3242)  loss: 2.3552 (2.3242)  time: 0.1817\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 2.1481 (2.3046)  loss: 2.4484 (2.3046)  time: 0.1811\n",
      "Test: Total time: 0:02:04 (0.2204 s / it)\n",
      "Averaged stats: loss_labels: 2.1481 (2.3046)  loss: 2.4484 (2.3046)\n",
      "acc: 0.5413365960121155\n",
      "top 1 and top 5 accuracies {'top1': 0.5413366199523625, 'top5': 0.7281476762864898, 'loss': tensor(0.0180, device='cuda:0')}\n",
      "Epoch: [2]  [   0/5008]  eta: 1 day, 13:36:37  lr: 0.004000  loss_labels: 1.6525 (1.6525)  loss: 1.6525 (1.6525)  time: 27.0362\n",
      "Epoch: [2]  [ 100/5008]  eta: 0:53:47  lr: 0.004000  loss_labels: 1.5983 (1.6129)  loss: 1.5110 (1.6129)  time: 0.3823\n",
      "Epoch: [2]  [ 200/5008]  eta: 0:42:21  lr: 0.004000  loss_labels: 1.6132 (1.6093)  loss: 1.6632 (1.6093)  time: 0.3982\n",
      "Epoch: [2]  [ 300/5008]  eta: 0:37:59  lr: 0.004000  loss_labels: 1.5955 (1.6035)  loss: 1.6849 (1.6035)  time: 0.3860\n",
      "Epoch: [2]  [ 400/5008]  eta: 0:35:35  lr: 0.004000  loss_labels: 1.5496 (1.5920)  loss: 1.5523 (1.5920)  time: 0.3951\n",
      "Epoch: [2]  [ 500/5008]  eta: 0:33:52  lr: 0.004000  loss_labels: 1.5616 (1.5868)  loss: 1.5453 (1.5868)  time: 0.3947\n",
      "Epoch: [2]  [ 600/5008]  eta: 0:32:28  lr: 0.004000  loss_labels: 1.4941 (1.5727)  loss: 1.4674 (1.5727)  time: 0.3852\n",
      "Epoch: [2]  [ 700/5008]  eta: 0:31:20  lr: 0.004000  loss_labels: 1.5176 (1.5622)  loss: 1.5339 (1.5622)  time: 0.4071\n",
      "Epoch: [2]  [ 800/5008]  eta: 0:30:15  lr: 0.004000  loss_labels: 1.4711 (1.5540)  loss: 1.4976 (1.5540)  time: 0.4062\n",
      "Epoch: [2]  [ 900/5008]  eta: 0:29:15  lr: 0.004000  loss_labels: 1.4813 (1.5467)  loss: 1.4344 (1.5467)  time: 0.3972\n",
      "Epoch: [2]  [1000/5008]  eta: 0:28:23  lr: 0.004000  loss_labels: 1.4594 (1.5379)  loss: 1.4561 (1.5379)  time: 0.3971\n",
      "Epoch: [2]  [1100/5008]  eta: 0:27:30  lr: 0.004000  loss_labels: 1.4373 (1.5288)  loss: 1.4042 (1.5288)  time: 0.4007\n",
      "Epoch: [2]  [1200/5008]  eta: 0:26:39  lr: 0.004000  loss_labels: 1.4446 (1.5212)  loss: 1.4647 (1.5212)  time: 0.3949\n",
      "Epoch: [2]  [1300/5008]  eta: 0:25:50  lr: 0.004000  loss_labels: 1.4115 (1.5121)  loss: 1.4233 (1.5121)  time: 0.3927\n",
      "Epoch: [2]  [1400/5008]  eta: 0:25:03  lr: 0.004000  loss_labels: 1.4072 (1.5042)  loss: 1.4245 (1.5042)  time: 0.3891\n",
      "Epoch: [2]  [1500/5008]  eta: 0:24:16  lr: 0.004000  loss_labels: 1.3764 (1.4977)  loss: 1.3139 (1.4977)  time: 0.3939\n",
      "Epoch: [2]  [1600/5008]  eta: 0:23:31  lr: 0.004000  loss_labels: 1.3778 (1.4903)  loss: 1.3512 (1.4903)  time: 0.4009\n",
      "Epoch: [2]  [1700/5008]  eta: 0:22:46  lr: 0.004000  loss_labels: 1.3840 (1.4840)  loss: 1.3672 (1.4840)  time: 0.4042\n",
      "Epoch: [2]  [1800/5008]  eta: 0:22:02  lr: 0.004000  loss_labels: 1.3256 (1.4756)  loss: 1.3060 (1.4756)  time: 0.3921\n",
      "Epoch: [2]  [1900/5008]  eta: 0:21:19  lr: 0.004000  loss_labels: 1.3271 (1.4673)  loss: 1.3006 (1.4673)  time: 0.4005\n",
      "Epoch: [2]  [2000/5008]  eta: 0:20:35  lr: 0.004000  loss_labels: 1.3274 (1.4601)  loss: 1.2253 (1.4601)  time: 0.4065\n",
      "Epoch: [2]  [2100/5008]  eta: 0:19:53  lr: 0.004000  loss_labels: 1.2818 (1.4517)  loss: 1.3166 (1.4517)  time: 0.4144\n",
      "Epoch: [2]  [2200/5008]  eta: 0:19:11  lr: 0.004000  loss_labels: 1.2649 (1.4438)  loss: 1.2632 (1.4438)  time: 0.4090\n",
      "Epoch: [2]  [2300/5008]  eta: 0:18:29  lr: 0.004000  loss_labels: 1.2942 (1.4375)  loss: 1.2730 (1.4375)  time: 0.4016\n",
      "Epoch: [2]  [2400/5008]  eta: 0:17:47  lr: 0.004000  loss_labels: 1.2666 (1.4311)  loss: 1.2666 (1.4311)  time: 0.4023\n",
      "Epoch: [2]  [2500/5008]  eta: 0:17:05  lr: 0.004000  loss_labels: 1.2647 (1.4244)  loss: 1.2470 (1.4244)  time: 0.4035\n",
      "Epoch: [2]  [2600/5008]  eta: 0:16:23  lr: 0.004000  loss_labels: 1.2506 (1.4182)  loss: 1.1813 (1.4182)  time: 0.3925\n",
      "Epoch: [2]  [2700/5008]  eta: 0:15:42  lr: 0.004000  loss_labels: 1.2420 (1.4115)  loss: 1.2016 (1.4115)  time: 0.3948\n",
      "Epoch: [2]  [2800/5008]  eta: 0:15:00  lr: 0.004000  loss_labels: 1.2026 (1.4045)  loss: 1.1852 (1.4045)  time: 0.4112\n",
      "Epoch: [2]  [2900/5008]  eta: 0:14:19  lr: 0.004000  loss_labels: 1.2101 (1.3978)  loss: 1.2083 (1.3978)  time: 0.4031\n",
      "Epoch: [2]  [3000/5008]  eta: 0:13:37  lr: 0.004000  loss_labels: 1.1909 (1.3916)  loss: 1.1574 (1.3916)  time: 0.3961\n",
      "Epoch: [2]  [3100/5008]  eta: 0:12:49  lr: 0.004000  loss_labels: 1.2095 (1.3855)  loss: 1.1264 (1.3855)  time: 0.2421\n",
      "Epoch: [2]  [3200/5008]  eta: 0:12:01  lr: 0.004000  loss_labels: 1.1689 (1.3790)  loss: 1.2014 (1.3790)  time: 0.2442\n",
      "Epoch: [2]  [3300/5008]  eta: 0:11:13  lr: 0.004000  loss_labels: 1.1754 (1.3732)  loss: 1.1376 (1.3732)  time: 0.2419\n",
      "Epoch: [2]  [3400/5008]  eta: 0:10:27  lr: 0.004000  loss_labels: 1.1596 (1.3667)  loss: 1.1822 (1.3667)  time: 0.2398\n",
      "Epoch: [2]  [3500/5008]  eta: 0:09:41  lr: 0.004000  loss_labels: 1.1710 (1.3608)  loss: 1.1914 (1.3608)  time: 0.2408\n",
      "Epoch: [2]  [3600/5008]  eta: 0:08:57  lr: 0.004000  loss_labels: 1.1244 (1.3547)  loss: 1.0705 (1.3547)  time: 0.2404\n",
      "Epoch: [2]  [3700/5008]  eta: 0:08:15  lr: 0.004000  loss_labels: 1.1115 (1.3486)  loss: 1.1276 (1.3486)  time: 0.3559\n",
      "Epoch: [2]  [3800/5008]  eta: 0:07:35  lr: 0.004000  loss_labels: 1.1297 (1.3427)  loss: 1.0761 (1.3427)  time: 0.2409\n",
      "Epoch: [2]  [3900/5008]  eta: 0:06:54  lr: 0.004000  loss_labels: 1.1287 (1.3373)  loss: 1.0941 (1.3373)  time: 0.2409\n",
      "Epoch: [2]  [4000/5008]  eta: 0:06:13  lr: 0.004000  loss_labels: 1.1051 (1.3317)  loss: 1.1051 (1.3317)  time: 0.2404\n",
      "Epoch: [2]  [4100/5008]  eta: 0:05:33  lr: 0.004000  loss_labels: 1.0956 (1.3260)  loss: 1.0806 (1.3260)  time: 0.2408\n",
      "Epoch: [2]  [4200/5008]  eta: 0:04:54  lr: 0.004000  loss_labels: 1.0849 (1.3201)  loss: 1.1183 (1.3201)  time: 0.2411\n",
      "Epoch: [2]  [4300/5008]  eta: 0:04:15  lr: 0.004000  loss_labels: 1.0572 (1.3141)  loss: 1.0300 (1.3141)  time: 0.2412\n",
      "Epoch: [2]  [4400/5008]  eta: 0:03:38  lr: 0.004000  loss_labels: 1.0773 (1.3091)  loss: 1.0992 (1.3091)  time: 0.2413\n",
      "Epoch: [2]  [4500/5008]  eta: 0:03:00  lr: 0.004000  loss_labels: 1.0640 (1.3034)  loss: 1.0836 (1.3034)  time: 0.2405\n",
      "Epoch: [2]  [4600/5008]  eta: 0:02:24  lr: 0.004000  loss_labels: 1.0588 (1.2979)  loss: 1.0826 (1.2979)  time: 0.2431\n",
      "Epoch: [2]  [4700/5008]  eta: 0:01:48  lr: 0.004000  loss_labels: 1.0293 (1.2923)  loss: 1.0784 (1.2923)  time: 0.2417\n",
      "Epoch: [2]  [4800/5008]  eta: 0:01:12  lr: 0.004000  loss_labels: 1.0283 (1.2869)  loss: 1.0999 (1.2869)  time: 0.2423\n",
      "Epoch: [2]  [4900/5008]  eta: 0:00:37  lr: 0.004000  loss_labels: 1.0169 (1.2814)  loss: 0.9942 (1.2814)  time: 0.2404\n",
      "Epoch: [2]  [5000/5008]  eta: 0:00:02  lr: 0.004000  loss_labels: 1.0237 (1.2763)  loss: 1.0837 (1.2763)  time: 0.2414\n",
      "Epoch: [2]  [5007/5008]  eta: 0:00:00  lr: 0.004000  loss_labels: 1.0310 (1.2760)  loss: 1.0565 (1.2760)  time: 0.2408\n",
      "Epoch: [2] Total time: 0:28:46 (0.3447 s / it)\n",
      "Averaged stats: lr: 0.004000  loss_labels: 1.0310 (1.2760)  loss: 1.0565 (1.2760)\n",
      "Test:  [  0/565]  eta: 2:35:45  loss_labels: 1.6410 (1.6410)  loss: 1.6410 (1.6410)  time: 16.5402\n",
      "Test:  [100/565]  eta: 0:02:13  loss_labels: 1.8092 (1.9128)  loss: 1.9369 (1.9128)  time: 0.1215\n",
      "Test:  [200/565]  eta: 0:01:14  loss_labels: 1.6705 (1.8610)  loss: 1.9853 (1.8610)  time: 0.1209\n",
      "Test:  [300/565]  eta: 0:00:47  loss_labels: 1.5640 (1.8040)  loss: 1.4980 (1.8040)  time: 0.1264\n",
      "Test:  [400/565]  eta: 0:00:27  loss_labels: 1.9473 (1.8455)  loss: 1.8392 (1.8455)  time: 0.1222\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.6231 (1.8155)  loss: 1.9319 (1.8155)  time: 0.1220\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 1.6565 (1.7939)  loss: 1.8225 (1.7939)  time: 0.1201\n",
      "Test: Total time: 0:01:26 (0.1539 s / it)\n",
      "Averaged stats: loss_labels: 1.6565 (1.7939)  loss: 1.8225 (1.7939)\n",
      "acc: 0.6365562677383423\n",
      "top 1 and top 5 accuracies {'top1': 0.6365562510386086, 'top5': 0.8003101977510663, 'loss': tensor(0.0140, device='cuda:0')}\n",
      "Epoch: [3]  [   0/5008]  eta: 1 day, 6:38:08  lr: 0.002000  loss_labels: 1.1474 (1.1474)  loss: 1.1474 (1.1474)  time: 22.0225\n",
      "Epoch: [3]  [ 100/5008]  eta: 0:37:22  lr: 0.002000  loss_labels: 0.9183 (0.9189)  loss: 0.8796 (0.9189)  time: 0.2396\n",
      "Epoch: [3]  [ 200/5008]  eta: 0:28:00  lr: 0.002000  loss_labels: 0.8727 (0.8980)  loss: 0.8672 (0.8980)  time: 0.2415\n",
      "Epoch: [3]  [ 300/5008]  eta: 0:24:37  lr: 0.002000  loss_labels: 0.8687 (0.8883)  loss: 0.9045 (0.8883)  time: 0.2418\n",
      "Epoch: [3]  [ 400/5008]  eta: 0:22:42  lr: 0.002000  loss_labels: 0.8391 (0.8779)  loss: 0.8554 (0.8779)  time: 0.2416\n",
      "Epoch: [3]  [ 500/5008]  eta: 0:21:24  lr: 0.002000  loss_labels: 0.8378 (0.8698)  loss: 0.7629 (0.8698)  time: 0.2417\n",
      "Epoch: [3]  [ 600/5008]  eta: 0:20:23  lr: 0.002000  loss_labels: 0.8112 (0.8607)  loss: 0.7629 (0.8607)  time: 0.2406\n",
      "Epoch: [3]  [ 700/5008]  eta: 0:19:45  lr: 0.002000  loss_labels: 0.8229 (0.8551)  loss: 0.8570 (0.8551)  time: 0.3443\n",
      "Epoch: [3]  [ 800/5008]  eta: 0:19:00  lr: 0.002000  loss_labels: 0.8027 (0.8497)  loss: 0.8167 (0.8497)  time: 0.2398\n",
      "Epoch: [3]  [ 900/5008]  eta: 0:18:30  lr: 0.002000  loss_labels: 0.8047 (0.8449)  loss: 0.7955 (0.8449)  time: 0.3587\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/BLT_models/main.py\", line 281, in <module>\n",
      "    mp.spawn(main, args=(args.world_size, args), nprocs=args.world_size)\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 239, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 197, in start_processes\n",
      "    while not context.join():\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 109, in join\n",
      "    ready = multiprocessing.connection.wait(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/multiprocessing/connection.py\", line 930, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --distributed 1 --save_model 0 --dataset 'vggface2' --num_layers 6 --model 'blt_b3lt3' --epochs 100 --lr .004 --lr_drop 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --save_model 0 --dataset 'imagenet' --wandb_p 'imagenet' --wandb_r 'blt_bl'  --model 'blt_bl' --epochs 100 --lr .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3527220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "script = f'''#!/bin/sh\n",
    "#\n",
    "#\n",
    "#SBATCH --account=nklab\n",
    "#SBATCH --job-name=b_small_vggface2  # The job name.\n",
    "#SBATCH --gres=gpu:2\n",
    "#SBATCH --nodelist=ax02\n",
    "#SBATCH --cpus-per-task=12\n",
    "\n",
    "ml load anaconda3-2019.03\n",
    "\n",
    "cd /engram/nklab/hossein/recurrent_models/BLT_models/\n",
    "\n",
    "conda activate py39\n",
    "\n",
    "python main.py --save_model 1 --dataset 'vggface2' --wandb_p 'vggface2' --wandb_r 'b_small_0.0005'  --model 'blt_b' --num_layers 6 --epochs 100 --lr .0005 --lr_drop 10 --port '12391' --run 22\n",
    "\n",
    "'''\n",
    "\n",
    "bash_script_path = \"/engram/nklab/hossein/batch_scripts/imagenet_nb.sh\"\n",
    "os.chdir('/engram/nklab/hossein/batch_scripts/')\n",
    "\n",
    "with open(bash_script_path, \"w+\") as bash_script_file:\n",
    "    bash_script_file.write(script)\n",
    "\n",
    "subprocess.run(['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py39]",
   "language": "python",
   "name": "conda-env-.conda-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
