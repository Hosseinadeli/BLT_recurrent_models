{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/BLT_models\n"
     ]
    }
   ],
   "source": [
    "data_path = '/share/data/imagenet-pytorch'\n",
    "\n",
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/BLT_models/')\n",
    "!pwd\n",
    "\n",
    "#--wandb_p 'vggface2' --wandb_r 'blt_bl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/BLT_models/models/blt.py:230: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if conn_input is not 0:\n",
      "| distributed init (rank 0): env://\n",
      "| distributed init (rank 1): env://\n",
      "cuda\n",
      "100%|██████████████████████████████████████| 3890/3890 [00:06<00:00, 618.47it/s]\n",
      "100%|██████████████████████████████████████| 3890/3890 [00:06<00:00, 618.47it/s]\n",
      "100%|█████████████████████████████████████| 3890/3890 [00:02<00:00, 1602.47it/s]\n",
      "100%|█████████████████████████████████████| 3890/3890 [00:02<00:00, 1577.15it/s]\n",
      "56 28 2\n",
      "28 14 2\n",
      "14 7 2\n",
      "7 1 7\n",
      "1 1 1\n",
      "Number of model parameters: 12473650\n",
      "blt(\n",
      "  (conv_input): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (maxpool_input): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (non_lin_0): ReLU(inplace=True)\n",
      "  (norm_0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (output_0): Identity()\n",
      "  (conv_0_1): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (non_lin_1): ReLU(inplace=True)\n",
      "  (norm_1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (output_1): Identity()\n",
      "  (conv_1_2): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (non_lin_2): ReLU(inplace=True)\n",
      "  (norm_2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (output_2): Identity()\n",
      "  (conv_2_3): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (non_lin_3): ReLU(inplace=True)\n",
      "  (norm_3): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (output_3): Identity()\n",
      "  (conv_3_4): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (Unsqueeze): Unsqueeze()\n",
      "  )\n",
      "  (non_lin_4): ReLU(inplace=True)\n",
      "  (norm_4): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "  (output_4): Identity()\n",
      "  (conv_4_5): Sequential(\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (Unsqueeze): Unsqueeze()\n",
      "  )\n",
      "  (non_lin_5): ReLU(inplace=True)\n",
      "  (norm_5): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (output_5): Identity()\n",
      "  (read_out): Sequential(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=512, out_features=3890, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['conv_input.weight', 'conv_input.bias', 'norm_0.weight', 'norm_0.bias', 'conv_0_1.conv.weight', 'conv_0_1.conv.bias', 'norm_1.weight', 'norm_1.bias', 'conv_1_2.conv.weight', 'conv_1_2.conv.bias', 'norm_2.weight', 'norm_2.bias', 'conv_2_3.conv.weight', 'conv_2_3.conv.bias', 'norm_3.weight', 'norm_3.bias', 'conv_3_4.linear.weight', 'conv_3_4.linear.bias', 'norm_4.weight', 'norm_4.bias', 'conv_4_5.linear.weight', 'conv_4_5.linear.bias', 'norm_5.weight', 'norm_5.bias', 'read_out.linear.weight', 'read_out.linear.bias']\n",
      "Start training\n",
      "[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch: [0]  [    0/20033]  eta: 6 days, 16:37:29  lr: 0.000500  loss_labels: 8.3049 (8.3049)  loss: 8.3049 (8.3049)  time: 28.8648\n",
      "Epoch: [0]  [  100/20033]  eta: 1:44:36  lr: 0.000500  loss_labels: 8.4667 (8.4666)  loss: 8.5043 (8.4666)  time: 0.0274\n",
      "Epoch: [0]  [  200/20033]  eta: 0:57:26  lr: 0.000500  loss_labels: 8.4730 (8.4641)  loss: 8.4216 (8.4641)  time: 0.0510\n",
      "Epoch: [0]  [  300/20033]  eta: 0:41:02  lr: 0.000500  loss_labels: 8.3695 (8.4314)  loss: 8.3384 (8.4314)  time: 0.0240\n",
      "Epoch: [0]  [  400/20033]  eta: 0:32:47  lr: 0.000500  loss_labels: 8.2867 (8.3981)  loss: 8.2714 (8.3981)  time: 0.0248\n",
      "Epoch: [0]  [  500/20033]  eta: 0:28:17  lr: 0.000500  loss_labels: 8.2744 (8.3732)  loss: 8.2597 (8.3732)  time: 0.0266\n",
      "Epoch: [0]  [  600/20033]  eta: 0:25:16  lr: 0.000500  loss_labels: 8.2532 (8.3536)  loss: 8.2350 (8.3536)  time: 0.0257\n",
      "Epoch: [0]  [  700/20033]  eta: 0:22:45  lr: 0.000500  loss_labels: 8.2603 (8.3398)  loss: 8.2507 (8.3398)  time: 0.0274\n",
      "Epoch: [0]  [  800/20033]  eta: 0:20:49  lr: 0.000500  loss_labels: 8.2508 (8.3290)  loss: 8.2647 (8.3290)  time: 0.0242\n",
      "Epoch: [0]  [  900/20033]  eta: 0:19:41  lr: 0.000500  loss_labels: 8.2474 (8.3201)  loss: 8.2403 (8.3201)  time: 0.0253\n",
      "Epoch: [0]  [ 1000/20033]  eta: 0:18:51  lr: 0.000500  loss_labels: 8.2493 (8.3133)  loss: 8.2321 (8.3133)  time: 0.0902\n",
      "Epoch: [0]  [ 1100/20033]  eta: 0:17:47  lr: 0.000500  loss_labels: 8.2462 (8.3074)  loss: 8.2441 (8.3074)  time: 0.0249\n",
      "Epoch: [0]  [ 1200/20033]  eta: 0:16:53  lr: 0.000500  loss_labels: 8.2454 (8.3022)  loss: 8.2383 (8.3022)  time: 0.0279\n",
      "Epoch: [0]  [ 1300/20033]  eta: 0:16:27  lr: 0.000500  loss_labels: 8.2452 (8.2979)  loss: 8.2452 (8.2979)  time: 0.0248\n",
      "Epoch: [0]  [ 1400/20033]  eta: 0:15:44  lr: 0.000500  loss_labels: 8.2309 (8.2934)  loss: 8.2294 (8.2934)  time: 0.0262\n",
      "Epoch: [0]  [ 1500/20033]  eta: 0:15:14  lr: 0.000500  loss_labels: 8.2174 (8.2883)  loss: 8.2251 (8.2883)  time: 0.0275\n",
      "Epoch: [0]  [ 1600/20033]  eta: 0:14:50  lr: 0.000500  loss_labels: 8.1866 (8.2821)  loss: 8.1572 (8.2821)  time: 0.0515\n",
      "Epoch: [0]  [ 1700/20033]  eta: 0:14:20  lr: 0.000500  loss_labels: 8.1584 (8.2746)  loss: 8.1612 (8.2746)  time: 0.0248\n",
      "Epoch: [0]  [ 1800/20033]  eta: 0:13:53  lr: 0.000500  loss_labels: 8.1204 (8.2660)  loss: 8.1151 (8.2660)  time: 0.0266\n",
      "Epoch: [0]  [ 1900/20033]  eta: 0:13:35  lr: 0.000500  loss_labels: 8.0871 (8.2567)  loss: 8.0805 (8.2567)  time: 0.0254\n",
      "Epoch: [0]  [ 2000/20033]  eta: 0:13:13  lr: 0.000500  loss_labels: 8.0421 (8.2460)  loss: 8.0582 (8.2460)  time: 0.0248\n",
      "Epoch: [0]  [ 2100/20033]  eta: 0:12:59  lr: 0.000500  loss_labels: 8.0006 (8.2342)  loss: 8.0006 (8.2342)  time: 0.0229\n",
      "Epoch: [0]  [ 2200/20033]  eta: 0:12:40  lr: 0.000500  loss_labels: 7.9832 (8.2226)  loss: 7.9678 (8.2226)  time: 0.0253\n",
      "Epoch: [0]  [ 2300/20033]  eta: 0:12:24  lr: 0.000500  loss_labels: 7.9277 (8.2102)  loss: 7.9277 (8.2102)  time: 0.0344\n",
      "Epoch: [0]  [ 2400/20033]  eta: 0:12:07  lr: 0.000500  loss_labels: 7.9008 (8.1976)  loss: 7.9224 (8.1976)  time: 0.0239\n",
      "Epoch: [0]  [ 2500/20033]  eta: 0:12:00  lr: 0.000500  loss_labels: 7.8587 (8.1842)  loss: 7.8423 (8.1842)  time: 0.0234\n",
      "Epoch: [0]  [ 2600/20033]  eta: 0:11:46  lr: 0.000500  loss_labels: 7.8474 (8.1713)  loss: 7.8339 (8.1713)  time: 0.0252\n",
      "Epoch: [0]  [ 2700/20033]  eta: 0:11:36  lr: 0.000500  loss_labels: 7.8065 (8.1573)  loss: 7.8040 (8.1573)  time: 0.0263\n",
      "Epoch: [0]  [ 2800/20033]  eta: 0:11:23  lr: 0.000500  loss_labels: 7.7791 (8.1437)  loss: 7.7526 (8.1437)  time: 0.0226\n",
      "Epoch: [0]  [ 2900/20033]  eta: 0:11:10  lr: 0.000500  loss_labels: 7.7342 (8.1295)  loss: 7.7058 (8.1295)  time: 0.0246\n",
      "Epoch: [0]  [ 3000/20033]  eta: 0:11:03  lr: 0.000500  loss_labels: 7.7092 (8.1154)  loss: 7.7290 (8.1154)  time: 0.0256\n",
      "Epoch: [0]  [ 3100/20033]  eta: 0:10:52  lr: 0.000500  loss_labels: 7.6494 (8.1007)  loss: 7.6516 (8.1007)  time: 0.0265\n",
      "Epoch: [0]  [ 3200/20033]  eta: 0:10:47  lr: 0.000500  loss_labels: 7.6097 (8.0858)  loss: 7.5970 (8.0858)  time: 0.0252\n",
      "Epoch: [0]  [ 3300/20033]  eta: 0:10:37  lr: 0.000500  loss_labels: 7.5819 (8.0707)  loss: 7.5573 (8.0707)  time: 0.0272\n",
      "Epoch: [0]  [ 3400/20033]  eta: 0:10:29  lr: 0.000500  loss_labels: 7.5837 (8.0558)  loss: 7.5415 (8.0558)  time: 0.0264\n",
      "Epoch: [0]  [ 3500/20033]  eta: 0:10:19  lr: 0.000500  loss_labels: 7.4887 (8.0398)  loss: 7.4560 (8.0398)  time: 0.0273\n",
      "Epoch: [0]  [ 3600/20033]  eta: 0:10:16  lr: 0.000500  loss_labels: 7.4466 (8.0233)  loss: 7.4511 (8.0233)  time: 0.0262\n",
      "Epoch: [0]  [ 3700/20033]  eta: 0:10:09  lr: 0.000500  loss_labels: 7.3798 (8.0060)  loss: 7.3949 (8.0060)  time: 0.0485\n",
      "Epoch: [0]  [ 3800/20033]  eta: 0:10:00  lr: 0.000500  loss_labels: 7.3467 (7.9885)  loss: 7.2727 (7.9885)  time: 0.0277\n",
      "Epoch: [0]  [ 3900/20033]  eta: 0:09:53  lr: 0.000500  loss_labels: 7.2913 (7.9710)  loss: 7.2873 (7.9710)  time: 0.0258\n",
      "Epoch: [0]  [ 4000/20033]  eta: 0:09:45  lr: 0.000500  loss_labels: 7.2292 (7.9529)  loss: 7.1786 (7.9529)  time: 0.0254\n",
      "Epoch: [0]  [ 4100/20033]  eta: 0:09:39  lr: 0.000500  loss_labels: 7.1857 (7.9345)  loss: 7.1375 (7.9345)  time: 0.0261\n",
      "Epoch: [0]  [ 4200/20033]  eta: 0:09:31  lr: 0.000500  loss_labels: 7.1356 (7.9158)  loss: 7.1148 (7.9158)  time: 0.0265\n",
      "Epoch: [0]  [ 4300/20033]  eta: 0:09:26  lr: 0.000500  loss_labels: 7.1513 (7.8978)  loss: 7.1681 (7.8978)  time: 0.0262\n",
      "Epoch: [0]  [ 4400/20033]  eta: 0:09:18  lr: 0.000500  loss_labels: 7.0878 (7.8794)  loss: 7.0915 (7.8794)  time: 0.0247\n",
      "Epoch: [0]  [ 4500/20033]  eta: 0:09:13  lr: 0.000500  loss_labels: 7.0544 (7.8611)  loss: 7.0847 (7.8611)  time: 0.0253\n",
      "Epoch: [0]  [ 4600/20033]  eta: 0:09:08  lr: 0.000500  loss_labels: 7.0310 (7.8431)  loss: 7.0211 (7.8431)  time: 0.0333\n",
      "Epoch: [0]  [ 4700/20033]  eta: 0:09:06  lr: 0.000500  loss_labels: 6.9912 (7.8251)  loss: 6.8845 (7.8251)  time: 0.0359\n",
      "Epoch: [0]  [ 4800/20033]  eta: 0:09:00  lr: 0.000500  loss_labels: 6.9683 (7.8073)  loss: 6.9900 (7.8073)  time: 0.0298\n",
      "Epoch: [0]  [ 4900/20033]  eta: 0:08:57  lr: 0.000500  loss_labels: 6.9332 (7.7899)  loss: 6.9323 (7.7899)  time: 0.0242\n",
      "Epoch: [0]  [ 5000/20033]  eta: 0:08:51  lr: 0.000500  loss_labels: 6.9260 (7.7727)  loss: 6.9003 (7.7727)  time: 0.0263\n",
      "Epoch: [0]  [ 5100/20033]  eta: 0:08:45  lr: 0.000500  loss_labels: 6.9014 (7.7555)  loss: 6.8610 (7.7555)  time: 0.0296\n",
      "Epoch: [0]  [ 5200/20033]  eta: 0:08:40  lr: 0.000500  loss_labels: 6.8860 (7.7387)  loss: 6.8950 (7.7387)  time: 0.0271\n",
      "Epoch: [0]  [ 5300/20033]  eta: 0:08:34  lr: 0.000500  loss_labels: 6.8261 (7.7216)  loss: 6.8049 (7.7216)  time: 0.0274\n",
      "Epoch: [0]  [ 5400/20033]  eta: 0:08:31  lr: 0.000500  loss_labels: 6.7960 (7.7049)  loss: 6.7955 (7.7049)  time: 0.0299\n",
      "Epoch: [0]  [ 5500/20033]  eta: 0:08:28  lr: 0.000500  loss_labels: 6.7756 (7.6879)  loss: 6.6977 (7.6879)  time: 0.0297\n",
      "Epoch: [0]  [ 5600/20033]  eta: 0:08:23  lr: 0.000500  loss_labels: 6.7617 (7.6712)  loss: 6.6788 (7.6712)  time: 0.0268\n",
      "Epoch: [0]  [ 5700/20033]  eta: 0:08:18  lr: 0.000500  loss_labels: 6.7257 (7.6544)  loss: 6.7441 (7.6544)  time: 0.0288\n",
      "Epoch: [0]  [ 5800/20033]  eta: 0:08:14  lr: 0.000500  loss_labels: 6.6955 (7.6381)  loss: 6.7121 (7.6381)  time: 0.0387\n",
      "Epoch: [0]  [ 5900/20033]  eta: 0:08:11  lr: 0.000500  loss_labels: 6.6657 (7.6217)  loss: 6.7210 (7.6217)  time: 0.0293\n",
      "Epoch: [0]  [ 6000/20033]  eta: 0:08:07  lr: 0.000500  loss_labels: 6.5987 (7.6049)  loss: 6.5776 (7.6049)  time: 0.0267\n",
      "Epoch: [0]  [ 6100/20033]  eta: 0:08:03  lr: 0.000500  loss_labels: 6.5640 (7.5879)  loss: 6.5590 (7.5879)  time: 0.0261\n",
      "Epoch: [0]  [ 6200/20033]  eta: 0:08:01  lr: 0.000500  loss_labels: 6.5684 (7.5718)  loss: 6.5614 (7.5718)  time: 0.0280\n",
      "Epoch: [0]  [ 6300/20033]  eta: 0:07:58  lr: 0.000500  loss_labels: 6.5395 (7.5552)  loss: 6.4352 (7.5552)  time: 0.0829\n",
      "Epoch: [0]  [ 6400/20033]  eta: 0:07:57  lr: 0.000500  loss_labels: 6.5172 (7.5390)  loss: 6.4268 (7.5390)  time: 0.0306\n",
      "Epoch: [0]  [ 6500/20033]  eta: 0:07:53  lr: 0.000500  loss_labels: 6.4755 (7.5227)  loss: 6.4784 (7.5227)  time: 0.0318\n",
      "Epoch: [0]  [ 6600/20033]  eta: 0:07:51  lr: 0.000500  loss_labels: 6.4530 (7.5065)  loss: 6.3682 (7.5065)  time: 0.0575\n",
      "Epoch: [0]  [ 6700/20033]  eta: 0:07:46  lr: 0.000500  loss_labels: 6.4340 (7.4906)  loss: 6.4459 (7.4906)  time: 0.0266\n",
      "Epoch: [0]  [ 6800/20033]  eta: 0:07:41  lr: 0.000500  loss_labels: 6.4246 (7.4748)  loss: 6.4328 (7.4748)  time: 0.0291\n",
      "Epoch: [0]  [ 6900/20033]  eta: 0:07:38  lr: 0.000500  loss_labels: 6.3317 (7.4585)  loss: 6.3410 (7.4585)  time: 0.0297\n",
      "Epoch: [0]  [ 7000/20033]  eta: 0:07:35  lr: 0.000500  loss_labels: 6.3273 (7.4423)  loss: 6.2894 (7.4423)  time: 0.0353\n",
      "Epoch: [0]  [ 7100/20033]  eta: 0:07:31  lr: 0.000500  loss_labels: 6.2597 (7.4260)  loss: 6.2309 (7.4260)  time: 0.0263\n",
      "Epoch: [0]  [ 7200/20033]  eta: 0:07:27  lr: 0.000500  loss_labels: 6.3084 (7.4101)  loss: 6.2781 (7.4101)  time: 0.0265\n",
      "Epoch: [0]  [ 7300/20033]  eta: 0:07:23  lr: 0.000500  loss_labels: 6.2618 (7.3945)  loss: 6.3151 (7.3945)  time: 0.0383\n",
      "Epoch: [0]  [ 7400/20033]  eta: 0:07:19  lr: 0.000500  loss_labels: 6.1971 (7.3787)  loss: 6.1561 (7.3787)  time: 0.0306\n",
      "Epoch: [0]  [ 7500/20033]  eta: 0:07:16  lr: 0.000500  loss_labels: 6.2105 (7.3634)  loss: 6.2671 (7.3634)  time: 0.0255\n",
      "Epoch: [0]  [ 7600/20033]  eta: 0:07:12  lr: 0.000500  loss_labels: 6.1970 (7.3480)  loss: 6.2246 (7.3480)  time: 0.0433\n",
      "Epoch: [0]  [ 7700/20033]  eta: 0:07:08  lr: 0.000500  loss_labels: 6.1218 (7.3323)  loss: 6.0332 (7.3323)  time: 0.0271\n",
      "Epoch: [0]  [ 7800/20033]  eta: 0:07:04  lr: 0.000500  loss_labels: 6.1024 (7.3167)  loss: 6.0574 (7.3167)  time: 0.0226\n",
      "Epoch: [0]  [ 7900/20033]  eta: 0:07:01  lr: 0.000500  loss_labels: 6.0596 (7.3012)  loss: 6.0242 (7.3012)  time: 0.0396\n",
      "Epoch: [0]  [ 8000/20033]  eta: 0:06:56  lr: 0.000500  loss_labels: 6.0397 (7.2853)  loss: 6.0332 (7.2853)  time: 0.0261\n",
      "Epoch: [0]  [ 8100/20033]  eta: 0:06:53  lr: 0.000500  loss_labels: 6.0538 (7.2701)  loss: 6.0219 (7.2701)  time: 0.0297\n",
      "Epoch: [0]  [ 8200/20033]  eta: 0:06:49  lr: 0.000500  loss_labels: 6.0398 (7.2549)  loss: 5.9075 (7.2549)  time: 0.0256\n",
      "Epoch: [0]  [ 8300/20033]  eta: 0:06:45  lr: 0.000500  loss_labels: 5.9924 (7.2399)  loss: 5.9338 (7.2399)  time: 0.0269\n",
      "Epoch: [0]  [ 8400/20033]  eta: 0:06:40  lr: 0.000500  loss_labels: 5.9345 (7.2244)  loss: 5.9828 (7.2244)  time: 0.0304\n",
      "Epoch: [0]  [ 8500/20033]  eta: 0:06:35  lr: 0.000500  loss_labels: 5.9132 (7.2089)  loss: 5.9116 (7.2089)  time: 0.0224\n",
      "Epoch: [0]  [ 8600/20033]  eta: 0:06:32  lr: 0.000500  loss_labels: 5.8797 (7.1935)  loss: 5.8797 (7.1935)  time: 0.0245\n",
      "Epoch: [0]  [ 8700/20033]  eta: 0:06:27  lr: 0.000500  loss_labels: 5.8365 (7.1779)  loss: 5.7568 (7.1779)  time: 0.0272\n",
      "Epoch: [0]  [ 8800/20033]  eta: 0:06:25  lr: 0.000500  loss_labels: 5.8599 (7.1628)  loss: 5.8306 (7.1628)  time: 0.0308\n",
      "Epoch: [0]  [ 8900/20033]  eta: 0:06:21  lr: 0.000500  loss_labels: 5.7541 (7.1474)  loss: 5.7382 (7.1474)  time: 0.0444\n",
      "Epoch: [0]  [ 9000/20033]  eta: 0:06:17  lr: 0.000500  loss_labels: 5.7802 (7.1327)  loss: 5.7518 (7.1327)  time: 0.0416\n",
      "Epoch: [0]  [ 9100/20033]  eta: 0:06:13  lr: 0.000500  loss_labels: 5.8202 (7.1183)  loss: 5.7398 (7.1183)  time: 0.0276\n",
      "Epoch: [0]  [ 9200/20033]  eta: 0:06:09  lr: 0.000500  loss_labels: 5.6956 (7.1032)  loss: 5.7112 (7.1032)  time: 0.0250\n",
      "Epoch: [0]  [ 9300/20033]  eta: 0:06:07  lr: 0.000500  loss_labels: 5.6676 (7.0882)  loss: 5.6275 (7.0882)  time: 0.0268\n",
      "Epoch: [0]  [ 9400/20033]  eta: 0:06:04  lr: 0.000500  loss_labels: 5.7003 (7.0734)  loss: 5.6980 (7.0734)  time: 0.0248\n",
      "Epoch: [0]  [ 9500/20033]  eta: 0:06:06  lr: 0.000500  loss_labels: 5.6418 (7.0584)  loss: 5.6109 (7.0584)  time: 0.1100\n",
      "Epoch: [0]  [ 9600/20033]  eta: 0:06:08  lr: 0.000500  loss_labels: 5.6475 (7.0433)  loss: 5.5526 (7.0433)  time: 0.0649\n",
      "Epoch: [0]  [ 9700/20033]  eta: 0:06:05  lr: 0.000500  loss_labels: 5.6037 (7.0285)  loss: 5.5482 (7.0285)  time: 0.0661\n",
      "Epoch: [0]  [ 9800/20033]  eta: 0:06:06  lr: 0.000500  loss_labels: 5.5709 (7.0138)  loss: 5.5959 (7.0138)  time: 0.0614\n",
      "Epoch: [0]  [ 9900/20033]  eta: 0:06:06  lr: 0.000500  loss_labels: 5.5356 (6.9991)  loss: 5.5573 (6.9991)  time: 0.0800\n",
      "Epoch: [0]  [10000/20033]  eta: 0:06:06  lr: 0.000500  loss_labels: 5.4761 (6.9841)  loss: 5.3692 (6.9841)  time: 0.0522\n",
      "Epoch: [0]  [10100/20033]  eta: 0:06:05  lr: 0.000500  loss_labels: 5.4784 (6.9692)  loss: 5.4927 (6.9692)  time: 0.0734\n",
      "Epoch: [0]  [10200/20033]  eta: 0:06:01  lr: 0.000500  loss_labels: 5.4592 (6.9544)  loss: 5.4235 (6.9544)  time: 0.0248\n",
      "Epoch: [0]  [10300/20033]  eta: 0:05:57  lr: 0.000500  loss_labels: 5.4515 (6.9399)  loss: 5.4388 (6.9399)  time: 0.0246\n",
      "Epoch: [0]  [10400/20033]  eta: 0:05:52  lr: 0.000500  loss_labels: 5.4377 (6.9252)  loss: 5.4677 (6.9252)  time: 0.0270\n",
      "Epoch: [0]  [10500/20033]  eta: 0:05:47  lr: 0.000500  loss_labels: 5.4377 (6.9110)  loss: 5.4416 (6.9110)  time: 0.0243\n",
      "Epoch: [0]  [10600/20033]  eta: 0:05:43  lr: 0.000500  loss_labels: 5.3073 (6.8960)  loss: 5.3768 (6.8960)  time: 0.0243\n",
      "Epoch: [0]  [10700/20033]  eta: 0:05:39  lr: 0.000500  loss_labels: 5.3478 (6.8814)  loss: 5.2712 (6.8814)  time: 0.0230\n",
      "Epoch: [0]  [10800/20033]  eta: 0:05:34  lr: 0.000500  loss_labels: 5.3180 (6.8667)  loss: 5.3308 (6.8667)  time: 0.0271\n",
      "Epoch: [0]  [10900/20033]  eta: 0:05:30  lr: 0.000500  loss_labels: 5.2772 (6.8523)  loss: 5.2341 (6.8523)  time: 0.0266\n",
      "Epoch: [0]  [11000/20033]  eta: 0:05:25  lr: 0.000500  loss_labels: 5.2590 (6.8378)  loss: 5.3386 (6.8378)  time: 0.0285\n",
      "Epoch: [0]  [11100/20033]  eta: 0:05:21  lr: 0.000500  loss_labels: 5.2013 (6.8234)  loss: 5.2032 (6.8234)  time: 0.0271\n",
      "Epoch: [0]  [11200/20033]  eta: 0:05:17  lr: 0.000500  loss_labels: 5.2533 (6.8091)  loss: 5.2043 (6.8091)  time: 0.0272\n",
      "Epoch: [0]  [11300/20033]  eta: 0:05:13  lr: 0.000500  loss_labels: 5.1965 (6.7950)  loss: 5.1266 (6.7950)  time: 0.0281\n",
      "Epoch: [0]  [11400/20033]  eta: 0:05:08  lr: 0.000500  loss_labels: 5.2123 (6.7812)  loss: 5.1891 (6.7812)  time: 0.0272\n",
      "Epoch: [0]  [11500/20033]  eta: 0:05:05  lr: 0.000500  loss_labels: 5.0885 (6.7666)  loss: 5.0046 (6.7666)  time: 0.0278\n",
      "Epoch: [0]  [11600/20033]  eta: 0:05:00  lr: 0.000500  loss_labels: 5.0564 (6.7522)  loss: 5.0550 (6.7522)  time: 0.0251\n",
      "Epoch: [0]  [11700/20033]  eta: 0:04:56  lr: 0.000500  loss_labels: 5.1342 (6.7383)  loss: 5.0570 (6.7383)  time: 0.0264\n",
      "Epoch: [0]  [11800/20033]  eta: 0:04:52  lr: 0.000500  loss_labels: 5.1174 (6.7244)  loss: 4.9387 (6.7244)  time: 0.0244\n",
      "Epoch: [0]  [11900/20033]  eta: 0:04:49  lr: 0.000500  loss_labels: 5.0226 (6.7103)  loss: 4.9796 (6.7103)  time: 0.0257\n",
      "Epoch: [0]  [12000/20033]  eta: 0:04:44  lr: 0.000500  loss_labels: 5.0943 (6.6966)  loss: 5.0769 (6.6966)  time: 0.0247\n",
      "Epoch: [0]  [12100/20033]  eta: 0:04:41  lr: 0.000500  loss_labels: 5.0252 (6.6828)  loss: 5.0014 (6.6828)  time: 0.0480\n",
      "Epoch: [0]  [12200/20033]  eta: 0:04:37  lr: 0.000500  loss_labels: 5.0101 (6.6692)  loss: 4.9322 (6.6692)  time: 0.0250\n",
      "Epoch: [0]  [12300/20033]  eta: 0:04:33  lr: 0.000500  loss_labels: 4.9965 (6.6556)  loss: 4.9612 (6.6556)  time: 0.0266\n",
      "Epoch: [0]  [12400/20033]  eta: 0:04:29  lr: 0.000500  loss_labels: 4.9040 (6.6418)  loss: 4.9457 (6.6418)  time: 0.0240\n",
      "Epoch: [0]  [12500/20033]  eta: 0:04:25  lr: 0.000500  loss_labels: 4.9506 (6.6283)  loss: 4.9203 (6.6283)  time: 0.0219\n",
      "Epoch: [0]  [12600/20033]  eta: 0:04:21  lr: 0.000500  loss_labels: 4.8651 (6.6145)  loss: 4.8187 (6.6145)  time: 0.0243\n",
      "Epoch: [0]  [12700/20033]  eta: 0:04:17  lr: 0.000500  loss_labels: 4.8397 (6.6008)  loss: 4.9835 (6.6008)  time: 0.0268\n",
      "Epoch: [0]  [12800/20033]  eta: 0:04:13  lr: 0.000500  loss_labels: 4.8238 (6.5873)  loss: 4.8035 (6.5873)  time: 0.0228\n",
      "Epoch: [0]  [12900/20033]  eta: 0:04:09  lr: 0.000500  loss_labels: 4.8444 (6.5739)  loss: 4.7250 (6.5739)  time: 0.0353\n",
      "Epoch: [0]  [13000/20033]  eta: 0:04:05  lr: 0.000500  loss_labels: 4.8660 (6.5605)  loss: 4.7475 (6.5605)  time: 0.0216\n",
      "Epoch: [0]  [13100/20033]  eta: 0:04:01  lr: 0.000500  loss_labels: 4.7731 (6.5472)  loss: 4.7513 (6.5472)  time: 0.0247\n",
      "Epoch: [0]  [13200/20033]  eta: 0:03:58  lr: 0.000500  loss_labels: 4.7789 (6.5339)  loss: 4.6637 (6.5339)  time: 0.0551\n",
      "Epoch: [0]  [13300/20033]  eta: 0:03:54  lr: 0.000500  loss_labels: 4.7353 (6.5208)  loss: 4.6865 (6.5208)  time: 0.0249\n",
      "Epoch: [0]  [13400/20033]  eta: 0:03:50  lr: 0.000500  loss_labels: 4.6683 (6.5071)  loss: 4.6356 (6.5071)  time: 0.0230\n",
      "Epoch: [0]  [13500/20033]  eta: 0:03:46  lr: 0.000500  loss_labels: 4.7109 (6.4940)  loss: 4.7352 (6.4940)  time: 0.0250\n",
      "Epoch: [0]  [13600/20033]  eta: 0:03:43  lr: 0.000500  loss_labels: 4.7669 (6.4810)  loss: 4.7290 (6.4810)  time: 0.0773\n",
      "Epoch: [0]  [13700/20033]  eta: 0:03:39  lr: 0.000500  loss_labels: 4.7242 (6.4681)  loss: 4.6988 (6.4681)  time: 0.0260\n",
      "Epoch: [0]  [13800/20033]  eta: 0:03:35  lr: 0.000500  loss_labels: 4.7046 (6.4553)  loss: 4.7622 (6.4553)  time: 0.0247\n",
      "Epoch: [0]  [13900/20033]  eta: 0:03:31  lr: 0.000500  loss_labels: 4.6741 (6.4423)  loss: 4.6887 (6.4423)  time: 0.0254\n",
      "Epoch: [0]  [14000/20033]  eta: 0:03:28  lr: 0.000500  loss_labels: 4.6729 (6.4296)  loss: 4.6936 (6.4296)  time: 0.0217\n",
      "Epoch: [0]  [14100/20033]  eta: 0:03:24  lr: 0.000500  loss_labels: 4.6058 (6.4168)  loss: 4.5705 (6.4168)  time: 0.0214\n",
      "Epoch: [0]  [14200/20033]  eta: 0:03:20  lr: 0.000500  loss_labels: 4.6352 (6.4043)  loss: 4.6002 (6.4043)  time: 0.0230\n",
      "Epoch: [0]  [14300/20033]  eta: 0:03:16  lr: 0.000500  loss_labels: 4.6771 (6.3922)  loss: 4.6292 (6.3922)  time: 0.0232\n",
      "Epoch: [0]  [14400/20033]  eta: 0:03:13  lr: 0.000500  loss_labels: 4.6568 (6.3797)  loss: 4.6962 (6.3797)  time: 0.0255\n",
      "Epoch: [0]  [14500/20033]  eta: 0:03:09  lr: 0.000500  loss_labels: 4.5407 (6.3671)  loss: 4.5297 (6.3671)  time: 0.0260\n",
      "Epoch: [0]  [14600/20033]  eta: 0:03:05  lr: 0.000500  loss_labels: 4.6038 (6.3549)  loss: 4.6061 (6.3549)  time: 0.0249\n",
      "Epoch: [0]  [14700/20033]  eta: 0:03:02  lr: 0.000500  loss_labels: 4.5912 (6.3428)  loss: 4.4594 (6.3428)  time: 0.0285\n",
      "Epoch: [0]  [14800/20033]  eta: 0:02:58  lr: 0.000500  loss_labels: 4.4721 (6.3303)  loss: 4.4011 (6.3303)  time: 0.0266\n",
      "Epoch: [0]  [14900/20033]  eta: 0:02:55  lr: 0.000500  loss_labels: 4.5075 (6.3180)  loss: 4.5182 (6.3180)  time: 0.0225\n",
      "Epoch: [0]  [15000/20033]  eta: 0:02:51  lr: 0.000500  loss_labels: 4.4977 (6.3060)  loss: 4.6121 (6.3060)  time: 0.0291\n",
      "Epoch: [0]  [15100/20033]  eta: 0:02:47  lr: 0.000500  loss_labels: 4.5087 (6.2941)  loss: 4.3831 (6.2941)  time: 0.0252\n",
      "Epoch: [0]  [15200/20033]  eta: 0:02:44  lr: 0.000500  loss_labels: 4.3839 (6.2818)  loss: 4.3642 (6.2818)  time: 0.0246\n",
      "Epoch: [0]  [15300/20033]  eta: 0:02:40  lr: 0.000500  loss_labels: 4.3974 (6.2697)  loss: 4.4444 (6.2697)  time: 0.0244\n",
      "Epoch: [0]  [15400/20033]  eta: 0:02:37  lr: 0.000500  loss_labels: 4.4503 (6.2580)  loss: 4.3589 (6.2580)  time: 0.0299\n",
      "Epoch: [0]  [15500/20033]  eta: 0:02:33  lr: 0.000500  loss_labels: 4.4435 (6.2462)  loss: 4.3673 (6.2462)  time: 0.0241\n",
      "Epoch: [0]  [15600/20033]  eta: 0:02:30  lr: 0.000500  loss_labels: 4.3491 (6.2342)  loss: 4.4354 (6.2342)  time: 0.0260\n",
      "Epoch: [0]  [15700/20033]  eta: 0:02:26  lr: 0.000500  loss_labels: 4.3612 (6.2223)  loss: 4.2680 (6.2223)  time: 0.0236\n",
      "Epoch: [0]  [15800/20033]  eta: 0:02:23  lr: 0.000500  loss_labels: 4.3456 (6.2106)  loss: 4.3384 (6.2106)  time: 0.0289\n",
      "Epoch: [0]  [15900/20033]  eta: 0:02:19  lr: 0.000500  loss_labels: 4.4551 (6.1994)  loss: 4.4232 (6.1994)  time: 0.0240\n",
      "Epoch: [0]  [16000/20033]  eta: 0:02:16  lr: 0.000500  loss_labels: 4.3211 (6.1877)  loss: 4.3486 (6.1877)  time: 0.0273\n",
      "Epoch: [0]  [16100/20033]  eta: 0:02:13  lr: 0.000500  loss_labels: 4.2948 (6.1760)  loss: 4.2712 (6.1760)  time: 0.0263\n",
      "Epoch: [0]  [16200/20033]  eta: 0:02:09  lr: 0.000500  loss_labels: 4.2755 (6.1644)  loss: 4.2533 (6.1644)  time: 0.0282\n",
      "Epoch: [0]  [16300/20033]  eta: 0:02:06  lr: 0.000500  loss_labels: 4.3177 (6.1530)  loss: 4.3289 (6.1530)  time: 0.0264\n",
      "Epoch: [0]  [16400/20033]  eta: 0:02:02  lr: 0.000500  loss_labels: 4.2763 (6.1418)  loss: 4.3460 (6.1418)  time: 0.0340\n",
      "Epoch: [0]  [16500/20033]  eta: 0:01:59  lr: 0.000500  loss_labels: 4.2833 (6.1306)  loss: 4.2080 (6.1306)  time: 0.0255\n",
      "Epoch: [0]  [16600/20033]  eta: 0:01:55  lr: 0.000500  loss_labels: 4.2766 (6.1195)  loss: 4.2415 (6.1195)  time: 0.0279\n",
      "Epoch: [0]  [16700/20033]  eta: 0:01:52  lr: 0.000500  loss_labels: 4.1963 (6.1080)  loss: 4.1227 (6.1080)  time: 0.0269\n",
      "Epoch: [0]  [16800/20033]  eta: 0:01:48  lr: 0.000500  loss_labels: 4.1973 (6.0967)  loss: 4.2728 (6.0967)  time: 0.0244\n",
      "Epoch: [0]  [16900/20033]  eta: 0:01:45  lr: 0.000500  loss_labels: 4.2140 (6.0853)  loss: 4.2579 (6.0853)  time: 0.0272\n",
      "Epoch: [0]  [17000/20033]  eta: 0:01:41  lr: 0.000500  loss_labels: 4.2372 (6.0743)  loss: 4.3693 (6.0743)  time: 0.0234\n",
      "Epoch: [0]  [17100/20033]  eta: 0:01:38  lr: 0.000500  loss_labels: 4.1581 (6.0631)  loss: 4.2134 (6.0631)  time: 0.0442\n",
      "Epoch: [0]  [17200/20033]  eta: 0:01:34  lr: 0.000500  loss_labels: 4.1846 (6.0521)  loss: 4.1212 (6.0521)  time: 0.0255\n",
      "Epoch: [0]  [17300/20033]  eta: 0:01:31  lr: 0.000500  loss_labels: 4.1579 (6.0413)  loss: 4.1542 (6.0413)  time: 0.0286\n",
      "Epoch: [0]  [17400/20033]  eta: 0:01:28  lr: 0.000500  loss_labels: 4.1534 (6.0307)  loss: 4.0211 (6.0307)  time: 0.0242\n",
      "Epoch: [0]  [17500/20033]  eta: 0:01:24  lr: 0.000500  loss_labels: 4.1254 (6.0198)  loss: 4.1641 (6.0198)  time: 0.0222\n",
      "Epoch: [0]  [17600/20033]  eta: 0:01:21  lr: 0.000500  loss_labels: 4.1240 (6.0090)  loss: 3.9926 (6.0090)  time: 0.0277\n",
      "Epoch: [0]  [17700/20033]  eta: 0:01:17  lr: 0.000500  loss_labels: 4.0820 (5.9981)  loss: 4.0820 (5.9981)  time: 0.0245\n",
      "Epoch: [0]  [17800/20033]  eta: 0:01:14  lr: 0.000500  loss_labels: 4.0373 (5.9871)  loss: 3.9681 (5.9871)  time: 0.0254\n",
      "Epoch: [0]  [17900/20033]  eta: 0:01:10  lr: 0.000500  loss_labels: 4.1227 (5.9767)  loss: 4.1048 (5.9767)  time: 0.0230\n",
      "Epoch: [0]  [18000/20033]  eta: 0:01:07  lr: 0.000500  loss_labels: 4.0603 (5.9660)  loss: 4.0810 (5.9660)  time: 0.0250\n",
      "Epoch: [0]  [18100/20033]  eta: 0:01:04  lr: 0.000500  loss_labels: 3.9986 (5.9552)  loss: 4.0723 (5.9552)  time: 0.0281\n",
      "Epoch: [0]  [18200/20033]  eta: 0:01:00  lr: 0.000500  loss_labels: 4.0173 (5.9445)  loss: 3.8098 (5.9445)  time: 0.0255\n",
      "Epoch: [0]  [18300/20033]  eta: 0:00:57  lr: 0.000500  loss_labels: 3.9684 (5.9338)  loss: 4.1898 (5.9338)  time: 0.0262\n",
      "Epoch: [0]  [18400/20033]  eta: 0:00:54  lr: 0.000500  loss_labels: 3.9744 (5.9233)  loss: 3.9892 (5.9233)  time: 0.0251\n",
      "Epoch: [0]  [18500/20033]  eta: 0:00:50  lr: 0.000500  loss_labels: 3.9560 (5.9128)  loss: 3.8661 (5.9128)  time: 0.0245\n",
      "Epoch: [0]  [18600/20033]  eta: 0:00:47  lr: 0.000500  loss_labels: 3.9993 (5.9024)  loss: 3.9054 (5.9024)  time: 0.0360\n",
      "Epoch: [0]  [18700/20033]  eta: 0:00:44  lr: 0.000500  loss_labels: 3.8708 (5.8919)  loss: 3.8168 (5.8919)  time: 0.0255\n",
      "Epoch: [0]  [18800/20033]  eta: 0:00:40  lr: 0.000500  loss_labels: 3.9285 (5.8815)  loss: 3.6959 (5.8815)  time: 0.0245\n",
      "Epoch: [0]  [18900/20033]  eta: 0:00:37  lr: 0.000500  loss_labels: 3.9119 (5.8711)  loss: 3.9193 (5.8711)  time: 0.0289\n",
      "Epoch: [0]  [19000/20033]  eta: 0:00:34  lr: 0.000500  loss_labels: 3.8738 (5.8607)  loss: 3.8063 (5.8607)  time: 0.0259\n",
      "Epoch: [0]  [19100/20033]  eta: 0:00:30  lr: 0.000500  loss_labels: 3.8791 (5.8504)  loss: 3.9042 (5.8504)  time: 0.0235\n",
      "Epoch: [0]  [19200/20033]  eta: 0:00:27  lr: 0.000500  loss_labels: 3.8589 (5.8403)  loss: 3.8937 (5.8403)  time: 0.0249\n",
      "Epoch: [0]  [19300/20033]  eta: 0:00:24  lr: 0.000500  loss_labels: 3.9163 (5.8303)  loss: 3.9163 (5.8303)  time: 0.0244\n",
      "Epoch: [0]  [19400/20033]  eta: 0:00:20  lr: 0.000500  loss_labels: 3.8288 (5.8201)  loss: 3.7817 (5.8201)  time: 0.0293\n",
      "Epoch: [0]  [19500/20033]  eta: 0:00:17  lr: 0.000500  loss_labels: 3.8861 (5.8100)  loss: 3.8998 (5.8100)  time: 0.0265\n",
      "Epoch: [0]  [19600/20033]  eta: 0:00:14  lr: 0.000500  loss_labels: 3.8569 (5.8001)  loss: 3.8839 (5.8001)  time: 0.0228\n",
      "Epoch: [0]  [19700/20033]  eta: 0:00:10  lr: 0.000500  loss_labels: 3.8164 (5.7901)  loss: 3.6954 (5.7901)  time: 0.0258\n",
      "Epoch: [0]  [19800/20033]  eta: 0:00:07  lr: 0.000500  loss_labels: 3.8367 (5.7802)  loss: 4.0280 (5.7802)  time: 0.0257\n",
      "Epoch: [0]  [19900/20033]  eta: 0:00:04  lr: 0.000500  loss_labels: 3.7244 (5.7700)  loss: 3.8007 (5.7700)  time: 0.0242\n",
      "Epoch: [0]  [20000/20033]  eta: 0:00:01  lr: 0.000500  loss_labels: 3.8570 (5.7603)  loss: 3.8400 (5.7603)  time: 0.0257\n",
      "Epoch: [0]  [20032/20033]  eta: 0:00:00  lr: 0.000500  loss_labels: 3.8400 (5.7573)  loss: 3.8187 (5.7573)  time: 0.0561\n",
      "Epoch: [0] Total time: 0:10:57 (0.0328 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 3.8400 (5.7573)  loss: 3.8187 (5.7573)\n",
      "Test:  [   0/2257]  eta: 14:23:24  loss_labels: 4.9762 (4.9762)  loss: 4.9762 (4.9762)  time: 22.9529\n",
      "Test:  [ 100/2257]  eta: 0:09:12  loss_labels: 4.3226 (4.2962)  loss: 3.7932 (4.2962)  time: 0.0311\n",
      "Test:  [ 200/2257]  eta: 0:04:52  loss_labels: 4.3390 (4.3549)  loss: 4.2643 (4.3549)  time: 0.0279\n",
      "Test:  [ 300/2257]  eta: 0:03:26  loss_labels: 4.3748 (4.4016)  loss: 3.8297 (4.4016)  time: 0.0260\n",
      "Test:  [ 400/2257]  eta: 0:02:44  loss_labels: 4.4281 (4.4064)  loss: 3.5035 (4.4064)  time: 0.0225\n",
      "Test:  [ 500/2257]  eta: 0:02:13  loss_labels: 4.2567 (4.3783)  loss: 4.2641 (4.3783)  time: 0.0258\n",
      "Test:  [ 600/2257]  eta: 0:01:51  loss_labels: 4.3504 (4.3747)  loss: 4.2068 (4.3747)  time: 0.0262\n",
      "Test:  [ 700/2257]  eta: 0:01:35  loss_labels: 4.0861 (4.3401)  loss: 4.0861 (4.3401)  time: 0.0241\n",
      "Test:  [ 800/2257]  eta: 0:01:23  loss_labels: 4.5538 (4.3638)  loss: 4.7661 (4.3638)  time: 0.0247\n",
      "Test:  [ 900/2257]  eta: 0:01:14  loss_labels: 4.0826 (4.3340)  loss: 3.7875 (4.3340)  time: 0.0616\n",
      "Test:  [1000/2257]  eta: 0:01:05  loss_labels: 4.1900 (4.3215)  loss: 4.2762 (4.3215)  time: 0.0254\n",
      "Test:  [1100/2257]  eta: 0:00:57  loss_labels: 4.4190 (4.3220)  loss: 4.0312 (4.3220)  time: 0.0257\n",
      "Test:  [1200/2257]  eta: 0:00:50  loss_labels: 4.1557 (4.3135)  loss: 4.3424 (4.3135)  time: 0.0245\n",
      "Test:  [1300/2257]  eta: 0:00:44  loss_labels: 4.1427 (4.3040)  loss: 4.3673 (4.3040)  time: 0.0255\n",
      "Test:  [1400/2257]  eta: 0:00:39  loss_labels: 4.6351 (4.3262)  loss: 4.6803 (4.3262)  time: 0.0633\n",
      "Test:  [1500/2257]  eta: 0:00:33  loss_labels: 4.3443 (4.3302)  loss: 4.5852 (4.3302)  time: 0.0248\n",
      "Test:  [1600/2257]  eta: 0:00:28  loss_labels: 4.2331 (4.3323)  loss: 4.5187 (4.3323)  time: 0.0252\n",
      "Test:  [1700/2257]  eta: 0:00:23  loss_labels: 3.9997 (4.3143)  loss: 4.7105 (4.3143)  time: 0.0264\n",
      "Test:  [1800/2257]  eta: 0:00:18  loss_labels: 4.0008 (4.3087)  loss: 4.0659 (4.3087)  time: 0.0258\n",
      "Test:  [1900/2257]  eta: 0:00:14  loss_labels: 3.9778 (4.2936)  loss: 4.0604 (4.2936)  time: 0.0233\n",
      "Test:  [2000/2257]  eta: 0:00:10  loss_labels: 4.2919 (4.3001)  loss: 4.2226 (4.3001)  time: 0.0261\n",
      "Test:  [2100/2257]  eta: 0:00:06  loss_labels: 4.3197 (4.2972)  loss: 3.9265 (4.2972)  time: 0.0266\n",
      "Test:  [2200/2257]  eta: 0:00:02  loss_labels: 4.3849 (4.2985)  loss: 4.8077 (4.2985)  time: 0.0376\n",
      "Test:  [2256/2257]  eta: 0:00:00  loss_labels: 4.2921 (4.2983)  loss: 4.1492 (4.2983)  time: 0.0393\n",
      "Test: Total time: 0:01:29 (0.0395 s / it)\n",
      "Averaged stats: loss_labels: 4.2921 (4.2983)  loss: 4.1492 (4.2983)\n",
      "acc: 0.21281781792640686\n",
      "top 1 and top 5 accuracies {'top1': 0.2128178142137041, 'top5': 0.4121752617293525, 'loss': tensor(0.1343, device='cuda:0')}\n",
      "Epoch: [1]  [    0/20033]  eta: 9 days, 21:48:52  lr: 0.000500  loss_labels: 3.8955 (3.8955)  loss: 3.8955 (3.8955)  time: 42.7361\n",
      "Epoch: [1]  [  100/20033]  eta: 2:31:10  lr: 0.000500  loss_labels: 3.7906 (3.8072)  loss: 3.7699 (3.8072)  time: 0.0283\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/BLT_models/main.py\", line 304, in <module>\n",
      "    mp.spawn(main, args=(args.world_size, args), nprocs=args.world_size)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 281, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 237, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 117, in join\n",
      "    ready = multiprocessing.connection.wait(\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --distributed 1 --save_model 0 --dataset 'vggface2' --num_layers 6 --model 'blt_b_top2linear' --epochs 100 --lr .0005 --loss_choice 'weighted' --loss_gamma 0.2 --recurrent_steps 10 --batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --save_model 0 --dataset 'imagenet' --wandb_p 'imagenet' --wandb_r 'blt_bl'  --model 'blt_bl' --epochs 100 --lr .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:3: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/tmp/ipykernel_71418/692844870.py:3: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  print(a is not 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.zeros(3,3)\n",
    "print(a is not 0)\n",
    "\n",
    "_top2linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3583175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'], returncode=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "script = f'''#!/bin/sh\n",
    "#\n",
    "#\n",
    "#SBATCH --account=nklab\n",
    "#SBATCH --job-name=b_pm_g  # The job name.\n",
    "#SBATCH --gres=gpu:3\n",
    "#SBATCH --nodelist=ax01\n",
    "#SBATCH --cpus-per-task=18\n",
    "\n",
    "ml load anaconda3-2019.03\n",
    "\n",
    "cd /engram/nklab/hossein/recurrent_models/BLT_models/\n",
    "\n",
    "conda activate py39\n",
    "\n",
    "python main.py --distributed 1 --save_model 1 --dataset 'imagenet' --wandb_p 'imagenet' --wandb_r 'b_top2linear_new_lrd4' --num_layers 6 --model 'blt_b_top2linear' --epochs 100 --lr .0005 --loss_choice 'weighted' --loss_gamma 0 --recurrent_steps 10 --lr_drop 4 --port '12370' --run 2 --batch_size 16\n",
    "\n",
    "'''\n",
    "\n",
    "bash_script_path = \"/engram/nklab/hossein/batch_scripts/imagenet_nb.sh\"\n",
    "os.chdir('/engram/nklab/hossein/batch_scripts/')\n",
    "\n",
    "with open(bash_script_path, \"w+\") as bash_script_file:\n",
    "    bash_script_file.write(script)\n",
    "\n",
    "subprocess.run(['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py --save_model 1 --dataset 'vggface2' --wandb_p 'vggface2' --wandb_r 'b_small_0.0005'  --model 'blt_b' --num_layers 6 --epochs 100 --lr .0005 --lr_drop 10 --port '12391' --run 22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', choices=['weighted', 'decay'] \n",
    "                        , default='decay', type=str, \n",
    "                        help='how to apply loss to earlier readout layers')  \n",
    "    parser.add_argument("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py39]",
   "language": "python",
   "name": "conda-env-.conda-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
