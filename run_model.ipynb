{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/BLT_models\n"
     ]
    }
   ],
   "source": [
    "data_path = '/share/data/imagenet-pytorch'\n",
    "\n",
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/BLT_models/')\n",
    "!pwd\n",
    "\n",
    "#--wandb_p 'vggface2' --wandb_r 'blt_bl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 0): env://\n",
      "batch_size:64;   workers:4\n",
      "cuda\n",
      "Number of model parameters: 15437736\n",
      "blt(\n",
      "  (conv_input): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (maxpool_input): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (output_prenorm_0): Identity()\n",
      "  (non_lin_0): ReLU(inplace=True)\n",
      "  (norm_0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (output_0): Identity()\n",
      "  (conv_0_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_0_1): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (output_prenorm_1): Identity()\n",
      "  (non_lin_1): ReLU(inplace=True)\n",
      "  (norm_1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (output_1): Identity()\n",
      "  (conv_1_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_1_2): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (output_prenorm_2): Identity()\n",
      "  (non_lin_2): ReLU(inplace=True)\n",
      "  (norm_2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (output_2): Identity()\n",
      "  (conv_2_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_2_3): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (output_prenorm_3): Identity()\n",
      "  (non_lin_3): ReLU(inplace=True)\n",
      "  (norm_3): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (output_3): Identity()\n",
      "  (conv_3_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_3_4): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (Unsqueeze): Unsqueeze()\n",
      "  )\n",
      "  (output_prenorm_4): Identity()\n",
      "  (non_lin_4): ReLU(inplace=True)\n",
      "  (norm_4): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "  (output_4): Identity()\n",
      "  (conv_4_4): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_4_5): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (output_prenorm_5): Identity()\n",
      "  (non_lin_5): ReLU(inplace=True)\n",
      "  (norm_5): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (output_5): Identity()\n",
      "  (conv_5_5): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (read_out): Sequential(\n",
      "    (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['conv_input.weight', 'conv_input.bias', 'norm_0.weight', 'norm_0.bias', 'conv_0_0.weight', 'conv_0_0.bias', 'conv_0_1.conv.weight', 'conv_0_1.conv.bias', 'norm_1.weight', 'norm_1.bias', 'conv_1_1.weight', 'conv_1_1.bias', 'conv_1_2.conv.weight', 'conv_1_2.conv.bias', 'norm_2.weight', 'norm_2.bias', 'conv_2_2.weight', 'conv_2_2.bias', 'conv_2_3.conv.weight', 'conv_2_3.conv.bias', 'norm_3.weight', 'norm_3.bias', 'conv_3_3.weight', 'conv_3_3.bias', 'conv_3_4.linear.weight', 'conv_3_4.linear.bias', 'norm_4.weight', 'norm_4.bias', 'conv_4_4.weight', 'conv_4_4.bias', 'conv_4_5.weight', 'conv_4_5.bias', 'norm_5.weight', 'norm_5.bias', 'conv_5_5.weight', 'conv_5_5.bias', 'read_out.linear.weight', 'read_out.linear.bias']\n",
      "Start training\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch: [0]  [    0/20018]  eta: 6 days, 14:20:10  lr: 0.000500  loss: 6.9899 (6.9899)  time: 28.4749\n",
      "Epoch: [0]  [  100/20018]  eta: 2:22:04  lr: 0.000500  loss: 7.0697 (7.0657)  time: 0.1515\n",
      "Epoch: [0]  [  200/20018]  eta: 1:35:20  lr: 0.000500  loss: 6.9868 (7.0254)  time: 0.1524\n",
      "Epoch: [0]  [  300/20018]  eta: 1:18:39  lr: 0.000500  loss: 6.9604 (7.0040)  time: 0.1461\n",
      "Epoch: [0]  [  400/20018]  eta: 1:10:02  lr: 0.000500  loss: 6.9481 (6.9908)  time: 0.1484\n",
      "Epoch: [0]  [  500/20018]  eta: 1:05:03  lr: 0.000500  loss: 6.9400 (6.9808)  time: 0.1466\n",
      "Epoch: [0]  [  600/20018]  eta: 1:01:29  lr: 0.000500  loss: 6.9408 (6.9743)  time: 0.1349\n",
      "Epoch: [0]  [  700/20018]  eta: 0:59:06  lr: 0.000500  loss: 6.9387 (6.9696)  time: 0.1318\n",
      "Epoch: [0]  [  800/20018]  eta: 0:57:14  lr: 0.000500  loss: 6.9351 (6.9653)  time: 0.1343\n",
      "Epoch: [0]  [  900/20018]  eta: 0:55:24  lr: 0.000500  loss: 6.9391 (6.9622)  time: 0.1383\n",
      "Epoch: [0]  [ 1000/20018]  eta: 0:54:07  lr: 0.000500  loss: 6.9243 (6.9586)  time: 0.1401\n",
      "Epoch: [0]  [ 1100/20018]  eta: 0:52:52  lr: 0.000500  loss: 6.9302 (6.9560)  time: 0.1354\n",
      "Epoch: [0]  [ 1200/20018]  eta: 0:52:01  lr: 0.000500  loss: 6.9273 (6.9536)  time: 0.1297\n",
      "Epoch: [0]  [ 1300/20018]  eta: 0:51:12  lr: 0.000500  loss: 6.9230 (6.9513)  time: 0.1580\n",
      "Epoch: [0]  [ 1400/20018]  eta: 0:50:35  lr: 0.000500  loss: 6.9297 (6.9494)  time: 0.1496\n",
      "Epoch: [0]  [ 1500/20018]  eta: 0:49:42  lr: 0.000500  loss: 6.9198 (6.9473)  time: 0.1396\n",
      "Epoch: [0]  [ 1600/20018]  eta: 0:49:00  lr: 0.000500  loss: 6.9085 (6.9450)  time: 0.1444\n",
      "Epoch: [0]  [ 1700/20018]  eta: 0:48:33  lr: 0.000500  loss: 6.9001 (6.9423)  time: 0.1334\n",
      "Epoch: [0]  [ 1800/20018]  eta: 0:47:52  lr: 0.000500  loss: 6.8736 (6.9387)  time: 0.1346\n",
      "Epoch: [0]  [ 1900/20018]  eta: 0:47:16  lr: 0.000500  loss: 6.8474 (6.9340)  time: 0.1376\n",
      "Epoch: [0]  [ 2000/20018]  eta: 0:47:05  lr: 0.000500  loss: 6.8506 (6.9298)  time: 0.1494\n",
      "Epoch: [0]  [ 2100/20018]  eta: 0:46:57  lr: 0.000500  loss: 6.8417 (6.9254)  time: 0.1669\n",
      "Epoch: [0]  [ 2200/20018]  eta: 0:55:34  lr: 0.000500  loss: 6.8217 (6.9207)  time: 0.8509\n",
      "Epoch: [0]  [ 2300/20018]  eta: 1:03:51  lr: 0.000500  loss: 6.8000 (6.9153)  time: 0.8084\n",
      "Epoch: [0]  [ 2400/20018]  eta: 1:11:26  lr: 0.000500  loss: 6.7862 (6.9100)  time: 0.8352\n",
      "Epoch: [0]  [ 2500/20018]  eta: 1:18:06  lr: 0.000500  loss: 6.7804 (6.9051)  time: 0.8112\n",
      "Epoch: [0]  [ 2600/20018]  eta: 1:27:44  lr: 0.000500  loss: 6.7744 (6.9002)  time: 1.3696\n",
      "Epoch: [0]  [ 2700/20018]  eta: 1:37:46  lr: 0.000500  loss: 6.7673 (6.8951)  time: 1.2518\n",
      "Epoch: [0]  [ 2800/20018]  eta: 1:44:52  lr: 0.000500  loss: 6.7630 (6.8905)  time: 1.0701\n",
      "Epoch: [0]  [ 2900/20018]  eta: 1:50:54  lr: 0.000500  loss: 6.7573 (6.8858)  time: 1.0189\n",
      "Epoch: [0]  [ 3000/20018]  eta: 1:56:15  lr: 0.000500  loss: 6.7459 (6.8811)  time: 0.9736\n",
      "Epoch: [0]  [ 3100/20018]  eta: 2:00:56  lr: 0.000500  loss: 6.7244 (6.8761)  time: 1.0807\n",
      "Epoch: [0]  [ 3200/20018]  eta: 2:05:52  lr: 0.000500  loss: 6.7316 (6.8714)  time: 1.0307\n",
      "Epoch: [0]  [ 3300/20018]  eta: 2:09:54  lr: 0.000500  loss: 6.7254 (6.8667)  time: 0.9720\n",
      "Epoch: [0]  [ 3400/20018]  eta: 2:13:20  lr: 0.000500  loss: 6.7011 (6.8619)  time: 0.9750\n",
      "Epoch: [0]  [ 3500/20018]  eta: 2:19:20  lr: 0.000500  loss: 6.6618 (6.8565)  time: 1.5752\n",
      "Epoch: [0]  [ 3600/20018]  eta: 2:26:05  lr: 0.000500  loss: 6.6740 (6.8512)  time: 1.4857\n",
      "Epoch: [0]  [ 3700/20018]  eta: 2:32:21  lr: 0.000500  loss: 6.6316 (6.8455)  time: 1.5154\n",
      "Epoch: [0]  [ 3800/20018]  eta: 2:38:16  lr: 0.000500  loss: 6.6465 (6.8401)  time: 1.5424\n",
      "Epoch: [0]  [ 3900/20018]  eta: 2:43:28  lr: 0.000500  loss: 6.5983 (6.8341)  time: 1.4894\n",
      "Epoch: [0]  [ 4000/20018]  eta: 2:48:19  lr: 0.000500  loss: 6.5818 (6.8280)  time: 1.4343\n",
      "Epoch: [0]  [ 4100/20018]  eta: 2:52:23  lr: 0.000500  loss: 6.6093 (6.8228)  time: 1.3641\n",
      "Epoch: [0]  [ 4200/20018]  eta: 2:55:58  lr: 0.000500  loss: 6.5566 (6.8165)  time: 1.3870\n",
      "Epoch: [0]  [ 4300/20018]  eta: 3:02:21  lr: 0.000500  loss: 6.5457 (6.8105)  time: 1.9930\n",
      "Epoch: [0]  [ 4400/20018]  eta: 3:06:30  lr: 0.000500  loss: 6.5351 (6.8044)  time: 1.4529\n",
      "Epoch: [0]  [ 4500/20018]  eta: 3:09:32  lr: 0.000500  loss: 6.5171 (6.7982)  time: 1.5015\n",
      "Epoch: [0]  [ 4600/20018]  eta: 3:12:23  lr: 0.000500  loss: 6.5411 (6.7926)  time: 1.4848\n",
      "Epoch: [0]  [ 4700/20018]  eta: 3:15:23  lr: 0.000500  loss: 6.4716 (6.7859)  time: 1.4630\n",
      "Epoch: [0]  [ 4800/20018]  eta: 3:17:55  lr: 0.000500  loss: 6.4764 (6.7793)  time: 1.5494\n",
      "Epoch: [0]  [ 4900/20018]  eta: 3:20:28  lr: 0.000500  loss: 6.4343 (6.7722)  time: 1.4761\n",
      "Epoch: [0]  [ 5000/20018]  eta: 3:22:25  lr: 0.000500  loss: 6.4438 (6.7657)  time: 1.4209\n",
      "Epoch: [0]  [ 5100/20018]  eta: 3:24:17  lr: 0.000500  loss: 6.4003 (6.7583)  time: 1.5032\n",
      "Epoch: [0]  [ 5200/20018]  eta: 3:25:35  lr: 0.000500  loss: 6.3889 (6.7508)  time: 1.4071\n",
      "Epoch: [0]  [ 5300/20018]  eta: 3:26:55  lr: 0.000500  loss: 6.3679 (6.7435)  time: 1.4471\n",
      "Epoch: [0]  [ 5400/20018]  eta: 3:28:33  lr: 0.000500  loss: 6.3321 (6.7359)  time: 1.4781\n",
      "Epoch: [0]  [ 5500/20018]  eta: 3:29:51  lr: 0.000500  loss: 6.3033 (6.7280)  time: 1.4389\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python main.py --lr 0.0005 --save_model 1 --dataset 'imagenet' --model 'blt_bl_top2linear' --port '12374' --run '1' --batch_size 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 4740756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "script = f'''#!/bin/sh\n",
    "#\n",
    "#\n",
    "#SBATCH --account=nklab\n",
    "#SBATCH --job-name=b  # The job name.\n",
    "#SBATCH --gres=gpu:3\n",
    "#SBATCH --nodelist=ax17\n",
    "#SBATCH --cpus-per-task=12\n",
    "#SBATCH --mem-per-cpu=8G \n",
    "\n",
    "ml load anaconda3-2023.07\n",
    "\n",
    "cd /engram/nklab/hossein/recurrent_models/BLT_models/\n",
    "\n",
    "conda activate py39\n",
    "\n",
    "python main.py --save_model 1 --dataset 'imagenet_vggface2' --model 'blt_bl_top2linear' --lr 0.0005 --port '12374' --wandb_p 'imagenet vggface2 adam' --run '1' --batch_size 192\n",
    "\n",
    "'''\n",
    "\n",
    "bash_script_path = \"/engram/nklab/hossein/batch_scripts/imagenet_nb.sh\"\n",
    "os.chdir('/engram/nklab/hossein/batch_scripts/')\n",
    "\n",
    "with open(bash_script_path, \"w+\") as bash_script_file:\n",
    "    bash_script_file.write(script)\n",
    "\n",
    "subprocess.run(['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env (py39)",
   "language": "python",
   "name": "py39"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
