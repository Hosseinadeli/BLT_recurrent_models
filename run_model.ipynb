{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/BLT_models\n"
     ]
    }
   ],
   "source": [
    "data_path = '/share/data/imagenet-pytorch'\n",
    "\n",
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/BLT_models/')\n",
    "!pwd\n",
    "\n",
    "#--wandb_p 'vggface2' --wandb_r 'blt_bl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 0): env://\n",
      "| distributed init (rank 1): env://\n",
      "| distributed init (rank 2): env://\n",
      "cuda\n",
      "Number of model parameters: 7310632\n",
      "blt(\n",
      "  (conv_input): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (maxpool_input): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (output_prenorm_0): Identity()\n",
      "  (non_lin_0): ReLU(inplace=True)\n",
      "  (norm_0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (output_0): Identity()\n",
      "  (conv_0_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_0_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (output_prenorm_1): Identity()\n",
      "  (non_lin_1): ReLU(inplace=True)\n",
      "  (norm_1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (output_1): Identity()\n",
      "  (conv_1_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_1_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_1_2): Sequential(\n",
      "    (blurpool): BlurPool(\n",
      "      (pad): ReflectionPad2d((1, 2, 1, 2))\n",
      "    )\n",
      "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (output_prenorm_2): Identity()\n",
      "  (non_lin_2): ReLU(inplace=True)\n",
      "  (norm_2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (output_2): Identity()\n",
      "  (conv_2_1): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_2_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (output_prenorm_3): Identity()\n",
      "  (non_lin_3): ReLU(inplace=True)\n",
      "  (norm_3): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (output_3): Identity()\n",
      "  (conv_3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_3_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_3_4): Sequential(\n",
      "    (blurpool): BlurPool(\n",
      "      (pad): ReflectionPad2d((1, 2, 1, 2))\n",
      "    )\n",
      "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (output_prenorm_4): Identity()\n",
      "  (non_lin_4): ReLU(inplace=True)\n",
      "  (norm_4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (output_4): Identity()\n",
      "  (conv_4_3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_4_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_4_5): Sequential(\n",
      "    (blurpool): BlurPool(\n",
      "      (pad): ReflectionPad2d((1, 2, 1, 2))\n",
      "    )\n",
      "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (output_prenorm_5): Identity()\n",
      "  (non_lin_5): ReLU(inplace=True)\n",
      "  (norm_5): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (output_5): Identity()\n",
      "  (conv_5_4): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (conv_5_5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (read_out): Sequential(\n",
      "    (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['conv_input.weight', 'conv_input.bias', 'norm_0.weight', 'norm_0.bias', 'conv_0_0.weight', 'conv_0_0.bias', 'conv_0_1.weight', 'conv_0_1.bias', 'norm_1.weight', 'norm_1.bias', 'conv_1_0.weight', 'conv_1_0.bias', 'conv_1_1.weight', 'conv_1_1.bias', 'conv_1_2.conv.weight', 'conv_1_2.conv.bias', 'norm_2.weight', 'norm_2.bias', 'conv_2_1.weight', 'conv_2_1.bias', 'conv_2_2.weight', 'conv_2_2.bias', 'conv_2_3.weight', 'conv_2_3.bias', 'norm_3.weight', 'norm_3.bias', 'conv_3_2.weight', 'conv_3_2.bias', 'conv_3_3.weight', 'conv_3_3.bias', 'conv_3_4.conv.weight', 'conv_3_4.conv.bias', 'norm_4.weight', 'norm_4.bias', 'conv_4_3.weight', 'conv_4_3.bias', 'conv_4_4.weight', 'conv_4_4.bias', 'conv_4_5.conv.weight', 'conv_4_5.conv.bias', 'norm_5.weight', 'norm_5.bias', 'conv_5_4.weight', 'conv_5_4.bias', 'conv_5_5.weight', 'conv_5_5.bias', 'read_out.linear.weight', 'read_out.linear.bias']\n",
      "Start training\n",
      "[rank0]:[W112 11:40:56.255347056 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank2]:[W112 11:40:56.408364359 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W112 11:40:56.553737218 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch: [0]  [   0/3336]  eta: 1 day, 6:34:27  lr: 0.100000  loss_labels: 6.9597 (6.9597)  loss: 6.9597 (6.9597)  time: 32.9939\n",
      "Epoch: [0]  [ 100/3336]  eta: 1:24:14  lr: 0.100000  loss_labels: 6.9325 (6.9368)  loss: 6.9225 (6.9368)  time: 1.2528\n",
      "Epoch: [0]  [ 200/3336]  eta: 0:59:05  lr: 0.100000  loss_labels: 6.9188 (6.9276)  loss: 6.9147 (6.9276)  time: 0.3426\n",
      "Epoch: [0]  [ 300/3336]  eta: 0:43:44  lr: 0.100000  loss_labels: 6.9130 (6.9231)  loss: 6.9121 (6.9231)  time: 0.3047\n",
      "Epoch: [0]  [ 400/3336]  eta: 0:39:06  lr: 0.100000  loss_labels: 6.9092 (6.9197)  loss: 6.9082 (6.9197)  time: 1.4156\n",
      "Epoch: [0]  [ 500/3336]  eta: 0:33:20  lr: 0.100000  loss_labels: 6.8973 (6.9150)  loss: 6.8849 (6.9150)  time: 0.3117\n",
      "Epoch: [0]  [ 600/3336]  eta: 0:29:08  lr: 0.100000  loss_labels: 6.8596 (6.9055)  loss: 6.8333 (6.9055)  time: 0.2953\n",
      "Epoch: [0]  [ 700/3336]  eta: 0:26:00  lr: 0.100000  loss_labels: 6.8222 (6.8933)  loss: 6.7973 (6.8933)  time: 0.2941\n",
      "Epoch: [0]  [ 800/3336]  eta: 0:28:06  lr: 0.100000  loss_labels: 6.7896 (6.8805)  loss: 6.7837 (6.8805)  time: 1.3126\n",
      "Epoch: [0]  [ 900/3336]  eta: 0:29:56  lr: 0.100000  loss_labels: 6.7628 (6.8675)  loss: 6.7502 (6.8675)  time: 1.3199\n",
      "Epoch: [0]  [1000/3336]  eta: 0:30:57  lr: 0.100000  loss_labels: 6.7360 (6.8546)  loss: 6.7259 (6.8546)  time: 1.3263\n",
      "Epoch: [0]  [1100/3336]  eta: 0:31:26  lr: 0.100000  loss_labels: 6.7254 (6.8431)  loss: 6.7122 (6.8431)  time: 1.3102\n",
      "Epoch: [0]  [1200/3336]  eta: 0:31:27  lr: 0.100000  loss_labels: 6.7013 (6.8313)  loss: 6.6909 (6.8313)  time: 1.3035\n",
      "Epoch: [0]  [1300/3336]  eta: 0:31:09  lr: 0.100000  loss_labels: 6.6821 (6.8198)  loss: 6.6672 (6.8198)  time: 1.3248\n",
      "Epoch: [0]  [1400/3336]  eta: 0:30:19  lr: 0.100000  loss_labels: 6.6632 (6.8086)  loss: 6.6679 (6.8086)  time: 0.8059\n",
      "Epoch: [0]  [1500/3336]  eta: 0:27:29  lr: 0.100000  loss_labels: 6.6441 (6.7977)  loss: 6.6305 (6.7977)  time: 0.3277\n",
      "Epoch: [0]  [1600/3336]  eta: 0:24:57  lr: 0.100000  loss_labels: 6.6182 (6.7864)  loss: 6.5942 (6.7864)  time: 0.3323\n",
      "Epoch: [0]  [1700/3336]  eta: 0:23:07  lr: 0.100000  loss_labels: 6.5948 (6.7752)  loss: 6.5853 (6.7752)  time: 0.2892\n",
      "Epoch: [0]  [1800/3336]  eta: 0:20:56  lr: 0.100000  loss_labels: 6.5743 (6.7643)  loss: 6.5606 (6.7643)  time: 0.3159\n",
      "Epoch: [0]  [1900/3336]  eta: 0:18:56  lr: 0.100000  loss_labels: 6.5540 (6.7535)  loss: 6.5431 (6.7535)  time: 0.3109\n",
      "Epoch: [0]  [2000/3336]  eta: 0:17:37  lr: 0.100000  loss_labels: 6.5307 (6.7426)  loss: 6.5401 (6.7426)  time: 1.3358\n",
      "Epoch: [0]  [2100/3336]  eta: 0:16:49  lr: 0.100000  loss_labels: 6.5205 (6.7320)  loss: 6.5129 (6.7320)  time: 1.3475\n",
      "Epoch: [0]  [2200/3336]  eta: 0:15:54  lr: 0.100000  loss_labels: 6.5070 (6.7219)  loss: 6.5027 (6.7219)  time: 1.3373\n",
      "Epoch: [0]  [2300/3336]  eta: 0:14:52  lr: 0.100000  loss_labels: 6.4852 (6.7117)  loss: 6.4811 (6.7117)  time: 1.3208\n",
      "Epoch: [0]  [2400/3336]  eta: 0:13:44  lr: 0.100000  loss_labels: 6.4762 (6.7019)  loss: 6.4761 (6.7019)  time: 1.3422\n",
      "Epoch: [0]  [2500/3336]  eta: 0:12:32  lr: 0.100000  loss_labels: 6.4386 (6.6915)  loss: 6.4331 (6.6915)  time: 1.3540\n",
      "Epoch: [0]  [2600/3336]  eta: 0:11:14  lr: 0.100000  loss_labels: 6.4421 (6.6818)  loss: 6.4480 (6.6818)  time: 1.3171\n",
      "Epoch: [0]  [2700/3336]  eta: 0:09:35  lr: 0.100000  loss_labels: 6.4271 (6.6725)  loss: 6.4237 (6.6725)  time: 0.3185\n",
      "Epoch: [0]  [2800/3336]  eta: 0:07:53  lr: 0.100000  loss_labels: 6.4173 (6.6634)  loss: 6.3877 (6.6634)  time: 0.3035\n",
      "Epoch: [0]  [2900/3336]  eta: 0:06:21  lr: 0.100000  loss_labels: 6.3980 (6.6543)  loss: 6.3985 (6.6543)  time: 0.2970\n",
      "Epoch: [0]  [3000/3336]  eta: 0:04:47  lr: 0.100000  loss_labels: 6.3917 (6.6456)  loss: 6.3765 (6.6456)  time: 0.2966\n",
      "Epoch: [0]  [3100/3336]  eta: 0:03:17  lr: 0.100000  loss_labels: 6.3683 (6.6366)  loss: 6.3424 (6.6366)  time: 0.3021\n",
      "Epoch: [0]  [3200/3336]  eta: 0:01:53  lr: 0.100000  loss_labels: 6.3535 (6.6279)  loss: 6.3268 (6.6279)  time: 1.3057\n",
      "Epoch: [0]  [3300/3336]  eta: 0:00:30  lr: 0.100000  loss_labels: 6.3546 (6.6195)  loss: 6.3448 (6.6195)  time: 1.3196\n",
      "Epoch: [0]  [3335/3336]  eta: 0:00:00  lr: 0.100000  loss_labels: 6.3458 (6.6166)  loss: 6.3438 (6.6166)  time: 1.2691\n",
      "Epoch: [0] Total time: 0:47:21 (0.8519 s / it)\n",
      "Averaged stats: lr: 0.100000  loss_labels: 6.3458 (6.6166)  loss: 6.3438 (6.6166)\n",
      "Test:  [  0/131]  eta: 0:48:47  loss_labels: 6.1101 (6.1101)  loss: 6.1101 (6.1101)  time: 22.3473\n",
      "Test:  [100/131]  eta: 0:00:43  loss_labels: 6.3295 (6.3290)  loss: 6.4892 (6.3290)  time: 1.1907\n",
      "Test:  [130/131]  eta: 0:00:01  loss_labels: 6.4116 (6.3111)  loss: 6.1202 (6.3111)  time: 1.1836\n",
      "Test: Total time: 0:02:58 (1.3611 s / it)\n",
      "Averaged stats: loss_labels: 6.4116 (6.3111)  loss: 6.1202 (6.3111)\n",
      "acc: 0.010919782333076\n",
      "top 1 and top 5 accuracies {'top1': 0.010919781604367913, 'top5': 0.04859902801943961, 'loss': tensor(0.0496, device='cuda:0')}\n",
      "Epoch: [1]  [   0/3336]  eta: 1 day, 0:12:22  lr: 0.100000  loss_labels: 6.2657 (6.2657)  loss: 6.2657 (6.2657)  time: 26.1217\n",
      "Epoch: [1]  [ 100/3336]  eta: 1:20:20  lr: 0.100000  loss_labels: 6.3455 (6.3435)  loss: 6.3466 (6.3435)  time: 1.2347\n",
      "Epoch: [1]  [ 200/3336]  eta: 1:12:54  lr: 0.100000  loss_labels: 6.3209 (6.3325)  loss: 6.2963 (6.3325)  time: 1.3223\n",
      "Epoch: [1]  [ 300/3336]  eta: 1:09:17  lr: 0.100000  loss_labels: 6.3297 (6.3315)  loss: 6.3179 (6.3315)  time: 1.3058\n",
      "Epoch: [1]  [ 400/3336]  eta: 0:57:18  lr: 0.100000  loss_labels: 6.3062 (6.3253)  loss: 6.3156 (6.3253)  time: 0.3089\n",
      "Epoch: [1]  [ 500/3336]  eta: 0:47:13  lr: 0.100000  loss_labels: 6.2964 (6.3195)  loss: 6.3070 (6.3195)  time: 0.3199\n",
      "Epoch: [1]  [ 600/3336]  eta: 0:42:26  lr: 0.100000  loss_labels: 6.2911 (6.3156)  loss: 6.3152 (6.3156)  time: 0.3024\n",
      "Epoch: [1]  [ 700/3336]  eta: 0:36:58  lr: 0.100000  loss_labels: 6.2864 (6.3119)  loss: 6.2642 (6.3119)  time: 0.3026\n",
      "Epoch: [1]  [ 800/3336]  eta: 0:32:42  lr: 0.100000  loss_labels: 6.2859 (6.3082)  loss: 6.2915 (6.3082)  time: 0.3006\n",
      "Epoch: [1]  [ 900/3336]  eta: 0:31:28  lr: 0.100000  loss_labels: 6.2724 (6.3041)  loss: 6.2758 (6.3041)  time: 1.3201\n",
      "Epoch: [1]  [1000/3336]  eta: 0:32:21  lr: 0.100000  loss_labels: 6.2597 (6.3002)  loss: 6.2566 (6.3002)  time: 1.3329\n",
      "Epoch: [1]  [1100/3336]  eta: 0:32:39  lr: 0.100000  loss_labels: 6.2647 (6.2966)  loss: 6.2571 (6.2966)  time: 1.3447\n",
      "Epoch: [1]  [1200/3336]  eta: 0:32:35  lr: 0.100000  loss_labels: 6.2522 (6.2926)  loss: 6.2742 (6.2926)  time: 1.3111\n",
      "Epoch: [1]  [1300/3336]  eta: 0:32:08  lr: 0.100000  loss_labels: 6.2437 (6.2888)  loss: 6.2283 (6.2888)  time: 1.3061\n",
      "Epoch: [1]  [1400/3336]  eta: 0:31:22  lr: 0.100000  loss_labels: 6.2218 (6.2844)  loss: 6.2276 (6.2844)  time: 1.2681\n",
      "Epoch: [1]  [1500/3336]  eta: 0:30:25  lr: 0.100000  loss_labels: 6.2345 (6.2814)  loss: 6.2296 (6.2814)  time: 1.2703\n",
      "Epoch: [1]  [1600/3336]  eta: 0:28:12  lr: 0.100000  loss_labels: 6.2144 (6.2773)  loss: 6.2116 (6.2773)  time: 0.6218\n",
      "Epoch: [1]  [1700/3336]  eta: 0:25:42  lr: 0.100000  loss_labels: 6.2108 (6.2737)  loss: 6.2163 (6.2737)  time: 0.3288\n",
      "Epoch: [1]  [1800/3336]  eta: 0:23:34  lr: 0.100000  loss_labels: 6.2133 (6.2701)  loss: 6.1928 (6.2701)  time: 1.4630\n",
      "Epoch: [1]  [1900/3336]  eta: 0:21:22  lr: 0.100000  loss_labels: 6.2055 (6.2670)  loss: 6.1991 (6.2670)  time: 0.3269\n",
      "Epoch: [1]  [2000/3336]  eta: 0:19:15  lr: 0.100000  loss_labels: 6.2013 (6.2638)  loss: 6.1869 (6.2638)  time: 0.3380\n",
      "Epoch: [1]  [2100/3336]  eta: 0:17:23  lr: 0.100000  loss_labels: 6.1892 (6.2603)  loss: 6.1744 (6.2603)  time: 0.6700\n",
      "Epoch: [1]  [2200/3336]  eta: 0:16:23  lr: 0.100000  loss_labels: 6.1855 (6.2571)  loss: 6.1648 (6.2571)  time: 1.3101\n",
      "Epoch: [1]  [2300/3336]  eta: 0:15:17  lr: 0.100000  loss_labels: 6.1766 (6.2539)  loss: 6.1870 (6.2539)  time: 1.3226\n",
      "Epoch: [1]  [2400/3336]  eta: 0:14:09  lr: 0.100000  loss_labels: 6.1862 (6.2512)  loss: 6.1862 (6.2512)  time: 1.3269\n",
      "Epoch: [1]  [2500/3336]  eta: 0:12:55  lr: 0.100000  loss_labels: 6.1707 (6.2482)  loss: 6.1685 (6.2482)  time: 1.5671\n",
      "Epoch: [1]  [2600/3336]  eta: 0:11:34  lr: 0.100000  loss_labels: 6.1797 (6.2457)  loss: 6.2036 (6.2457)  time: 1.3183\n",
      "Epoch: [1]  [2700/3336]  eta: 0:10:11  lr: 0.100000  loss_labels: 6.1610 (6.2427)  loss: 6.1798 (6.2427)  time: 1.2948\n",
      "Epoch: [1]  [2800/3336]  eta: 0:08:39  lr: 0.100000  loss_labels: 6.1579 (6.2396)  loss: 6.1613 (6.2396)  time: 0.3942\n",
      "Epoch: [1]  [2900/3336]  eta: 0:06:52  lr: 0.100000  loss_labels: 6.1714 (6.2373)  loss: 6.1936 (6.2373)  time: 0.3742\n",
      "Epoch: [1]  [3000/3336]  eta: 0:05:11  lr: 0.100000  loss_labels: 6.1427 (6.2343)  loss: 6.1530 (6.2343)  time: 0.3279\n",
      "Epoch: [1]  [3100/3336]  eta: 0:03:36  lr: 0.100000  loss_labels: 6.1407 (6.2314)  loss: 6.1500 (6.2314)  time: 0.2924\n",
      "Epoch: [1]  [3200/3336]  eta: 0:02:02  lr: 0.100000  loss_labels: 6.1428 (6.2286)  loss: 6.1396 (6.2286)  time: 0.2988\n",
      "Epoch: [1]  [3300/3336]  eta: 0:00:31  lr: 0.100000  loss_labels: 6.1388 (6.2258)  loss: 6.1337 (6.2258)  time: 0.3064\n",
      "Epoch: [1]  [3335/3336]  eta: 0:00:00  lr: 0.100000  loss_labels: 6.1496 (6.2252)  loss: 6.1496 (6.2252)  time: 0.2936\n",
      "Epoch: [1] Total time: 0:48:37 (0.8745 s / it)\n",
      "Averaged stats: lr: 0.100000  loss_labels: 6.1496 (6.2252)  loss: 6.1496 (6.2252)\n",
      "Test:  [  0/131]  eta: 0:46:48  loss_labels: 5.6970 (5.6970)  loss: 5.6970 (5.6970)  time: 21.4379\n",
      "Test:  [100/131]  eta: 0:00:43  loss_labels: 6.1101 (6.1210)  loss: 6.3298 (6.1210)  time: 1.2074\n",
      "Test:  [130/131]  eta: 0:00:01  loss_labels: 6.2040 (6.1164)  loss: 6.0113 (6.1164)  time: 1.1910\n",
      "Test: Total time: 0:02:55 (1.3405 s / it)\n",
      "Averaged stats: loss_labels: 6.2040 (6.1164)  loss: 6.0113 (6.1164)\n",
      "acc: 0.01787964254617691\n",
      "top 1 and top 5 accuracies {'top1': 0.01787964240715186, 'top5': 0.06833863322733545, 'loss': tensor(0.0481, device='cuda:0')}\n",
      "Epoch: [2]  [   0/3336]  eta: 1 day, 17:09:54  lr: 0.100000  loss_labels: 6.1730 (6.1730)  loss: 6.1730 (6.1730)  time: 44.4229\n",
      "Epoch: [2]  [ 100/3336]  eta: 1:30:55  lr: 0.100000  loss_labels: 6.1301 (6.1338)  loss: 6.1447 (6.1338)  time: 1.2981\n",
      "Epoch: [2]  [ 200/3336]  eta: 1:20:15  lr: 0.100000  loss_labels: 6.1266 (6.1312)  loss: 6.1169 (6.1312)  time: 1.3025\n",
      "Epoch: [2]  [ 300/3336]  eta: 1:14:40  lr: 0.100000  loss_labels: 6.1185 (6.1263)  loss: 6.1136 (6.1263)  time: 1.5351\n",
      "Epoch: [2]  [ 400/3336]  eta: 1:11:22  lr: 0.100000  loss_labels: 6.1205 (6.1245)  loss: 6.1205 (6.1245)  time: 1.7539\n",
      "Epoch: [2]  [ 500/3336]  eta: 1:07:46  lr: 0.100000  loss_labels: 6.1058 (6.1200)  loss: 6.0933 (6.1200)  time: 1.2752\n",
      "Epoch: [2]  [ 600/3336]  eta: 0:56:51  lr: 0.100000  loss_labels: 6.0958 (6.1168)  loss: 6.0840 (6.1168)  time: 0.3260\n",
      "Epoch: [2]  [ 700/3336]  eta: 0:49:31  lr: 0.100000  loss_labels: 6.1135 (6.1162)  loss: 6.1048 (6.1162)  time: 0.4119\n",
      "Epoch: [2]  [ 800/3336]  eta: 0:45:11  lr: 0.100000  loss_labels: 6.0934 (6.1130)  loss: 6.0773 (6.1130)  time: 0.3767\n",
      "Epoch: [2]  [ 900/3336]  eta: 0:40:56  lr: 0.100000  loss_labels: 6.0949 (6.1109)  loss: 6.0927 (6.1109)  time: 0.4525\n",
      "Epoch: [2]  [1000/3336]  eta: 0:36:56  lr: 0.100000  loss_labels: 6.1007 (6.1102)  loss: 6.0933 (6.1102)  time: 0.3440\n",
      "Epoch: [2]  [1100/3336]  eta: 0:35:03  lr: 0.100000  loss_labels: 6.0883 (6.1085)  loss: 6.0850 (6.1085)  time: 1.3423\n",
      "Epoch: [2]  [1200/3336]  eta: 0:34:39  lr: 0.100000  loss_labels: 6.1030 (6.1080)  loss: 6.1297 (6.1080)  time: 1.3200\n",
      "Epoch: [2]  [1300/3336]  eta: 0:34:00  lr: 0.100000  loss_labels: 6.0918 (6.1072)  loss: 6.1472 (6.1072)  time: 1.2973\n",
      "Epoch: [2]  [1400/3336]  eta: 0:33:02  lr: 0.100000  loss_labels: 6.0848 (6.1058)  loss: 6.0797 (6.1058)  time: 1.3261\n",
      "Epoch: [2]  [1500/3336]  eta: 0:31:53  lr: 0.100000  loss_labels: 6.0851 (6.1046)  loss: 6.1054 (6.1046)  time: 1.3062\n",
      "Epoch: [2]  [1600/3336]  eta: 0:30:38  lr: 0.100000  loss_labels: 6.0906 (6.1042)  loss: 6.0843 (6.1042)  time: 1.3056\n",
      "Epoch: [2]  [1700/3336]  eta: 0:29:17  lr: 0.100000  loss_labels: 6.0915 (6.1035)  loss: 6.0908 (6.1035)  time: 1.3386\n",
      "Epoch: [2]  [1800/3336]  eta: 0:26:47  lr: 0.100000  loss_labels: 6.0688 (6.1017)  loss: 6.0658 (6.1017)  time: 0.2998\n",
      "Epoch: [2]  [1900/3336]  eta: 0:24:07  lr: 0.100000  loss_labels: 6.0748 (6.1003)  loss: 6.0737 (6.1003)  time: 0.3070\n",
      "Epoch: [2]  [2000/3336]  eta: 0:21:59  lr: 0.100000  loss_labels: 6.0726 (6.0989)  loss: 6.0685 (6.0989)  time: 0.2964\n",
      "Epoch: [2]  [2100/3336]  eta: 0:19:40  lr: 0.100000  loss_labels: 6.0663 (6.0974)  loss: 6.0641 (6.0974)  time: 0.3040\n",
      "Epoch: [2]  [2200/3336]  eta: 0:17:32  lr: 0.100000  loss_labels: 6.0542 (6.0953)  loss: 6.0593 (6.0953)  time: 0.3149\n",
      "Epoch: [2]  [2300/3336]  eta: 0:15:55  lr: 0.100000  loss_labels: 6.0605 (6.0935)  loss: 6.0616 (6.0935)  time: 1.3170\n",
      "Epoch: [2]  [2400/3336]  eta: 0:14:38  lr: 0.100000  loss_labels: 6.0607 (6.0923)  loss: 6.0832 (6.0923)  time: 1.3278\n",
      "Epoch: [2]  [2500/3336]  eta: 0:13:17  lr: 0.100000  loss_labels: 6.0472 (6.0906)  loss: 6.0417 (6.0906)  time: 1.3106\n",
      "Epoch: [2]  [2600/3336]  eta: 0:11:50  lr: 0.100000  loss_labels: 6.0399 (6.0888)  loss: 6.0640 (6.0888)  time: 1.2744\n",
      "Epoch: [2]  [2700/3336]  eta: 0:10:22  lr: 0.100000  loss_labels: 6.0436 (6.0873)  loss: 6.0401 (6.0873)  time: 1.3302\n",
      "Epoch: [2]  [2800/3336]  eta: 0:08:51  lr: 0.100000  loss_labels: 6.0531 (6.0861)  loss: 6.0481 (6.0861)  time: 1.3033\n",
      "Epoch: [2]  [2900/3336]  eta: 0:07:16  lr: 0.100000  loss_labels: 6.0542 (6.0853)  loss: 6.0601 (6.0853)  time: 1.3307\n",
      "Epoch: [2]  [3000/3336]  eta: 0:05:31  lr: 0.100000  loss_labels: 6.0445 (6.0840)  loss: 6.0747 (6.0840)  time: 0.3080\n",
      "Epoch: [2]  [3100/3336]  eta: 0:03:47  lr: 0.100000  loss_labels: 6.0499 (6.0830)  loss: 6.0490 (6.0830)  time: 0.3050\n",
      "Epoch: [2]  [3200/3336]  eta: 0:02:09  lr: 0.100000  loss_labels: 6.0298 (6.0815)  loss: 6.0383 (6.0815)  time: 0.3136\n",
      "Epoch: [2]  [3300/3336]  eta: 0:00:33  lr: 0.100000  loss_labels: 6.0332 (6.0802)  loss: 6.0475 (6.0802)  time: 0.3503\n",
      "Epoch: [2]  [3335/3336]  eta: 0:00:00  lr: 0.100000  loss_labels: 6.0391 (6.0796)  loss: 6.0230 (6.0796)  time: 0.3166\n",
      "Epoch: [2] Total time: 0:51:43 (0.9303 s / it)\n",
      "Averaged stats: lr: 0.100000  loss_labels: 6.0391 (6.0796)  loss: 6.0230 (6.0796)\n",
      "Test:  [  0/131]  eta: 0:40:06  loss_labels: 5.5732 (5.5732)  loss: 5.5732 (5.5732)  time: 18.3736\n",
      "Test:  [100/131]  eta: 0:00:21  loss_labels: 5.9847 (5.9723)  loss: 6.0562 (5.9723)  time: 1.2027\n",
      "Test:  [130/131]  eta: 0:00:00  loss_labels: 6.0216 (5.9456)  loss: 5.7801 (5.9456)  time: 1.1581\n",
      "Test: Total time: 0:01:46 (0.8139 s / it)\n",
      "Averaged stats: loss_labels: 6.0216 (5.9456)  loss: 5.7801 (5.9456)\n",
      "acc: 0.02441951259970665\n",
      "top 1 and top 5 accuracies {'top1': 0.024419511609767804, 'top5': 0.09239815203695927, 'loss': tensor(0.0467, device='cuda:0')}\n",
      "Epoch: [3]  [   0/3336]  eta: 1 day, 0:50:18  lr: 0.100000  loss_labels: 6.0272 (6.0272)  loss: 6.0272 (6.0272)  time: 26.8042\n",
      "Epoch: [3]  [ 100/3336]  eta: 1:22:48  lr: 0.100000  loss_labels: 6.0230 (6.0269)  loss: 6.0319 (6.0269)  time: 1.2802\n",
      "Epoch: [3]  [ 200/3336]  eta: 1:14:34  lr: 0.100000  loss_labels: 6.0258 (6.0271)  loss: 6.0200 (6.0271)  time: 1.3241\n",
      "Epoch: [3]  [ 300/3336]  eta: 1:10:16  lr: 0.100000  loss_labels: 6.0308 (6.0273)  loss: 6.0158 (6.0273)  time: 1.3366\n",
      "Epoch: [3]  [ 400/3336]  eta: 1:07:08  lr: 0.100000  loss_labels: 6.0273 (6.0278)  loss: 6.0186 (6.0278)  time: 1.3370\n",
      "Epoch: [3]  [ 500/3336]  eta: 1:04:23  lr: 0.100000  loss_labels: 6.0297 (6.0287)  loss: 6.0234 (6.0287)  time: 1.3211\n",
      "Epoch: [3]  [ 600/3336]  eta: 1:01:08  lr: 0.100000  loss_labels: 6.0354 (6.0298)  loss: 6.0593 (6.0298)  time: 0.9210\n",
      "Epoch: [3]  [ 700/3336]  eta: 0:52:25  lr: 0.100000  loss_labels: 6.0066 (6.0262)  loss: 5.9935 (6.0262)  time: 0.3136\n",
      "Epoch: [3]  [ 800/3336]  eta: 0:45:47  lr: 0.100000  loss_labels: 6.0114 (6.0243)  loss: 6.0024 (6.0243)  time: 0.3010\n",
      "Epoch: [3]  [ 900/3336]  eta: 0:41:46  lr: 0.100000  loss_labels: 5.9955 (6.0213)  loss: 5.9816 (6.0213)  time: 0.2890\n",
      "Epoch: [3]  [1000/3336]  eta: 0:37:13  lr: 0.100000  loss_labels: 6.0023 (6.0197)  loss: 6.0030 (6.0197)  time: 0.3108\n",
      "Epoch: [3]  [1100/3336]  eta: 0:33:27  lr: 0.100000  loss_labels: 6.0294 (6.0205)  loss: 6.0366 (6.0205)  time: 0.3157\n",
      "Epoch: [3]  [1200/3336]  eta: 0:32:11  lr: 0.100000  loss_labels: 6.0058 (6.0192)  loss: 6.0064 (6.0192)  time: 1.3228\n",
      "Epoch: [3]  [1300/3336]  eta: 0:31:47  lr: 0.100000  loss_labels: 5.9955 (6.0176)  loss: 5.9849 (6.0176)  time: 1.3323\n",
      "Epoch: [3]  [1400/3336]  eta: 0:31:08  lr: 0.100000  loss_labels: 6.0043 (6.0166)  loss: 6.0097 (6.0166)  time: 1.3241\n",
      "Epoch: [3]  [1500/3336]  eta: 0:30:15  lr: 0.100000  loss_labels: 5.9815 (6.0144)  loss: 5.9904 (6.0144)  time: 1.3279\n",
      "Epoch: [3]  [1600/3336]  eta: 0:29:14  lr: 0.100000  loss_labels: 5.9905 (6.0128)  loss: 5.9666 (6.0128)  time: 1.3506\n",
      "Epoch: [3]  [1700/3336]  eta: 0:28:03  lr: 0.100000  loss_labels: 5.9961 (6.0117)  loss: 5.9940 (6.0117)  time: 1.2971\n",
      "Epoch: [3]  [1800/3336]  eta: 0:26:45  lr: 0.100000  loss_labels: 5.9956 (6.0107)  loss: 5.9741 (6.0107)  time: 1.2914\n",
      "Epoch: [3]  [1900/3336]  eta: 0:24:13  lr: 0.100000  loss_labels: 5.9867 (6.0096)  loss: 5.9696 (6.0096)  time: 0.2921\n",
      "Epoch: [3]  [2000/3336]  eta: 0:21:45  lr: 0.100000  loss_labels: 5.9796 (6.0082)  loss: 5.9743 (6.0082)  time: 0.2849\n",
      "Epoch: [3]  [2100/3336]  eta: 0:19:45  lr: 0.100000  loss_labels: 5.9859 (6.0072)  loss: 6.0070 (6.0072)  time: 0.3149\n",
      "Epoch: [3]  [2200/3336]  eta: 0:17:35  lr: 0.100000  loss_labels: 5.9731 (6.0058)  loss: 5.9611 (6.0058)  time: 0.3128\n",
      "Epoch: [3]  [2300/3336]  eta: 0:15:33  lr: 0.100000  loss_labels: 5.9785 (6.0049)  loss: 5.9919 (6.0049)  time: 0.3036\n",
      "Epoch: [3]  [2400/3336]  eta: 0:13:56  lr: 0.100000  loss_labels: 5.9785 (6.0037)  loss: 5.9753 (6.0037)  time: 1.2930\n",
      "Epoch: [3]  [2500/3336]  eta: 0:12:40  lr: 0.100000  loss_labels: 5.9783 (6.0025)  loss: 5.9516 (6.0025)  time: 1.2834\n",
      "Epoch: [3]  [2600/3336]  eta: 0:11:20  lr: 0.100000  loss_labels: 5.9865 (6.0016)  loss: 5.9987 (6.0016)  time: 1.3027\n",
      "Epoch: [3]  [2700/3336]  eta: 0:09:57  lr: 0.100000  loss_labels: 5.9753 (6.0005)  loss: 5.9726 (6.0005)  time: 1.3134\n",
      "Epoch: [3]  [2800/3336]  eta: 0:08:30  lr: 0.100000  loss_labels: 5.9672 (5.9992)  loss: 5.9768 (5.9992)  time: 1.3552\n",
      "Epoch: [3]  [2900/3336]  eta: 0:07:01  lr: 0.100000  loss_labels: 5.9619 (5.9980)  loss: 5.9478 (5.9980)  time: 1.3201\n",
      "Epoch: [3]  [3000/3336]  eta: 0:05:28  lr: 0.100000  loss_labels: 5.9673 (5.9971)  loss: 5.9436 (5.9971)  time: 1.2973\n",
      "Epoch: [3]  [3100/3336]  eta: 0:03:48  lr: 0.100000  loss_labels: 5.9723 (5.9963)  loss: 5.9948 (5.9963)  time: 0.3054\n",
      "Epoch: [3]  [3200/3336]  eta: 0:02:08  lr: 0.100000  loss_labels: 5.9649 (5.9953)  loss: 5.9651 (5.9953)  time: 0.3090\n",
      "Epoch: [3]  [3300/3336]  eta: 0:00:33  lr: 0.100000  loss_labels: 5.9683 (5.9946)  loss: 5.9569 (5.9946)  time: 0.7270\n",
      "Epoch: [3]  [3335/3336]  eta: 0:00:00  lr: 0.100000  loss_labels: 5.9573 (5.9944)  loss: 5.9708 (5.9944)  time: 0.3022\n",
      "Epoch: [3] Total time: 0:51:40 (0.9293 s / it)\n",
      "Averaged stats: lr: 0.100000  loss_labels: 5.9573 (5.9944)  loss: 5.9708 (5.9944)\n",
      "Test:  [  0/131]  eta: 0:40:30  loss_labels: 5.5265 (5.5265)  loss: 5.5265 (5.5265)  time: 18.5547\n",
      "Test:  [100/131]  eta: 0:00:13  loss_labels: 5.9296 (5.9198)  loss: 6.0791 (5.9198)  time: 0.2430\n",
      "Test:  [130/131]  eta: 0:00:00  loss_labels: 5.9922 (5.9037)  loss: 5.6834 (5.9037)  time: 0.2528\n",
      "Test: Total time: 0:00:52 (0.3999 s / it)\n",
      "Averaged stats: loss_labels: 5.9922 (5.9037)  loss: 5.6834 (5.9037)\n",
      "acc: 0.025919482111930847\n",
      "top 1 and top 5 accuracies {'top1': 0.025919481610367792, 'top5': 0.09785804283914322, 'loss': tensor(0.0464, device='cuda:0')}\n",
      "Epoch: [4]  [   0/3336]  eta: 1 day, 0:20:52  lr: 0.100000  loss_labels: 5.9446 (5.9446)  loss: 5.9446 (5.9446)  time: 26.2748\n",
      "Epoch: [4]  [ 100/3336]  eta: 1:20:36  lr: 0.100000  loss_labels: 5.9617 (5.9581)  loss: 5.9487 (5.9581)  time: 1.2772\n",
      "Epoch: [4]  [ 200/3336]  eta: 1:12:37  lr: 0.100000  loss_labels: 5.9603 (5.9621)  loss: 5.9575 (5.9621)  time: 1.2684\n",
      "Epoch: [4]  [ 300/3336]  eta: 1:17:50  lr: 0.100000  loss_labels: 5.9515 (5.9598)  loss: 5.9214 (5.9598)  time: 1.3826\n",
      "Epoch: [4]  [ 400/3336]  eta: 1:12:25  lr: 0.100000  loss_labels: 5.9627 (5.9603)  loss: 5.9472 (5.9603)  time: 1.3262\n",
      "Epoch: [4]  [ 500/3336]  eta: 1:08:53  lr: 0.100000  loss_labels: 5.9558 (5.9594)  loss: 5.9677 (5.9594)  time: 1.6108\n",
      "Epoch: [4]  [ 600/3336]  eta: 1:09:37  lr: 0.100000  loss_labels: 5.9495 (5.9577)  loss: 5.9442 (5.9577)  time: 1.3201\n",
      "Epoch: [4]  [ 700/3336]  eta: 1:04:18  lr: 0.100000  loss_labels: 5.9549 (5.9565)  loss: 5.9495 (5.9565)  time: 0.3646\n",
      "Epoch: [4]  [ 800/3336]  eta: 0:56:24  lr: 0.100000  loss_labels: 5.9459 (5.9563)  loss: 5.9382 (5.9563)  time: 0.4680\n",
      "Epoch: [4]  [ 900/3336]  eta: 0:50:09  lr: 0.100000  loss_labels: 5.9429 (5.9553)  loss: 5.9426 (5.9553)  time: 0.4546\n",
      "Epoch: [4]  [1000/3336]  eta: 0:46:05  lr: 0.100000  loss_labels: 5.9455 (5.9545)  loss: 5.9141 (5.9545)  time: 0.4410\n",
      "Epoch: [4]  [1100/3336]  eta: 0:41:48  lr: 0.100000  loss_labels: 5.9353 (5.9535)  loss: 5.9356 (5.9535)  time: 0.6438\n",
      "Epoch: [4]  [1200/3336]  eta: 0:37:41  lr: 0.100000  loss_labels: 5.9466 (5.9524)  loss: 5.9518 (5.9524)  time: 0.3183\n",
      "Epoch: [4]  [1300/3336]  eta: 0:36:08  lr: 0.100000  loss_labels: 5.9447 (5.9519)  loss: 5.9340 (5.9519)  time: 1.3147\n",
      "Epoch: [4]  [1400/3336]  eta: 0:35:47  lr: 0.100000  loss_labels: 5.9287 (5.9508)  loss: 5.9256 (5.9508)  time: 1.3345\n",
      "Epoch: [4]  [1500/3336]  eta: 0:34:28  lr: 0.100000  loss_labels: 5.9286 (5.9497)  loss: 5.9265 (5.9497)  time: 1.3497\n",
      "Epoch: [4]  [1600/3336]  eta: 0:33:41  lr: 0.100000  loss_labels: 5.9244 (5.9479)  loss: 5.9196 (5.9479)  time: 1.2884\n",
      "Epoch: [4]  [1700/3336]  eta: 0:32:14  lr: 0.100000  loss_labels: 5.9380 (5.9472)  loss: 5.8887 (5.9472)  time: 1.9216\n",
      "Epoch: [4]  [1800/3336]  eta: 0:30:31  lr: 0.100000  loss_labels: 5.9279 (5.9456)  loss: 5.9339 (5.9456)  time: 1.2863\n",
      "Epoch: [4]  [1900/3336]  eta: 0:28:39  lr: 0.100000  loss_labels: 5.9207 (5.9444)  loss: 5.9070 (5.9444)  time: 1.3044\n",
      "Epoch: [4]  [2000/3336]  eta: 0:25:46  lr: 0.100000  loss_labels: 5.9223 (5.9436)  loss: 5.9252 (5.9436)  time: 0.2964\n",
      "Epoch: [4]  [2100/3336]  eta: 0:23:00  lr: 0.100000  loss_labels: 5.9337 (5.9432)  loss: 5.9207 (5.9432)  time: 0.3149\n",
      "Epoch: [4]  [2200/3336]  eta: 0:20:41  lr: 0.100000  loss_labels: 5.9201 (5.9423)  loss: 5.9338 (5.9423)  time: 0.2972\n",
      "Epoch: [4]  [2300/3336]  eta: 0:18:17  lr: 0.100000  loss_labels: 5.9256 (5.9414)  loss: 5.9056 (5.9414)  time: 0.3011\n",
      "Epoch: [4]  [2400/3336]  eta: 0:16:02  lr: 0.100000  loss_labels: 5.9057 (5.9402)  loss: 5.9021 (5.9402)  time: 0.3194\n",
      "Epoch: [4]  [2500/3336]  eta: 0:14:03  lr: 0.100000  loss_labels: 5.9041 (5.9387)  loss: 5.8928 (5.9387)  time: 1.3157\n",
      "Epoch: [4]  [2600/3336]  eta: 0:12:30  lr: 0.100000  loss_labels: 5.9098 (5.9377)  loss: 5.8952 (5.9377)  time: 1.3076\n",
      "Epoch: [4]  [2700/3336]  eta: 0:10:55  lr: 0.100000  loss_labels: 5.8999 (5.9364)  loss: 5.9217 (5.9364)  time: 1.3089\n",
      "Epoch: [4]  [2800/3336]  eta: 0:09:18  lr: 0.100000  loss_labels: 5.9148 (5.9356)  loss: 5.9093 (5.9356)  time: 1.3636\n",
      "Epoch: [4]  [2900/3336]  eta: 0:07:41  lr: 0.100000  loss_labels: 5.9058 (5.9346)  loss: 5.9114 (5.9346)  time: 1.2735\n",
      "Epoch: [4]  [3000/3336]  eta: 0:05:59  lr: 0.100000  loss_labels: 5.8969 (5.9335)  loss: 5.8845 (5.9335)  time: 1.3199\n",
      "Epoch: [4]  [3100/3336]  eta: 0:04:14  lr: 0.100000  loss_labels: 5.9051 (5.9326)  loss: 5.9187 (5.9326)  time: 1.3148\n",
      "Epoch: [4]  [3200/3336]  eta: 0:02:25  lr: 0.100000  loss_labels: 5.9065 (5.9319)  loss: 5.8774 (5.9319)  time: 0.2880\n",
      "Epoch: [4]  [3300/3336]  eta: 0:00:37  lr: 0.100000  loss_labels: 5.8975 (5.9310)  loss: 5.8943 (5.9310)  time: 0.2886\n",
      "Epoch: [4]  [3335/3336]  eta: 0:00:01  lr: 0.100000  loss_labels: 5.8943 (5.9307)  loss: 5.9311 (5.9307)  time: 0.3217\n",
      "Epoch: [4] Total time: 0:57:40 (1.0373 s / it)\n",
      "Averaged stats: lr: 0.100000  loss_labels: 5.8943 (5.9307)  loss: 5.9311 (5.9307)\n",
      "Test:  [  0/131]  eta: 0:44:34  loss_labels: 5.4857 (5.4857)  loss: 5.4857 (5.4857)  time: 20.4173\n",
      "Test:  [100/131]  eta: 0:00:31  loss_labels: 5.8531 (5.8740)  loss: 6.0624 (5.8740)  time: 0.3613\n",
      "Test:  [130/131]  eta: 0:00:00  loss_labels: 5.9680 (5.8796)  loss: 5.8393 (5.8796)  time: 0.3222\n",
      "Test: Total time: 0:01:53 (0.8651 s / it)\n",
      "Averaged stats: loss_labels: 5.9680 (5.8796)  loss: 5.8393 (5.8796)\n",
      "acc: 0.031619369983673096\n",
      "top 1 and top 5 accuracies {'top1': 0.03161936761264775, 'top5': 0.10565788684226315, 'loss': tensor(0.0462, device='cuda:0')}\n",
      "Epoch: [5]  [   0/3336]  eta: 1 day, 2:29:54  lr: 0.100000  loss_labels: 5.9696 (5.9696)  loss: 5.9696 (5.9696)  time: 28.5954\n",
      "Epoch: [5]  [ 100/3336]  eta: 0:33:52  lr: 0.100000  loss_labels: 5.9048 (5.9099)  loss: 5.8931 (5.9099)  time: 0.3680\n",
      "Epoch: [5]  [ 200/3336]  eta: 0:36:05  lr: 0.100000  loss_labels: 5.9080 (5.9085)  loss: 5.8821 (5.9085)  time: 1.2652\n",
      "Epoch: [5]  [ 300/3336]  eta: 0:44:55  lr: 0.100000  loss_labels: 5.8981 (5.9045)  loss: 5.8981 (5.9045)  time: 1.3167\n",
      "Epoch: [5]  [ 400/3336]  eta: 0:48:31  lr: 0.100000  loss_labels: 5.9008 (5.9038)  loss: 5.9078 (5.9038)  time: 1.3294\n",
      "Epoch: [5]  [ 500/3336]  eta: 0:53:16  lr: 0.100000  loss_labels: 5.8882 (5.9026)  loss: 5.9117 (5.9026)  time: 1.3168\n",
      "Epoch: [5]  [ 600/3336]  eta: 0:52:41  lr: 0.100000  loss_labels: 5.8812 (5.8992)  loss: 5.8763 (5.8992)  time: 1.3091\n",
      "Epoch: [5]  [ 700/3336]  eta: 0:52:14  lr: 0.100000  loss_labels: 5.8892 (5.8981)  loss: 5.8825 (5.8981)  time: 1.5222\n",
      "Epoch: [5]  [ 800/3336]  eta: 0:52:25  lr: 0.100000  loss_labels: 5.8803 (5.8962)  loss: 5.8766 (5.8962)  time: 1.2941\n",
      "Epoch: [5]  [ 900/3336]  eta: 0:47:42  lr: 0.100000  loss_labels: 5.8767 (5.8952)  loss: 5.8766 (5.8952)  time: 0.3483\n",
      "Epoch: [5]  [1000/3336]  eta: 0:42:32  lr: 0.100000  loss_labels: 5.8901 (5.8946)  loss: 5.8961 (5.8946)  time: 0.3598\n",
      "Epoch: [5]  [1100/3336]  eta: 0:39:00  lr: 0.100000  loss_labels: 5.8682 (5.8927)  loss: 5.8509 (5.8927)  time: 1.4125\n",
      "Epoch: [5]  [1200/3336]  eta: 0:35:15  lr: 0.100000  loss_labels: 5.8779 (5.8915)  loss: 5.8818 (5.8915)  time: 0.3178\n",
      "Epoch: [5]  [1300/3336]  eta: 0:31:51  lr: 0.100000  loss_labels: 5.9013 (5.8928)  loss: 5.8867 (5.8928)  time: 0.3209\n",
      "Epoch: [5]  [1400/3336]  eta: 0:28:52  lr: 0.100000  loss_labels: 5.8809 (5.8925)  loss: 5.8762 (5.8925)  time: 0.3225\n",
      "Epoch: [5]  [1500/3336]  eta: 0:28:03  lr: 0.100000  loss_labels: 5.8764 (5.8914)  loss: 5.8539 (5.8914)  time: 1.3453\n",
      "Epoch: [5]  [1600/3336]  eta: 0:27:14  lr: 0.100000  loss_labels: 5.8709 (5.8904)  loss: 5.8698 (5.8904)  time: 1.3174\n",
      "Epoch: [5]  [1700/3336]  eta: 0:26:14  lr: 0.100000  loss_labels: 5.8599 (5.8890)  loss: 5.8555 (5.8890)  time: 1.3132\n",
      "Epoch: [5]  [1800/3336]  eta: 0:25:07  lr: 0.100000  loss_labels: 5.8795 (5.8884)  loss: 5.9006 (5.8884)  time: 1.3040\n",
      "Epoch: [5]  [1900/3336]  eta: 0:23:51  lr: 0.100000  loss_labels: 5.8680 (5.8877)  loss: 5.8577 (5.8877)  time: 1.2584\n",
      "Epoch: [5]  [2000/3336]  eta: 0:22:32  lr: 0.100000  loss_labels: 5.8802 (5.8875)  loss: 5.8599 (5.8875)  time: 1.3081\n",
      "Epoch: [5]  [2100/3336]  eta: 0:20:55  lr: 0.100000  loss_labels: 5.8907 (5.8876)  loss: 5.8766 (5.8876)  time: 0.3727\n",
      "Epoch: [5]  [2200/3336]  eta: 0:18:38  lr: 0.100000  loss_labels: 5.8712 (5.8869)  loss: 5.8590 (5.8869)  time: 0.3321\n",
      "Epoch: [5]  [2300/3336]  eta: 0:16:30  lr: 0.100000  loss_labels: 5.8670 (5.8862)  loss: 5.8444 (5.8862)  time: 0.3326\n",
      "Epoch: [5]  [2400/3336]  eta: 0:14:40  lr: 0.100000  loss_labels: 5.8744 (5.8858)  loss: 5.8519 (5.8858)  time: 0.2972\n",
      "Epoch: [5]  [2500/3336]  eta: 0:12:44  lr: 0.100000  loss_labels: 5.8705 (5.8853)  loss: 5.8552 (5.8853)  time: 0.3188\n",
      "Epoch: [5]  [2600/3336]  eta: 0:10:56  lr: 0.100000  loss_labels: 5.8715 (5.8848)  loss: 5.8401 (5.8848)  time: 0.3235\n",
      "Epoch: [5]  [2700/3336]  eta: 0:09:27  lr: 0.100000  loss_labels: 5.8627 (5.8840)  loss: 5.8502 (5.8840)  time: 1.3094\n",
      "Epoch: [5]  [2800/3336]  eta: 0:08:05  lr: 0.100000  loss_labels: 5.8693 (5.8831)  loss: 5.8680 (5.8831)  time: 1.3103\n",
      "Epoch: [5]  [2900/3336]  eta: 0:06:40  lr: 0.100000  loss_labels: 5.8684 (5.8826)  loss: 5.8757 (5.8826)  time: 1.2746\n",
      "Epoch: [5]  [3000/3336]  eta: 0:05:12  lr: 0.100000  loss_labels: 5.8602 (5.8822)  loss: 5.8648 (5.8822)  time: 1.2689\n",
      "Epoch: [5]  [3100/3336]  eta: 0:03:42  lr: 0.100000  loss_labels: 5.8606 (5.8814)  loss: 5.8535 (5.8814)  time: 1.2993\n",
      "Epoch: [5]  [3200/3336]  eta: 0:02:09  lr: 0.100000  loss_labels: 5.8731 (5.8810)  loss: 5.8614 (5.8810)  time: 1.2756\n",
      "Epoch: [5]  [3300/3336]  eta: 0:00:34  lr: 0.100000  loss_labels: 5.8609 (5.8804)  loss: 5.8630 (5.8804)  time: 1.2952\n",
      "Epoch: [5]  [3335/3336]  eta: 0:00:00  lr: 0.100000  loss_labels: 5.8588 (5.8802)  loss: 5.8520 (5.8802)  time: 0.2986\n",
      "Epoch: [5] Total time: 0:53:22 (0.9600 s / it)\n",
      "Averaged stats: lr: 0.100000  loss_labels: 5.8588 (5.8802)  loss: 5.8520 (5.8802)\n",
      "Test:  [  0/131]  eta: 0:41:36  loss_labels: 5.6268 (5.6268)  loss: 5.6268 (5.6268)  time: 19.0568\n",
      "Test:  [100/131]  eta: 0:00:13  loss_labels: 5.7891 (5.7962)  loss: 5.9938 (5.7962)  time: 0.2415\n",
      "Test:  [130/131]  eta: 0:00:00  loss_labels: 5.8992 (5.7928)  loss: 5.6842 (5.7928)  time: 0.2532\n",
      "Test: Total time: 0:00:52 (0.4029 s / it)\n",
      "Averaged stats: loss_labels: 5.8992 (5.7928)  loss: 5.6842 (5.7928)\n",
      "acc: 0.03917921707034111\n",
      "top 1 and top 5 accuracies {'top1': 0.03917921641567169, 'top5': 0.13199736005279894, 'loss': tensor(0.0455, device='cuda:0')}\n",
      "Epoch: [6]  [   0/3336]  eta: 1 day, 1:09:06  lr: 0.100000  loss_labels: 5.9066 (5.9066)  loss: 5.9066 (5.9066)  time: 27.1423\n",
      "Epoch: [6]  [ 100/3336]  eta: 0:30:40  lr: 0.100000  loss_labels: 5.8385 (5.8412)  loss: 5.8326 (5.8412)  time: 0.3026\n",
      "Epoch: [6]  [ 200/3336]  eta: 0:22:52  lr: 0.100000  loss_labels: 5.8615 (5.8498)  loss: 5.8444 (5.8498)  time: 0.3064\n",
      "Epoch: [6]  [ 300/3336]  eta: 0:24:03  lr: 0.100000  loss_labels: 5.8478 (5.8512)  loss: 5.8572 (5.8512)  time: 1.2959\n",
      "Epoch: [6]  [ 400/3336]  eta: 0:32:56  lr: 0.100000  loss_labels: 5.8759 (5.8572)  loss: 5.8800 (5.8572)  time: 1.2710\n",
      "Epoch: [6]  [ 500/3336]  eta: 0:37:27  lr: 0.100000  loss_labels: 5.8483 (5.8558)  loss: 5.8555 (5.8558)  time: 1.2611\n",
      "Epoch: [6]  [ 600/3336]  eta: 0:39:54  lr: 0.100000  loss_labels: 5.8593 (5.8565)  loss: 5.8456 (5.8565)  time: 1.2803\n",
      "Epoch: [6]  [ 700/3336]  eta: 0:40:58  lr: 0.100000  loss_labels: 5.8418 (5.8555)  loss: 5.8405 (5.8555)  time: 1.2704\n",
      "Epoch: [6]  [ 800/3336]  eta: 0:41:15  lr: 0.100000  loss_labels: 5.8530 (5.8551)  loss: 5.8454 (5.8551)  time: 1.2674\n",
      "Epoch: [6]  [ 900/3336]  eta: 0:40:58  lr: 0.100000  loss_labels: 5.8461 (5.8538)  loss: 5.8565 (5.8538)  time: 1.2595\n",
      "Epoch: [6]  [1000/3336]  eta: 0:38:04  lr: 0.100000  loss_labels: 5.8426 (5.8530)  loss: 5.8254 (5.8530)  time: 0.3192\n",
      "Epoch: [6]  [1100/3336]  eta: 0:34:12  lr: 0.100000  loss_labels: 5.8436 (5.8527)  loss: 5.8390 (5.8527)  time: 0.3246\n",
      "Epoch: [6]  [1200/3336]  eta: 0:31:42  lr: 0.100000  loss_labels: 5.8443 (5.8524)  loss: 5.8480 (5.8524)  time: 0.9225\n",
      "Epoch: [6]  [1300/3336]  eta: 0:28:44  lr: 0.100000  loss_labels: 5.8495 (5.8523)  loss: 5.8728 (5.8523)  time: 0.3198\n",
      "Epoch: [6]  [1400/3336]  eta: 0:26:05  lr: 0.100000  loss_labels: 5.8715 (5.8541)  loss: 5.8513 (5.8541)  time: 0.3529\n",
      "Epoch: [6]  [1500/3336]  eta: 0:24:39  lr: 0.100000  loss_labels: 5.8420 (5.8533)  loss: 5.8473 (5.8533)  time: 1.2713\n",
      "Epoch: [6]  [1600/3336]  eta: 0:24:07  lr: 0.100000  loss_labels: 5.8518 (5.8532)  loss: 5.8382 (5.8532)  time: 1.2010\n",
      "Epoch: [6]  [1700/3336]  eta: 0:23:22  lr: 0.100000  loss_labels: 5.8504 (5.8528)  loss: 5.8348 (5.8528)  time: 1.2614\n",
      "Epoch: [6]  [1800/3336]  eta: 0:22:32  lr: 0.100000  loss_labels: 5.8503 (5.8529)  loss: 5.8512 (5.8529)  time: 1.2607\n",
      "Epoch: [6]  [1900/3336]  eta: 0:21:34  lr: 0.100000  loss_labels: 5.8516 (5.8526)  loss: 5.8573 (5.8526)  time: 1.2944\n",
      "Epoch: [6]  [2000/3336]  eta: 0:20:30  lr: 0.100000  loss_labels: 5.8379 (5.8521)  loss: 5.8437 (5.8521)  time: 1.2976\n",
      "Epoch: [6]  [2100/3336]  eta: 0:19:19  lr: 0.100000  loss_labels: 5.8406 (5.8514)  loss: 5.8326 (5.8514)  time: 1.3138\n",
      "Epoch: [6]  [2200/3336]  eta: 0:17:25  lr: 0.100000  loss_labels: 5.8265 (5.8507)  loss: 5.8287 (5.8507)  time: 0.2826\n",
      "Epoch: [6]  [2300/3336]  eta: 0:15:25  lr: 0.100000  loss_labels: 5.8439 (5.8501)  loss: 5.8094 (5.8501)  time: 0.3024\n",
      "Epoch: [6]  [2400/3336]  eta: 0:13:44  lr: 0.100000  loss_labels: 5.8300 (5.8497)  loss: 5.8787 (5.8497)  time: 0.4569\n",
      "Epoch: [6]  [2500/3336]  eta: 0:11:56  lr: 0.100000  loss_labels: 5.8335 (5.8490)  loss: 5.8335 (5.8490)  time: 0.3015\n",
      "Epoch: [6]  [2600/3336]  eta: 0:10:15  lr: 0.100000  loss_labels: 5.8394 (5.8486)  loss: 5.8421 (5.8486)  time: 0.2894\n"
     ]
    }
   ],
   "source": [
    "!python main.py --distributed 1 --save_model 0 --pool 'max' --dataset 'imagenet' --num_layers 6 --model 'blt_blt' --epochs 100 --lr .1 --loss_choice 'weighted' --loss_gamma 0 --recurrent_steps 10 --lr_drop 10 --port '12379' --run 'm' --batch_size 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works:\n",
    "\n",
    "!python main.py --distributed 1 --save_model 0 --pool 'blur' --dataset 'imagenet' --num_layers 6 --model 'blt_blt' --epochs 100 --lr .0005 --loss_choice 'weighted' --loss_gamma 0 --recurrent_steps 10 --lr_drop 100 --port '12379' --run 'm' --batch_size 128\n",
    "\n",
    "# 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "100%|██████████████████████████████████████| 1920/1920 [00:02<00:00, 685.75it/s]\n",
      "100%|█████████████████████████████████████| 1920/1920 [00:00<00:00, 1945.50it/s]\n",
      "Number of model parameters: 6305844\n",
      "blt(\n",
      "  (conv_input): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (maxpool_input): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (non_lin_0): ReLU(inplace=True)\n",
      "  (norm_0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (output_0): Identity()\n",
      "  (conv_0_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_0_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (non_lin_1): ReLU(inplace=True)\n",
      "  (norm_1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (output_1): Identity()\n",
      "  (conv_1_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_1_2): Sequential(\n",
      "    (blurpool): BlurPool(\n",
      "      (pad): ReflectionPad2d((1, 2, 1, 2))\n",
      "    )\n",
      "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (non_lin_2): ReLU(inplace=True)\n",
      "  (norm_2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (output_2): Identity()\n",
      "  (conv_2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_2_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (non_lin_3): ReLU(inplace=True)\n",
      "  (norm_3): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (output_3): Identity()\n",
      "  (conv_3_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_3_4): Sequential(\n",
      "    (blurpool): BlurPool(\n",
      "      (pad): ReflectionPad2d((1, 2, 1, 2))\n",
      "    )\n",
      "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (non_lin_4): ReLU(inplace=True)\n",
      "  (norm_4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (output_4): Identity()\n",
      "  (conv_4_4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv_4_5): Sequential(\n",
      "    (blurpool): BlurPool(\n",
      "      (pad): ReflectionPad2d((1, 2, 1, 2))\n",
      "    )\n",
      "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (non_lin_5): ReLU(inplace=True)\n",
      "  (norm_5): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (output_5): Identity()\n",
      "  (conv_5_5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (read_out): Sequential(\n",
      "    (maxpool): AdaptiveMaxPool2d(output_size=1)\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=512, out_features=2420, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['conv_input.weight', 'conv_input.bias', 'norm_0.weight', 'norm_0.bias', 'conv_0_0.weight', 'conv_0_0.bias', 'conv_0_1.weight', 'conv_0_1.bias', 'norm_1.weight', 'norm_1.bias', 'conv_1_1.weight', 'conv_1_1.bias', 'conv_1_2.conv.weight', 'conv_1_2.conv.bias', 'norm_2.weight', 'norm_2.bias', 'conv_2_2.weight', 'conv_2_2.bias', 'conv_2_3.weight', 'conv_2_3.bias', 'norm_3.weight', 'norm_3.bias', 'conv_3_3.weight', 'conv_3_3.bias', 'conv_3_4.conv.weight', 'conv_3_4.conv.bias', 'norm_4.weight', 'norm_4.bias', 'conv_4_4.weight', 'conv_4_4.bias', 'conv_4_5.conv.weight', 'conv_4_5.conv.bias', 'norm_5.weight', 'norm_5.bias', 'conv_5_5.weight', 'conv_5_5.bias', 'read_out.linear.weight', 'read_out.linear.bias']\n",
      "Start training\n",
      "Epoch: [0]  [    0/80149]  eta: 15:02:59  lr: 0.000500  loss_labels: 7.9596 (7.9596)  loss: 7.9596 (7.9596)  time: 0.6760\n",
      "Epoch: [0]  [  100/80149]  eta: 0:53:53  lr: 0.000500  loss_labels: 8.0715 (8.0851)  loss: 7.9502 (8.0851)  time: 0.0337\n",
      "Epoch: [0]  [  200/80149]  eta: 0:49:38  lr: 0.000500  loss_labels: 7.9420 (8.0487)  loss: 7.8669 (8.0487)  time: 0.0341\n",
      "Epoch: [0]  [  300/80149]  eta: 0:48:23  lr: 0.000500  loss_labels: 7.8242 (7.9877)  loss: 7.7794 (7.9877)  time: 0.0346\n",
      "Epoch: [0]  [  400/80149]  eta: 0:47:36  lr: 0.000500  loss_labels: 7.7354 (7.9323)  loss: 7.7201 (7.9323)  time: 0.0342\n",
      "Epoch: [0]  [  500/80149]  eta: 0:47:09  lr: 0.000500  loss_labels: 7.6513 (7.8815)  loss: 7.7404 (7.8815)  time: 0.0337\n",
      "Epoch: [0]  [  600/80149]  eta: 0:46:47  lr: 0.000500  loss_labels: 7.6757 (7.8545)  loss: 7.6030 (7.8545)  time: 0.0341\n",
      "Epoch: [0]  [  700/80149]  eta: 0:46:34  lr: 0.000500  loss_labels: 7.6002 (7.8190)  loss: 7.5709 (7.8190)  time: 0.0347\n",
      "Epoch: [0]  [  800/80149]  eta: 0:46:14  lr: 0.000500  loss_labels: 7.5179 (7.7791)  loss: 7.3965 (7.7791)  time: 0.0329\n",
      "Epoch: [0]  [  900/80149]  eta: 0:45:54  lr: 0.000500  loss_labels: 7.4341 (7.7406)  loss: 7.3006 (7.7406)  time: 0.0330\n",
      "Epoch: [0]  [ 1000/80149]  eta: 0:45:39  lr: 0.000500  loss_labels: 7.3802 (7.7079)  loss: 7.3204 (7.7079)  time: 0.0331\n",
      "Epoch: [0]  [ 1100/80149]  eta: 0:45:29  lr: 0.000500  loss_labels: 7.3806 (7.6779)  loss: 7.3254 (7.6779)  time: 0.0330\n",
      "Epoch: [0]  [ 1200/80149]  eta: 0:45:18  lr: 0.000500  loss_labels: 7.3552 (7.6498)  loss: 7.3417 (7.6498)  time: 0.0354\n",
      "Epoch: [0]  [ 1300/80149]  eta: 0:45:12  lr: 0.000500  loss_labels: 7.2395 (7.6211)  loss: 7.1369 (7.6211)  time: 0.0337\n",
      "Epoch: [0]  [ 1400/80149]  eta: 0:45:15  lr: 0.000500  loss_labels: 7.2399 (7.5918)  loss: 7.0395 (7.5918)  time: 0.0382\n",
      "Epoch: [0]  [ 1500/80149]  eta: 0:45:10  lr: 0.000500  loss_labels: 7.2327 (7.5687)  loss: 7.1855 (7.5687)  time: 0.0340\n",
      "Epoch: [0]  [ 1600/80149]  eta: 0:45:03  lr: 0.000500  loss_labels: 7.2377 (7.5484)  loss: 7.2377 (7.5484)  time: 0.0333\n",
      "Epoch: [0]  [ 1700/80149]  eta: 0:45:06  lr: 0.000500  loss_labels: 7.2171 (7.5292)  loss: 7.1891 (7.5292)  time: 0.0341\n",
      "Epoch: [0]  [ 1800/80149]  eta: 0:45:04  lr: 0.000500  loss_labels: 7.1910 (7.5104)  loss: 7.2063 (7.5104)  time: 0.0354\n",
      "Epoch: [0]  [ 1900/80149]  eta: 0:45:04  lr: 0.000500  loss_labels: 7.1854 (7.4945)  loss: 7.1803 (7.4945)  time: 0.0356\n",
      "Epoch: [0]  [ 2000/80149]  eta: 0:45:02  lr: 0.000500  loss_labels: 7.1682 (7.4789)  loss: 7.2560 (7.4789)  time: 0.0336\n",
      "Epoch: [0]  [ 2100/80149]  eta: 0:44:55  lr: 0.000500  loss_labels: 7.0982 (7.4630)  loss: 7.0502 (7.4630)  time: 0.0341\n",
      "Epoch: [0]  [ 2200/80149]  eta: 0:44:50  lr: 0.000500  loss_labels: 7.1564 (7.4503)  loss: 7.0576 (7.4503)  time: 0.0334\n",
      "Epoch: [0]  [ 2300/80149]  eta: 0:44:46  lr: 0.000500  loss_labels: 7.1321 (7.4359)  loss: 7.1348 (7.4359)  time: 0.0373\n",
      "Epoch: [0]  [ 2400/80149]  eta: 0:44:45  lr: 0.000500  loss_labels: 7.1431 (7.4238)  loss: 7.2010 (7.4238)  time: 0.0347\n",
      "Epoch: [0]  [ 2500/80149]  eta: 0:44:41  lr: 0.000500  loss_labels: 7.1692 (7.4139)  loss: 7.1873 (7.4139)  time: 0.0341\n",
      "Epoch: [0]  [ 2600/80149]  eta: 0:44:40  lr: 0.000500  loss_labels: 7.1380 (7.4040)  loss: 7.1304 (7.4040)  time: 0.0346\n",
      "Epoch: [0]  [ 2700/80149]  eta: 0:44:37  lr: 0.000500  loss_labels: 7.1764 (7.3963)  loss: 7.1605 (7.3963)  time: 0.0350\n",
      "Epoch: [0]  [ 2800/80149]  eta: 0:44:34  lr: 0.000500  loss_labels: 7.0713 (7.3854)  loss: 7.1079 (7.3854)  time: 0.0337\n",
      "Epoch: [0]  [ 2900/80149]  eta: 0:44:30  lr: 0.000500  loss_labels: 7.1156 (7.3752)  loss: 7.0183 (7.3752)  time: 0.0343\n",
      "Epoch: [0]  [ 3000/80149]  eta: 0:44:27  lr: 0.000500  loss_labels: 7.1840 (7.3684)  loss: 7.1607 (7.3684)  time: 0.0350\n",
      "Epoch: [0]  [ 3100/80149]  eta: 0:44:24  lr: 0.000500  loss_labels: 7.0965 (7.3601)  loss: 7.0616 (7.3601)  time: 0.0334\n",
      "Epoch: [0]  [ 3200/80149]  eta: 0:44:18  lr: 0.000500  loss_labels: 7.0985 (7.3521)  loss: 7.1554 (7.3521)  time: 0.0333\n",
      "Epoch: [0]  [ 3300/80149]  eta: 0:44:14  lr: 0.000500  loss_labels: 7.1200 (7.3453)  loss: 7.0339 (7.3453)  time: 0.0340\n",
      "Epoch: [0]  [ 3400/80149]  eta: 0:44:12  lr: 0.000500  loss_labels: 7.1496 (7.3390)  loss: 7.2193 (7.3390)  time: 0.0370\n",
      "Epoch: [0]  [ 3500/80149]  eta: 0:44:08  lr: 0.000500  loss_labels: 7.1440 (7.3333)  loss: 7.0952 (7.3333)  time: 0.0342\n",
      "Epoch: [0]  [ 3600/80149]  eta: 0:44:01  lr: 0.000500  loss_labels: 7.1277 (7.3273)  loss: 7.0952 (7.3273)  time: 0.0329\n",
      "Epoch: [0]  [ 3700/80149]  eta: 0:43:58  lr: 0.000500  loss_labels: 7.1180 (7.3218)  loss: 7.1119 (7.3218)  time: 0.0342\n",
      "Epoch: [0]  [ 3800/80149]  eta: 0:43:58  lr: 0.000500  loss_labels: 7.1127 (7.3167)  loss: 7.1304 (7.3167)  time: 0.0367\n",
      "Epoch: [0]  [ 3900/80149]  eta: 0:43:56  lr: 0.000500  loss_labels: 7.1084 (7.3112)  loss: 6.9932 (7.3112)  time: 0.0338\n",
      "Epoch: [0]  [ 4000/80149]  eta: 0:43:52  lr: 0.000500  loss_labels: 7.1215 (7.3066)  loss: 7.1087 (7.3066)  time: 0.0350\n",
      "Epoch: [0]  [ 4100/80149]  eta: 0:43:48  lr: 0.000500  loss_labels: 7.0488 (7.3016)  loss: 7.0603 (7.3016)  time: 0.0347\n",
      "Epoch: [0]  [ 4200/80149]  eta: 0:43:44  lr: 0.000500  loss_labels: 7.0799 (7.2971)  loss: 7.2476 (7.2971)  time: 0.0340\n",
      "Epoch: [0]  [ 4300/80149]  eta: 0:43:43  lr: 0.000500  loss_labels: 7.0992 (7.2931)  loss: 7.0222 (7.2931)  time: 0.0370\n",
      "Epoch: [0]  [ 4400/80149]  eta: 0:43:38  lr: 0.000500  loss_labels: 7.1029 (7.2892)  loss: 7.0658 (7.2892)  time: 0.0340\n",
      "Epoch: [0]  [ 4500/80149]  eta: 0:43:36  lr: 0.000500  loss_labels: 7.0966 (7.2849)  loss: 7.0608 (7.2849)  time: 0.0355\n",
      "Epoch: [0]  [ 4600/80149]  eta: 0:43:34  lr: 0.000500  loss_labels: 7.0918 (7.2806)  loss: 7.0055 (7.2806)  time: 0.0349\n",
      "Epoch: [0]  [ 4700/80149]  eta: 0:43:31  lr: 0.000500  loss_labels: 7.0774 (7.2760)  loss: 6.9466 (7.2760)  time: 0.0345\n",
      "Epoch: [0]  [ 4800/80149]  eta: 0:43:27  lr: 0.000500  loss_labels: 7.0810 (7.2724)  loss: 7.0810 (7.2724)  time: 0.0339\n",
      "Epoch: [0]  [ 4900/80149]  eta: 0:43:23  lr: 0.000500  loss_labels: 7.1442 (7.2700)  loss: 7.0470 (7.2700)  time: 0.0340\n",
      "Epoch: [0]  [ 5000/80149]  eta: 0:43:20  lr: 0.000500  loss_labels: 7.1049 (7.2670)  loss: 7.1598 (7.2670)  time: 0.0341\n",
      "Epoch: [0]  [ 5100/80149]  eta: 0:43:20  lr: 0.000500  loss_labels: 7.1096 (7.2634)  loss: 7.1122 (7.2634)  time: 0.0457\n",
      "Epoch: [0]  [ 5200/80149]  eta: 0:43:15  lr: 0.000500  loss_labels: 7.0783 (7.2601)  loss: 6.9990 (7.2601)  time: 0.0330\n",
      "Epoch: [0]  [ 5300/80149]  eta: 0:43:11  lr: 0.000500  loss_labels: 7.1066 (7.2574)  loss: 7.2094 (7.2574)  time: 0.0345\n",
      "Epoch: [0]  [ 5400/80149]  eta: 0:43:06  lr: 0.000500  loss_labels: 7.1106 (7.2545)  loss: 7.0917 (7.2545)  time: 0.0340\n",
      "Epoch: [0]  [ 5500/80149]  eta: 0:43:02  lr: 0.000500  loss_labels: 7.0807 (7.2515)  loss: 7.0069 (7.2515)  time: 0.0354\n",
      "Epoch: [0]  [ 5600/80149]  eta: 0:42:57  lr: 0.000500  loss_labels: 7.0975 (7.2485)  loss: 7.0275 (7.2485)  time: 0.0335\n",
      "Epoch: [0]  [ 5700/80149]  eta: 0:42:54  lr: 0.000500  loss_labels: 7.0704 (7.2453)  loss: 7.1367 (7.2453)  time: 0.0348\n",
      "Epoch: [0]  [ 5800/80149]  eta: 0:42:48  lr: 0.000500  loss_labels: 7.0880 (7.2428)  loss: 7.1288 (7.2428)  time: 0.0330\n",
      "Epoch: [0]  [ 5900/80149]  eta: 0:42:44  lr: 0.000500  loss_labels: 7.0394 (7.2395)  loss: 7.0523 (7.2395)  time: 0.0351\n",
      "Epoch: [0]  [ 6000/80149]  eta: 0:42:39  lr: 0.000500  loss_labels: 7.1172 (7.2373)  loss: 6.9864 (7.2373)  time: 0.0335\n",
      "Epoch: [0]  [ 6100/80149]  eta: 0:42:34  lr: 0.000500  loss_labels: 7.0873 (7.2348)  loss: 7.0544 (7.2348)  time: 0.0330\n",
      "Epoch: [0]  [ 6200/80149]  eta: 0:42:30  lr: 0.000500  loss_labels: 7.0911 (7.2323)  loss: 7.0891 (7.2323)  time: 0.0331\n",
      "Epoch: [0]  [ 6300/80149]  eta: 0:42:52  lr: 0.000500  loss_labels: 7.0698 (7.2297)  loss: 7.0698 (7.2297)  time: 0.0460\n",
      "Epoch: [0]  [ 6400/80149]  eta: 0:43:05  lr: 0.000500  loss_labels: 7.0171 (7.2271)  loss: 7.0165 (7.2271)  time: 0.0416\n",
      "Epoch: [0]  [ 6500/80149]  eta: 0:43:16  lr: 0.000500  loss_labels: 7.0759 (7.2250)  loss: 7.0262 (7.2250)  time: 0.0439\n",
      "Epoch: [0]  [ 6600/80149]  eta: 0:43:40  lr: 0.000500  loss_labels: 7.0819 (7.2227)  loss: 6.9811 (7.2227)  time: 0.0622\n",
      "Epoch: [0]  [ 6700/80149]  eta: 0:43:50  lr: 0.000500  loss_labels: 7.0747 (7.2204)  loss: 7.0335 (7.2204)  time: 0.0441\n",
      "Epoch: [0]  [ 6800/80149]  eta: 0:43:59  lr: 0.000500  loss_labels: 7.0711 (7.2183)  loss: 7.0491 (7.2183)  time: 0.0485\n",
      "Epoch: [0]  [ 6900/80149]  eta: 0:44:07  lr: 0.000500  loss_labels: 7.0526 (7.2159)  loss: 6.9891 (7.2159)  time: 0.0433\n",
      "Epoch: [0]  [ 7000/80149]  eta: 0:44:13  lr: 0.000500  loss_labels: 7.0290 (7.2134)  loss: 7.0783 (7.2134)  time: 0.0424\n",
      "Epoch: [0]  [ 7100/80149]  eta: 0:44:18  lr: 0.000500  loss_labels: 7.0685 (7.2114)  loss: 7.1273 (7.2114)  time: 0.0384\n",
      "Epoch: [0]  [ 7200/80149]  eta: 0:44:24  lr: 0.000500  loss_labels: 7.0227 (7.2090)  loss: 7.1156 (7.2090)  time: 0.0423\n",
      "Epoch: [0]  [ 7300/80149]  eta: 0:44:28  lr: 0.000500  loss_labels: 7.0759 (7.2072)  loss: 6.9724 (7.2072)  time: 0.0418\n",
      "Epoch: [0]  [ 7400/80149]  eta: 0:44:31  lr: 0.000500  loss_labels: 6.9995 (7.2045)  loss: 7.0829 (7.2045)  time: 0.0379\n",
      "Epoch: [0]  [ 7500/80149]  eta: 0:44:34  lr: 0.000500  loss_labels: 6.9967 (7.2020)  loss: 7.0398 (7.2020)  time: 0.0504\n",
      "Epoch: [0]  [ 7600/80149]  eta: 0:44:33  lr: 0.000500  loss_labels: 7.0789 (7.2003)  loss: 7.0631 (7.2003)  time: 0.0369\n",
      "Epoch: [0]  [ 7700/80149]  eta: 0:44:34  lr: 0.000500  loss_labels: 7.0302 (7.1982)  loss: 7.0095 (7.1982)  time: 0.0397\n",
      "Epoch: [0]  [ 7800/80149]  eta: 0:44:36  lr: 0.000500  loss_labels: 7.0665 (7.1968)  loss: 7.1846 (7.1968)  time: 0.0405\n",
      "Epoch: [0]  [ 7900/80149]  eta: 0:44:37  lr: 0.000500  loss_labels: 7.0953 (7.1953)  loss: 7.0845 (7.1953)  time: 0.0379\n",
      "Epoch: [0]  [ 8000/80149]  eta: 0:44:40  lr: 0.000500  loss_labels: 7.0341 (7.1935)  loss: 7.0465 (7.1935)  time: 0.0468\n",
      "Epoch: [0]  [ 8100/80149]  eta: 0:44:42  lr: 0.000500  loss_labels: 7.0460 (7.1918)  loss: 6.9704 (7.1918)  time: 0.0543\n",
      "Epoch: [0]  [ 8200/80149]  eta: 0:44:38  lr: 0.000500  loss_labels: 6.9987 (7.1892)  loss: 6.9289 (7.1892)  time: 0.0355\n",
      "Epoch: [0]  [ 8300/80149]  eta: 0:44:38  lr: 0.000500  loss_labels: 7.0335 (7.1874)  loss: 7.0070 (7.1874)  time: 0.0408\n",
      "Epoch: [0]  [ 8400/80149]  eta: 0:44:36  lr: 0.000500  loss_labels: 7.0020 (7.1853)  loss: 7.0426 (7.1853)  time: 0.0367\n",
      "Epoch: [0]  [ 8500/80149]  eta: 0:44:33  lr: 0.000500  loss_labels: 6.9951 (7.1834)  loss: 7.0648 (7.1834)  time: 0.0408\n",
      "Epoch: [0]  [ 8600/80149]  eta: 0:44:30  lr: 0.000500  loss_labels: 7.0500 (7.1818)  loss: 6.9611 (7.1818)  time: 0.0394\n",
      "Epoch: [0]  [ 8700/80149]  eta: 0:44:27  lr: 0.000500  loss_labels: 7.0143 (7.1803)  loss: 7.0047 (7.1803)  time: 0.0367\n",
      "Epoch: [0]  [ 8800/80149]  eta: 0:44:25  lr: 0.000500  loss_labels: 7.0243 (7.1788)  loss: 6.9410 (7.1788)  time: 0.0372\n",
      "Epoch: [0]  [ 8900/80149]  eta: 0:44:22  lr: 0.000500  loss_labels: 7.0400 (7.1774)  loss: 6.9335 (7.1774)  time: 0.0366\n",
      "Epoch: [0]  [ 9000/80149]  eta: 0:44:18  lr: 0.000500  loss_labels: 7.0001 (7.1754)  loss: 6.8810 (7.1754)  time: 0.0352\n",
      "Epoch: [0]  [ 9100/80149]  eta: 0:44:14  lr: 0.000500  loss_labels: 6.9888 (7.1734)  loss: 7.0436 (7.1734)  time: 0.0351\n",
      "Epoch: [0]  [ 9200/80149]  eta: 0:44:11  lr: 0.000500  loss_labels: 7.0530 (7.1718)  loss: 6.9751 (7.1718)  time: 0.0350\n",
      "Epoch: [0]  [ 9300/80149]  eta: 0:44:06  lr: 0.000500  loss_labels: 7.0108 (7.1700)  loss: 7.0158 (7.1700)  time: 0.0404\n",
      "Epoch: [0]  [ 9400/80149]  eta: 0:44:01  lr: 0.000500  loss_labels: 6.9726 (7.1681)  loss: 6.9300 (7.1681)  time: 0.0356\n",
      "Epoch: [0]  [ 9500/80149]  eta: 0:43:58  lr: 0.000500  loss_labels: 7.0063 (7.1665)  loss: 6.9160 (7.1665)  time: 0.0409\n",
      "Epoch: [0]  [ 9600/80149]  eta: 0:43:53  lr: 0.000500  loss_labels: 7.0191 (7.1648)  loss: 6.9073 (7.1648)  time: 0.0347\n",
      "Epoch: [0]  [ 9700/80149]  eta: 0:43:49  lr: 0.000500  loss_labels: 7.0184 (7.1632)  loss: 7.0824 (7.1632)  time: 0.0352\n",
      "Epoch: [0]  [ 9800/80149]  eta: 0:43:44  lr: 0.000500  loss_labels: 6.9617 (7.1608)  loss: 6.9375 (7.1608)  time: 0.0347\n",
      "Epoch: [0]  [ 9900/80149]  eta: 0:43:39  lr: 0.000500  loss_labels: 7.0364 (7.1595)  loss: 7.0047 (7.1595)  time: 0.0349\n",
      "Epoch: [0]  [10000/80149]  eta: 0:43:34  lr: 0.000500  loss_labels: 6.9172 (7.1574)  loss: 6.8877 (7.1574)  time: 0.0358\n",
      "Epoch: [0]  [10100/80149]  eta: 0:43:29  lr: 0.000500  loss_labels: 7.0005 (7.1560)  loss: 6.9727 (7.1560)  time: 0.0355\n",
      "Epoch: [0]  [10200/80149]  eta: 0:43:25  lr: 0.000500  loss_labels: 7.0201 (7.1547)  loss: 7.0582 (7.1547)  time: 0.0349\n",
      "Epoch: [0]  [10300/80149]  eta: 0:43:20  lr: 0.000500  loss_labels: 7.0558 (7.1535)  loss: 7.0706 (7.1535)  time: 0.0349\n",
      "Epoch: [0]  [10400/80149]  eta: 0:43:15  lr: 0.000500  loss_labels: 6.9752 (7.1519)  loss: 6.9878 (7.1519)  time: 0.0356\n",
      "Epoch: [0]  [10500/80149]  eta: 0:43:10  lr: 0.000500  loss_labels: 6.9520 (7.1502)  loss: 6.9157 (7.1502)  time: 0.0351\n",
      "Epoch: [0]  [10600/80149]  eta: 0:43:06  lr: 0.000500  loss_labels: 7.0279 (7.1488)  loss: 6.9671 (7.1488)  time: 0.0343\n",
      "Epoch: [0]  [10700/80149]  eta: 0:43:00  lr: 0.000500  loss_labels: 6.8992 (7.1468)  loss: 6.9704 (7.1468)  time: 0.0341\n",
      "Epoch: [0]  [10800/80149]  eta: 0:42:55  lr: 0.000500  loss_labels: 7.0123 (7.1455)  loss: 7.0357 (7.1455)  time: 0.0343\n",
      "Epoch: [0]  [10900/80149]  eta: 0:42:49  lr: 0.000500  loss_labels: 6.9940 (7.1444)  loss: 6.9894 (7.1444)  time: 0.0358\n",
      "Epoch: [0]  [11000/80149]  eta: 0:42:44  lr: 0.000500  loss_labels: 6.9873 (7.1432)  loss: 6.9219 (7.1432)  time: 0.0344\n",
      "Epoch: [0]  [11100/80149]  eta: 0:42:39  lr: 0.000500  loss_labels: 6.9617 (7.1416)  loss: 6.9111 (7.1416)  time: 0.0341\n",
      "Epoch: [0]  [11200/80149]  eta: 0:42:34  lr: 0.000500  loss_labels: 6.9784 (7.1400)  loss: 6.7783 (7.1400)  time: 0.0349\n",
      "Epoch: [0]  [11300/80149]  eta: 0:42:30  lr: 0.000500  loss_labels: 6.9933 (7.1386)  loss: 6.9933 (7.1386)  time: 0.0368\n",
      "Epoch: [0]  [11400/80149]  eta: 0:42:26  lr: 0.000500  loss_labels: 6.9518 (7.1371)  loss: 6.9644 (7.1371)  time: 0.0350\n",
      "Epoch: [0]  [11500/80149]  eta: 0:42:20  lr: 0.000500  loss_labels: 6.9759 (7.1359)  loss: 7.0155 (7.1359)  time: 0.0348\n",
      "Epoch: [0]  [11600/80149]  eta: 0:42:15  lr: 0.000500  loss_labels: 6.9478 (7.1345)  loss: 6.8637 (7.1345)  time: 0.0357\n",
      "Epoch: [0]  [11700/80149]  eta: 0:42:11  lr: 0.000500  loss_labels: 6.9739 (7.1329)  loss: 7.0624 (7.1329)  time: 0.0354\n",
      "Epoch: [0]  [11800/80149]  eta: 0:42:06  lr: 0.000500  loss_labels: 6.9558 (7.1313)  loss: 6.9246 (7.1313)  time: 0.0341\n",
      "Epoch: [0]  [11900/80149]  eta: 0:42:01  lr: 0.000500  loss_labels: 6.9376 (7.1296)  loss: 7.0202 (7.1296)  time: 0.0340\n",
      "Epoch: [0]  [12000/80149]  eta: 0:41:56  lr: 0.000500  loss_labels: 6.9999 (7.1285)  loss: 7.0701 (7.1285)  time: 0.0364\n",
      "Epoch: [0]  [12100/80149]  eta: 0:41:52  lr: 0.000500  loss_labels: 6.9430 (7.1274)  loss: 6.9408 (7.1274)  time: 0.0369\n",
      "Epoch: [0]  [12200/80149]  eta: 0:41:47  lr: 0.000500  loss_labels: 6.9330 (7.1261)  loss: 6.8185 (7.1261)  time: 0.0349\n",
      "Epoch: [0]  [12300/80149]  eta: 0:41:42  lr: 0.000500  loss_labels: 6.9782 (7.1247)  loss: 7.0261 (7.1247)  time: 0.0337\n",
      "Epoch: [0]  [12400/80149]  eta: 0:41:37  lr: 0.000500  loss_labels: 6.9607 (7.1235)  loss: 6.8881 (7.1235)  time: 0.0347\n",
      "Epoch: [0]  [12500/80149]  eta: 0:41:32  lr: 0.000500  loss_labels: 6.9641 (7.1223)  loss: 6.9630 (7.1223)  time: 0.0346\n",
      "Epoch: [0]  [12600/80149]  eta: 0:41:27  lr: 0.000500  loss_labels: 6.9742 (7.1211)  loss: 7.0465 (7.1211)  time: 0.0355\n",
      "Epoch: [0]  [12700/80149]  eta: 0:41:23  lr: 0.000500  loss_labels: 6.9367 (7.1196)  loss: 6.9726 (7.1196)  time: 0.0340\n",
      "Epoch: [0]  [12800/80149]  eta: 0:41:18  lr: 0.000500  loss_labels: 6.9479 (7.1185)  loss: 6.9425 (7.1185)  time: 0.0339\n",
      "Epoch: [0]  [12900/80149]  eta: 0:41:14  lr: 0.000500  loss_labels: 6.9663 (7.1173)  loss: 6.8677 (7.1173)  time: 0.0347\n",
      "Epoch: [0]  [13000/80149]  eta: 0:41:09  lr: 0.000500  loss_labels: 6.9481 (7.1160)  loss: 6.8865 (7.1160)  time: 0.0337\n",
      "Epoch: [0]  [13100/80149]  eta: 0:41:04  lr: 0.000500  loss_labels: 6.8913 (7.1143)  loss: 6.9417 (7.1143)  time: 0.0344\n",
      "Epoch: [0]  [13200/80149]  eta: 0:40:59  lr: 0.000500  loss_labels: 6.9593 (7.1131)  loss: 7.0387 (7.1131)  time: 0.0343\n",
      "Epoch: [0]  [13300/80149]  eta: 0:40:54  lr: 0.000500  loss_labels: 6.9279 (7.1118)  loss: 6.9167 (7.1118)  time: 0.0348\n",
      "Epoch: [0]  [13400/80149]  eta: 0:40:49  lr: 0.000500  loss_labels: 6.9222 (7.1103)  loss: 6.8198 (7.1103)  time: 0.0341\n",
      "Epoch: [0]  [13500/80149]  eta: 0:40:45  lr: 0.000500  loss_labels: 6.9763 (7.1092)  loss: 6.9170 (7.1092)  time: 0.0350\n",
      "Epoch: [0]  [13600/80149]  eta: 0:40:40  lr: 0.000500  loss_labels: 6.9183 (7.1079)  loss: 6.8178 (7.1079)  time: 0.0339\n",
      "Epoch: [0]  [13700/80149]  eta: 0:40:35  lr: 0.000500  loss_labels: 6.9393 (7.1068)  loss: 6.9199 (7.1068)  time: 0.0345\n",
      "Epoch: [0]  [13800/80149]  eta: 0:40:30  lr: 0.000500  loss_labels: 6.9259 (7.1054)  loss: 6.7602 (7.1054)  time: 0.0345\n",
      "Epoch: [0]  [13900/80149]  eta: 0:40:26  lr: 0.000500  loss_labels: 6.8969 (7.1041)  loss: 6.7629 (7.1041)  time: 0.0361\n",
      "Epoch: [0]  [14000/80149]  eta: 0:40:22  lr: 0.000500  loss_labels: 6.9734 (7.1031)  loss: 6.9677 (7.1031)  time: 0.0346\n",
      "Epoch: [0]  [14100/80149]  eta: 0:40:18  lr: 0.000500  loss_labels: 6.9354 (7.1019)  loss: 6.9490 (7.1019)  time: 0.0360\n",
      "Epoch: [0]  [14200/80149]  eta: 0:40:13  lr: 0.000500  loss_labels: 6.8904 (7.1007)  loss: 6.9357 (7.1007)  time: 0.0338\n",
      "Epoch: [0]  [14300/80149]  eta: 0:40:09  lr: 0.000500  loss_labels: 6.9280 (7.0995)  loss: 6.8664 (7.0995)  time: 0.0341\n",
      "Epoch: [0]  [14400/80149]  eta: 0:40:04  lr: 0.000500  loss_labels: 6.9669 (7.0985)  loss: 6.9401 (7.0985)  time: 0.0346\n",
      "Epoch: [0]  [14500/80149]  eta: 0:40:00  lr: 0.000500  loss_labels: 6.9224 (7.0973)  loss: 6.8461 (7.0973)  time: 0.0351\n",
      "Epoch: [0]  [14600/80149]  eta: 0:39:56  lr: 0.000500  loss_labels: 6.8539 (7.0959)  loss: 6.8717 (7.0959)  time: 0.0345\n",
      "Epoch: [0]  [14700/80149]  eta: 0:39:52  lr: 0.000500  loss_labels: 6.9108 (7.0947)  loss: 6.8841 (7.0947)  time: 0.0348\n",
      "Epoch: [0]  [14800/80149]  eta: 0:39:48  lr: 0.000500  loss_labels: 6.9203 (7.0934)  loss: 6.7746 (7.0934)  time: 0.0363\n",
      "Epoch: [0]  [14900/80149]  eta: 0:39:43  lr: 0.000500  loss_labels: 6.9042 (7.0922)  loss: 6.8090 (7.0922)  time: 0.0346\n",
      "Epoch: [0]  [15000/80149]  eta: 0:39:39  lr: 0.000500  loss_labels: 6.9152 (7.0909)  loss: 6.9588 (7.0909)  time: 0.0352\n",
      "Epoch: [0]  [15100/80149]  eta: 0:39:35  lr: 0.000500  loss_labels: 6.8578 (7.0893)  loss: 6.8645 (7.0893)  time: 0.0355\n",
      "Epoch: [0]  [15200/80149]  eta: 0:39:31  lr: 0.000500  loss_labels: 6.9397 (7.0882)  loss: 6.8901 (7.0882)  time: 0.0402\n",
      "Epoch: [0]  [15300/80149]  eta: 0:39:27  lr: 0.000500  loss_labels: 6.9208 (7.0869)  loss: 6.9577 (7.0869)  time: 0.0351\n",
      "Epoch: [0]  [15400/80149]  eta: 0:39:22  lr: 0.000500  loss_labels: 6.8869 (7.0856)  loss: 6.9026 (7.0856)  time: 0.0346\n",
      "Epoch: [0]  [15500/80149]  eta: 0:39:18  lr: 0.000500  loss_labels: 6.9381 (7.0847)  loss: 6.9790 (7.0847)  time: 0.0354\n",
      "Epoch: [0]  [15600/80149]  eta: 0:39:14  lr: 0.000500  loss_labels: 6.8968 (7.0835)  loss: 6.9400 (7.0835)  time: 0.0338\n",
      "Epoch: [0]  [15700/80149]  eta: 0:39:10  lr: 0.000500  loss_labels: 6.8814 (7.0822)  loss: 6.8979 (7.0822)  time: 0.0347\n",
      "Epoch: [0]  [15800/80149]  eta: 0:39:05  lr: 0.000500  loss_labels: 6.9001 (7.0809)  loss: 6.8852 (7.0809)  time: 0.0343\n",
      "Epoch: [0]  [15900/80149]  eta: 0:39:01  lr: 0.000500  loss_labels: 6.9073 (7.0798)  loss: 6.7686 (7.0798)  time: 0.0342\n",
      "Epoch: [0]  [16000/80149]  eta: 0:38:57  lr: 0.000500  loss_labels: 6.8826 (7.0785)  loss: 6.8594 (7.0785)  time: 0.0343\n",
      "Epoch: [0]  [16100/80149]  eta: 0:38:53  lr: 0.000500  loss_labels: 6.9438 (7.0776)  loss: 6.8007 (7.0776)  time: 0.0356\n",
      "Epoch: [0]  [16200/80149]  eta: 0:38:49  lr: 0.000500  loss_labels: 6.8764 (7.0764)  loss: 6.8764 (7.0764)  time: 0.0346\n",
      "Epoch: [0]  [16300/80149]  eta: 0:38:44  lr: 0.000500  loss_labels: 6.8717 (7.0752)  loss: 6.8660 (7.0752)  time: 0.0348\n",
      "Epoch: [0]  [16400/80149]  eta: 0:38:40  lr: 0.000500  loss_labels: 6.8302 (7.0740)  loss: 6.7957 (7.0740)  time: 0.0371\n",
      "Epoch: [0]  [16500/80149]  eta: 0:38:36  lr: 0.000500  loss_labels: 6.8920 (7.0729)  loss: 6.8774 (7.0729)  time: 0.0345\n",
      "Epoch: [0]  [16600/80149]  eta: 0:38:32  lr: 0.000500  loss_labels: 6.9057 (7.0720)  loss: 6.7360 (7.0720)  time: 0.0360\n",
      "Epoch: [0]  [16700/80149]  eta: 0:38:28  lr: 0.000500  loss_labels: 6.8636 (7.0708)  loss: 6.8105 (7.0708)  time: 0.0343\n",
      "Epoch: [0]  [16800/80149]  eta: 0:38:24  lr: 0.000500  loss_labels: 6.8546 (7.0696)  loss: 6.8313 (7.0696)  time: 0.0342\n",
      "Epoch: [0]  [16900/80149]  eta: 0:38:19  lr: 0.000500  loss_labels: 6.8989 (7.0688)  loss: 6.8781 (7.0688)  time: 0.0354\n",
      "Epoch: [0]  [17000/80149]  eta: 0:38:15  lr: 0.000500  loss_labels: 6.9065 (7.0679)  loss: 6.9184 (7.0679)  time: 0.0351\n",
      "Epoch: [0]  [17100/80149]  eta: 0:38:11  lr: 0.000500  loss_labels: 6.8585 (7.0668)  loss: 6.7692 (7.0668)  time: 0.0340\n",
      "Epoch: [0]  [17200/80149]  eta: 0:38:06  lr: 0.000500  loss_labels: 6.9190 (7.0659)  loss: 6.8781 (7.0659)  time: 0.0352\n",
      "Epoch: [0]  [17300/80149]  eta: 0:38:03  lr: 0.000500  loss_labels: 6.8383 (7.0648)  loss: 6.7352 (7.0648)  time: 0.0369\n",
      "Epoch: [0]  [17400/80149]  eta: 0:37:59  lr: 0.000500  loss_labels: 6.8857 (7.0638)  loss: 6.8475 (7.0638)  time: 0.0355\n",
      "Epoch: [0]  [17500/80149]  eta: 0:37:55  lr: 0.000500  loss_labels: 6.8871 (7.0629)  loss: 6.8693 (7.0629)  time: 0.0354\n",
      "Epoch: [0]  [17600/80149]  eta: 0:37:51  lr: 0.000500  loss_labels: 6.8602 (7.0618)  loss: 6.8547 (7.0618)  time: 0.0351\n",
      "Epoch: [0]  [17700/80149]  eta: 0:37:48  lr: 0.000500  loss_labels: 6.8646 (7.0605)  loss: 6.8727 (7.0605)  time: 0.0350\n",
      "Epoch: [0]  [17800/80149]  eta: 0:37:43  lr: 0.000500  loss_labels: 6.8458 (7.0594)  loss: 6.8028 (7.0594)  time: 0.0349\n",
      "Epoch: [0]  [17900/80149]  eta: 0:37:39  lr: 0.000500  loss_labels: 6.8598 (7.0583)  loss: 6.9393 (7.0583)  time: 0.0346\n",
      "Epoch: [0]  [18000/80149]  eta: 0:37:35  lr: 0.000500  loss_labels: 6.8660 (7.0572)  loss: 6.8618 (7.0572)  time: 0.0342\n",
      "Epoch: [0]  [18100/80149]  eta: 0:37:31  lr: 0.000500  loss_labels: 6.8830 (7.0563)  loss: 6.9556 (7.0563)  time: 0.0371\n",
      "Epoch: [0]  [18200/80149]  eta: 0:37:27  lr: 0.000500  loss_labels: 6.8892 (7.0551)  loss: 6.6428 (7.0551)  time: 0.0348\n",
      "Epoch: [0]  [18300/80149]  eta: 0:37:23  lr: 0.000500  loss_labels: 6.8174 (7.0543)  loss: 6.7713 (7.0543)  time: 0.0340\n",
      "Epoch: [0]  [18400/80149]  eta: 0:37:19  lr: 0.000500  loss_labels: 6.8349 (7.0531)  loss: 6.7407 (7.0531)  time: 0.0364\n",
      "Epoch: [0]  [18500/80149]  eta: 0:37:16  lr: 0.000500  loss_labels: 6.8610 (7.0519)  loss: 6.8277 (7.0519)  time: 0.0354\n",
      "Epoch: [0]  [18600/80149]  eta: 0:37:12  lr: 0.000500  loss_labels: 6.7940 (7.0506)  loss: 6.8609 (7.0506)  time: 0.0351\n",
      "Epoch: [0]  [18700/80149]  eta: 0:37:08  lr: 0.000500  loss_labels: 6.7814 (7.0493)  loss: 6.7325 (7.0493)  time: 0.0351\n",
      "Epoch: [0]  [18800/80149]  eta: 0:37:04  lr: 0.000500  loss_labels: 6.8413 (7.0482)  loss: 6.8682 (7.0482)  time: 0.0354\n",
      "Epoch: [0]  [18900/80149]  eta: 0:37:00  lr: 0.000500  loss_labels: 6.8696 (7.0471)  loss: 6.9777 (7.0471)  time: 0.0354\n",
      "Epoch: [0]  [19000/80149]  eta: 0:36:59  lr: 0.000500  loss_labels: 6.8545 (7.0459)  loss: 6.8401 (7.0459)  time: 0.0350\n",
      "Epoch: [0]  [19100/80149]  eta: 0:36:55  lr: 0.000500  loss_labels: 6.8269 (7.0447)  loss: 6.8051 (7.0447)  time: 0.0348\n",
      "Epoch: [0]  [19200/80149]  eta: 0:36:51  lr: 0.000500  loss_labels: 6.8491 (7.0437)  loss: 6.6943 (7.0437)  time: 0.0353\n",
      "Epoch: [0]  [19300/80149]  eta: 0:36:48  lr: 0.000500  loss_labels: 6.8195 (7.0425)  loss: 6.7410 (7.0425)  time: 0.0354\n",
      "Epoch: [0]  [19400/80149]  eta: 0:36:44  lr: 0.000500  loss_labels: 6.8010 (7.0413)  loss: 6.8268 (7.0413)  time: 0.0357\n",
      "Epoch: [0]  [19500/80149]  eta: 0:36:40  lr: 0.000500  loss_labels: 6.8496 (7.0403)  loss: 6.8496 (7.0403)  time: 0.0355\n",
      "Epoch: [0]  [19600/80149]  eta: 0:36:36  lr: 0.000500  loss_labels: 6.8049 (7.0393)  loss: 6.6545 (7.0393)  time: 0.0353\n",
      "Epoch: [0]  [19700/80149]  eta: 0:36:32  lr: 0.000500  loss_labels: 6.8060 (7.0382)  loss: 6.8112 (7.0382)  time: 0.0351\n",
      "Epoch: [0]  [19800/80149]  eta: 0:36:28  lr: 0.000500  loss_labels: 6.8277 (7.0371)  loss: 6.7969 (7.0371)  time: 0.0351\n",
      "Epoch: [0]  [19900/80149]  eta: 0:36:25  lr: 0.000500  loss_labels: 6.8940 (7.0364)  loss: 6.9819 (7.0364)  time: 0.0351\n",
      "Epoch: [0]  [20000/80149]  eta: 0:36:21  lr: 0.000500  loss_labels: 6.7802 (7.0352)  loss: 6.6357 (7.0352)  time: 0.0354\n",
      "Epoch: [0]  [20100/80149]  eta: 0:36:17  lr: 0.000500  loss_labels: 6.8591 (7.0344)  loss: 6.9148 (7.0344)  time: 0.0351\n",
      "Epoch: [0]  [20200/80149]  eta: 0:36:13  lr: 0.000500  loss_labels: 6.8268 (7.0336)  loss: 6.7467 (7.0336)  time: 0.0356\n",
      "Epoch: [0]  [20300/80149]  eta: 0:36:09  lr: 0.000500  loss_labels: 6.8557 (7.0327)  loss: 6.9442 (7.0327)  time: 0.0358\n",
      "Epoch: [0]  [20400/80149]  eta: 0:36:06  lr: 0.000500  loss_labels: 6.8732 (7.0320)  loss: 6.8884 (7.0320)  time: 0.0390\n",
      "Epoch: [0]  [20500/80149]  eta: 0:36:02  lr: 0.000500  loss_labels: 6.8310 (7.0309)  loss: 6.9064 (7.0309)  time: 0.0360\n",
      "Epoch: [0]  [20600/80149]  eta: 0:35:58  lr: 0.000500  loss_labels: 6.8187 (7.0300)  loss: 6.7095 (7.0300)  time: 0.0372\n",
      "Epoch: [0]  [20700/80149]  eta: 0:35:55  lr: 0.000500  loss_labels: 6.7514 (7.0287)  loss: 6.6963 (7.0287)  time: 0.0351\n",
      "Epoch: [0]  [20800/80149]  eta: 0:35:51  lr: 0.000500  loss_labels: 6.7804 (7.0276)  loss: 6.7664 (7.0276)  time: 0.0347\n",
      "Epoch: [0]  [20900/80149]  eta: 0:35:47  lr: 0.000500  loss_labels: 6.8531 (7.0268)  loss: 6.8029 (7.0268)  time: 0.0349\n",
      "Epoch: [0]  [21000/80149]  eta: 0:35:43  lr: 0.000500  loss_labels: 6.8216 (7.0257)  loss: 6.8349 (7.0257)  time: 0.0365\n",
      "Epoch: [0]  [21100/80149]  eta: 0:35:39  lr: 0.000500  loss_labels: 6.8607 (7.0248)  loss: 6.6883 (7.0248)  time: 0.0375\n",
      "Epoch: [0]  [21200/80149]  eta: 0:35:35  lr: 0.000500  loss_labels: 6.8683 (7.0241)  loss: 6.8456 (7.0241)  time: 0.0363\n",
      "Epoch: [0]  [21300/80149]  eta: 0:35:31  lr: 0.000500  loss_labels: 6.8119 (7.0232)  loss: 6.6916 (7.0232)  time: 0.0354\n",
      "Epoch: [0]  [21400/80149]  eta: 0:35:27  lr: 0.000500  loss_labels: 6.7350 (7.0220)  loss: 6.7672 (7.0220)  time: 0.0352\n",
      "Epoch: [0]  [21500/80149]  eta: 0:35:24  lr: 0.000500  loss_labels: 6.8161 (7.0213)  loss: 6.8388 (7.0213)  time: 0.0354\n",
      "Epoch: [0]  [21600/80149]  eta: 0:35:20  lr: 0.000500  loss_labels: 6.8427 (7.0205)  loss: 6.8142 (7.0205)  time: 0.0351\n",
      "Epoch: [0]  [21700/80149]  eta: 0:35:16  lr: 0.000500  loss_labels: 6.7991 (7.0196)  loss: 6.7579 (7.0196)  time: 0.0358\n",
      "Epoch: [0]  [21800/80149]  eta: 0:35:12  lr: 0.000500  loss_labels: 6.7806 (7.0185)  loss: 6.9268 (7.0185)  time: 0.0356\n",
      "Epoch: [0]  [21900/80149]  eta: 0:35:09  lr: 0.000500  loss_labels: 6.7488 (7.0172)  loss: 6.6698 (7.0172)  time: 0.0350\n",
      "Epoch: [0]  [22000/80149]  eta: 0:35:05  lr: 0.000500  loss_labels: 6.7315 (7.0160)  loss: 6.8194 (7.0160)  time: 0.0350\n",
      "Epoch: [0]  [22100/80149]  eta: 0:35:01  lr: 0.000500  loss_labels: 6.7721 (7.0151)  loss: 6.8216 (7.0151)  time: 0.0349\n",
      "Epoch: [0]  [22200/80149]  eta: 0:34:58  lr: 0.000500  loss_labels: 6.8418 (7.0141)  loss: 6.8197 (7.0141)  time: 0.0395\n",
      "Epoch: [0]  [22300/80149]  eta: 0:34:54  lr: 0.000500  loss_labels: 6.7431 (7.0129)  loss: 6.8223 (7.0129)  time: 0.0351\n",
      "Epoch: [0]  [22400/80149]  eta: 0:34:50  lr: 0.000500  loss_labels: 6.7512 (7.0117)  loss: 6.6707 (7.0117)  time: 0.0344\n",
      "Epoch: [0]  [22500/80149]  eta: 0:34:46  lr: 0.000500  loss_labels: 6.8562 (7.0109)  loss: 6.8473 (7.0109)  time: 0.0367\n",
      "Epoch: [0]  [22600/80149]  eta: 0:34:42  lr: 0.000500  loss_labels: 6.7366 (7.0098)  loss: 6.7245 (7.0098)  time: 0.0347\n",
      "Epoch: [0]  [22700/80149]  eta: 0:34:38  lr: 0.000500  loss_labels: 6.6969 (7.0085)  loss: 6.5686 (7.0085)  time: 0.0349\n",
      "Epoch: [0]  [22800/80149]  eta: 0:34:35  lr: 0.000500  loss_labels: 6.8016 (7.0076)  loss: 6.7811 (7.0076)  time: 0.0345\n",
      "Epoch: [0]  [22900/80149]  eta: 0:34:31  lr: 0.000500  loss_labels: 6.7722 (7.0066)  loss: 6.7367 (7.0066)  time: 0.0330\n",
      "Epoch: [0]  [23000/80149]  eta: 0:34:26  lr: 0.000500  loss_labels: 6.8240 (7.0056)  loss: 6.8655 (7.0056)  time: 0.0339\n",
      "Epoch: [0]  [23100/80149]  eta: 0:34:22  lr: 0.000500  loss_labels: 6.7462 (7.0045)  loss: 6.7441 (7.0045)  time: 0.0330\n",
      "Epoch: [0]  [23200/80149]  eta: 0:34:18  lr: 0.000500  loss_labels: 6.7940 (7.0037)  loss: 6.8065 (7.0037)  time: 0.0333\n",
      "Epoch: [0]  [23300/80149]  eta: 0:34:13  lr: 0.000500  loss_labels: 6.7363 (7.0026)  loss: 6.7729 (7.0026)  time: 0.0329\n",
      "Epoch: [0]  [23400/80149]  eta: 0:34:09  lr: 0.000500  loss_labels: 6.7313 (7.0013)  loss: 6.7602 (7.0013)  time: 0.0331\n",
      "Epoch: [0]  [23500/80149]  eta: 0:34:05  lr: 0.000500  loss_labels: 6.8238 (7.0005)  loss: 6.8260 (7.0005)  time: 0.0331\n",
      "Epoch: [0]  [23600/80149]  eta: 0:34:00  lr: 0.000500  loss_labels: 6.6755 (6.9992)  loss: 6.7973 (6.9992)  time: 0.0331\n",
      "Epoch: [0]  [23700/80149]  eta: 0:33:56  lr: 0.000500  loss_labels: 6.7722 (6.9984)  loss: 6.7451 (6.9984)  time: 0.0330\n",
      "Epoch: [0]  [23800/80149]  eta: 0:33:52  lr: 0.000500  loss_labels: 6.7155 (6.9973)  loss: 6.6714 (6.9973)  time: 0.0336\n",
      "Epoch: [0]  [23900/80149]  eta: 0:33:48  lr: 0.000500  loss_labels: 6.6990 (6.9962)  loss: 6.7465 (6.9962)  time: 0.0360\n",
      "Epoch: [0]  [24000/80149]  eta: 0:33:43  lr: 0.000500  loss_labels: 6.8177 (6.9954)  loss: 6.8177 (6.9954)  time: 0.0335\n",
      "Epoch: [0]  [24100/80149]  eta: 0:33:40  lr: 0.000500  loss_labels: 6.7466 (6.9944)  loss: 6.6741 (6.9944)  time: 0.0341\n",
      "Epoch: [0]  [24200/80149]  eta: 0:33:35  lr: 0.000500  loss_labels: 6.7294 (6.9933)  loss: 6.8393 (6.9933)  time: 0.0332\n",
      "Epoch: [0]  [24300/80149]  eta: 0:33:31  lr: 0.000500  loss_labels: 6.7465 (6.9924)  loss: 6.7545 (6.9924)  time: 0.0338\n",
      "Epoch: [0]  [24400/80149]  eta: 0:33:27  lr: 0.000500  loss_labels: 6.7068 (6.9912)  loss: 6.6566 (6.9912)  time: 0.0332\n",
      "Epoch: [0]  [24500/80149]  eta: 0:33:23  lr: 0.000500  loss_labels: 6.7558 (6.9902)  loss: 6.7694 (6.9902)  time: 0.0332\n",
      "Epoch: [0]  [24600/80149]  eta: 0:33:19  lr: 0.000500  loss_labels: 6.7420 (6.9892)  loss: 6.7540 (6.9892)  time: 0.0337\n",
      "Epoch: [0]  [24700/80149]  eta: 0:33:15  lr: 0.000500  loss_labels: 6.7074 (6.9882)  loss: 6.7254 (6.9882)  time: 0.0346\n",
      "Epoch: [0]  [24800/80149]  eta: 0:33:11  lr: 0.000500  loss_labels: 6.7075 (6.9872)  loss: 6.6684 (6.9872)  time: 0.0340\n",
      "Epoch: [0]  [24900/80149]  eta: 0:33:07  lr: 0.000500  loss_labels: 6.7227 (6.9862)  loss: 6.7227 (6.9862)  time: 0.0353\n",
      "Epoch: [0]  [25000/80149]  eta: 0:33:03  lr: 0.000500  loss_labels: 6.7816 (6.9853)  loss: 6.7964 (6.9853)  time: 0.0376\n",
      "Epoch: [0]  [25100/80149]  eta: 0:32:59  lr: 0.000500  loss_labels: 6.6995 (6.9841)  loss: 6.6781 (6.9841)  time: 0.0349\n",
      "Epoch: [0]  [25200/80149]  eta: 0:32:56  lr: 0.000500  loss_labels: 6.7146 (6.9830)  loss: 6.7425 (6.9830)  time: 0.0347\n",
      "Epoch: [0]  [25300/80149]  eta: 0:32:52  lr: 0.000500  loss_labels: 6.7269 (6.9819)  loss: 6.7269 (6.9819)  time: 0.0344\n",
      "Epoch: [0]  [25400/80149]  eta: 0:32:48  lr: 0.000500  loss_labels: 6.6943 (6.9807)  loss: 6.6694 (6.9807)  time: 0.0347\n",
      "Epoch: [0]  [25500/80149]  eta: 0:32:44  lr: 0.000500  loss_labels: 6.6869 (6.9798)  loss: 6.6869 (6.9798)  time: 0.0342\n",
      "Epoch: [0]  [25600/80149]  eta: 0:32:40  lr: 0.000500  loss_labels: 6.7493 (6.9788)  loss: 6.9131 (6.9788)  time: 0.0339\n",
      "Epoch: [0]  [25700/80149]  eta: 0:32:36  lr: 0.000500  loss_labels: 6.7762 (6.9779)  loss: 6.5880 (6.9779)  time: 0.0348\n",
      "Epoch: [0]  [25800/80149]  eta: 0:32:33  lr: 0.000500  loss_labels: 6.6777 (6.9767)  loss: 6.6762 (6.9767)  time: 0.0352\n",
      "Epoch: [0]  [25900/80149]  eta: 0:32:29  lr: 0.000500  loss_labels: 6.7360 (6.9756)  loss: 6.5271 (6.9756)  time: 0.0353\n",
      "Epoch: [0]  [26000/80149]  eta: 0:32:25  lr: 0.000500  loss_labels: 6.7497 (6.9749)  loss: 6.7575 (6.9749)  time: 0.0357\n",
      "Epoch: [0]  [26100/80149]  eta: 0:32:21  lr: 0.000500  loss_labels: 6.7206 (6.9738)  loss: 6.7062 (6.9738)  time: 0.0349\n",
      "Epoch: [0]  [26200/80149]  eta: 0:32:18  lr: 0.000500  loss_labels: 6.7618 (6.9730)  loss: 6.7050 (6.9730)  time: 0.0342\n",
      "Epoch: [0]  [26300/80149]  eta: 0:32:14  lr: 0.000500  loss_labels: 6.7059 (6.9721)  loss: 6.6154 (6.9721)  time: 0.0355\n",
      "Epoch: [0]  [26400/80149]  eta: 0:32:10  lr: 0.000500  loss_labels: 6.7427 (6.9712)  loss: 6.8052 (6.9712)  time: 0.0350\n",
      "Epoch: [0]  [26500/80149]  eta: 0:32:07  lr: 0.000500  loss_labels: 6.6798 (6.9701)  loss: 6.6939 (6.9701)  time: 0.0356\n",
      "Epoch: [0]  [26600/80149]  eta: 0:32:03  lr: 0.000500  loss_labels: 6.6312 (6.9690)  loss: 6.6913 (6.9690)  time: 0.0355\n",
      "Epoch: [0]  [26700/80149]  eta: 0:31:59  lr: 0.000500  loss_labels: 6.7232 (6.9680)  loss: 6.6899 (6.9680)  time: 0.0335\n",
      "Epoch: [0]  [26800/80149]  eta: 0:31:55  lr: 0.000500  loss_labels: 6.6885 (6.9670)  loss: 6.7055 (6.9670)  time: 0.0335\n",
      "Epoch: [0]  [26900/80149]  eta: 0:31:51  lr: 0.000500  loss_labels: 6.7809 (6.9661)  loss: 6.7945 (6.9661)  time: 0.0337\n",
      "Epoch: [0]  [27000/80149]  eta: 0:31:47  lr: 0.000500  loss_labels: 6.6691 (6.9649)  loss: 6.7069 (6.9649)  time: 0.0336\n",
      "Epoch: [0]  [27100/80149]  eta: 0:31:43  lr: 0.000500  loss_labels: 6.6840 (6.9638)  loss: 6.6633 (6.9638)  time: 0.0341\n",
      "Epoch: [0]  [27200/80149]  eta: 0:31:39  lr: 0.000500  loss_labels: 6.6295 (6.9626)  loss: 6.4950 (6.9626)  time: 0.0354\n",
      "Epoch: [0]  [27300/80149]  eta: 0:31:35  lr: 0.000500  loss_labels: 6.7331 (6.9615)  loss: 6.6317 (6.9615)  time: 0.0334\n",
      "Epoch: [0]  [27400/80149]  eta: 0:31:31  lr: 0.000500  loss_labels: 6.6450 (6.9605)  loss: 6.6512 (6.9605)  time: 0.0333\n",
      "Epoch: [0]  [27500/80149]  eta: 0:31:27  lr: 0.000500  loss_labels: 6.6027 (6.9593)  loss: 6.5509 (6.9593)  time: 0.0341\n",
      "Epoch: [0]  [27600/80149]  eta: 0:31:23  lr: 0.000500  loss_labels: 6.6434 (6.9582)  loss: 6.7345 (6.9582)  time: 0.0343\n",
      "Epoch: [0]  [27700/80149]  eta: 0:31:19  lr: 0.000500  loss_labels: 6.6276 (6.9571)  loss: 6.4730 (6.9571)  time: 0.0343\n",
      "Epoch: [0]  [27800/80149]  eta: 0:31:15  lr: 0.000500  loss_labels: 6.6571 (6.9560)  loss: 6.7266 (6.9560)  time: 0.0349\n",
      "Epoch: [0]  [27900/80149]  eta: 0:31:12  lr: 0.000500  loss_labels: 6.7022 (6.9550)  loss: 6.6129 (6.9550)  time: 0.0349\n",
      "Epoch: [0]  [28000/80149]  eta: 0:31:08  lr: 0.000500  loss_labels: 6.7323 (6.9540)  loss: 6.7657 (6.9540)  time: 0.0357\n",
      "Epoch: [0]  [28100/80149]  eta: 0:31:04  lr: 0.000500  loss_labels: 6.6710 (6.9529)  loss: 6.7866 (6.9529)  time: 0.0363\n",
      "Epoch: [0]  [28200/80149]  eta: 0:31:01  lr: 0.000500  loss_labels: 6.6646 (6.9519)  loss: 6.6859 (6.9519)  time: 0.0364\n",
      "Epoch: [0]  [28300/80149]  eta: 0:30:57  lr: 0.000500  loss_labels: 6.6418 (6.9507)  loss: 6.5774 (6.9507)  time: 0.0347\n",
      "Epoch: [0]  [28400/80149]  eta: 0:30:54  lr: 0.000500  loss_labels: 6.6467 (6.9497)  loss: 6.5763 (6.9497)  time: 0.0357\n",
      "Epoch: [0]  [28500/80149]  eta: 0:30:50  lr: 0.000500  loss_labels: 6.6138 (6.9486)  loss: 6.3620 (6.9486)  time: 0.0347\n",
      "Epoch: [0]  [28600/80149]  eta: 0:30:46  lr: 0.000500  loss_labels: 6.5942 (6.9474)  loss: 6.5853 (6.9474)  time: 0.0351\n",
      "Epoch: [0]  [28700/80149]  eta: 0:30:42  lr: 0.000500  loss_labels: 6.7126 (6.9464)  loss: 6.6423 (6.9464)  time: 0.0349\n",
      "Epoch: [0]  [28800/80149]  eta: 0:30:39  lr: 0.000500  loss_labels: 6.5843 (6.9453)  loss: 6.5519 (6.9453)  time: 0.0349\n",
      "Epoch: [0]  [28900/80149]  eta: 0:30:35  lr: 0.000500  loss_labels: 6.5961 (6.9442)  loss: 6.6925 (6.9442)  time: 0.0346\n",
      "Epoch: [0]  [29000/80149]  eta: 0:30:31  lr: 0.000500  loss_labels: 6.6415 (6.9432)  loss: 6.6900 (6.9432)  time: 0.0355\n",
      "Epoch: [0]  [29100/80149]  eta: 0:30:28  lr: 0.000500  loss_labels: 6.6696 (6.9421)  loss: 6.6694 (6.9421)  time: 0.0355\n",
      "Epoch: [0]  [29200/80149]  eta: 0:30:24  lr: 0.000500  loss_labels: 6.6260 (6.9410)  loss: 6.6755 (6.9410)  time: 0.0362\n",
      "Epoch: [0]  [29300/80149]  eta: 0:30:21  lr: 0.000500  loss_labels: 6.6451 (6.9400)  loss: 6.5875 (6.9400)  time: 0.0348\n",
      "Epoch: [0]  [29400/80149]  eta: 0:30:17  lr: 0.000500  loss_labels: 6.5834 (6.9389)  loss: 6.6064 (6.9389)  time: 0.0352\n",
      "Epoch: [0]  [29500/80149]  eta: 0:30:13  lr: 0.000500  loss_labels: 6.6224 (6.9377)  loss: 6.5402 (6.9377)  time: 0.0353\n",
      "Epoch: [0]  [29600/80149]  eta: 0:30:09  lr: 0.000500  loss_labels: 6.6948 (6.9368)  loss: 6.7614 (6.9368)  time: 0.0354\n",
      "Epoch: [0]  [29700/80149]  eta: 0:30:06  lr: 0.000500  loss_labels: 6.6273 (6.9358)  loss: 6.6570 (6.9358)  time: 0.0350\n",
      "Epoch: [0]  [29800/80149]  eta: 0:30:02  lr: 0.000500  loss_labels: 6.6046 (6.9348)  loss: 6.6908 (6.9348)  time: 0.0346\n",
      "Epoch: [0]  [29900/80149]  eta: 0:29:58  lr: 0.000500  loss_labels: 6.5280 (6.9335)  loss: 6.3339 (6.9335)  time: 0.0354\n",
      "Epoch: [0]  [30000/80149]  eta: 0:29:54  lr: 0.000500  loss_labels: 6.6587 (6.9324)  loss: 6.6272 (6.9324)  time: 0.0342\n",
      "Epoch: [0]  [30100/80149]  eta: 0:29:51  lr: 0.000500  loss_labels: 6.6516 (6.9313)  loss: 6.6726 (6.9313)  time: 0.0356\n",
      "Epoch: [0]  [30200/80149]  eta: 0:29:48  lr: 0.000500  loss_labels: 6.6800 (6.9302)  loss: 6.7356 (6.9302)  time: 0.0708\n",
      "Epoch: [0]  [30300/80149]  eta: 0:29:45  lr: 0.000500  loss_labels: 6.6393 (6.9292)  loss: 6.5977 (6.9292)  time: 0.0356\n",
      "Epoch: [0]  [30400/80149]  eta: 0:29:41  lr: 0.000500  loss_labels: 6.5496 (6.9281)  loss: 6.4875 (6.9281)  time: 0.0350\n",
      "Epoch: [0]  [30500/80149]  eta: 0:29:37  lr: 0.000500  loss_labels: 6.6168 (6.9270)  loss: 6.5860 (6.9270)  time: 0.0349\n",
      "Epoch: [0]  [30600/80149]  eta: 0:29:34  lr: 0.000500  loss_labels: 6.5574 (6.9257)  loss: 6.4995 (6.9257)  time: 0.0351\n",
      "Epoch: [0]  [30700/80149]  eta: 0:29:30  lr: 0.000500  loss_labels: 6.5841 (6.9247)  loss: 6.5827 (6.9247)  time: 0.0332\n",
      "Epoch: [0]  [30800/80149]  eta: 0:29:26  lr: 0.000500  loss_labels: 6.5665 (6.9235)  loss: 6.4845 (6.9235)  time: 0.0332\n",
      "Epoch: [0]  [30900/80149]  eta: 0:29:22  lr: 0.000500  loss_labels: 6.5391 (6.9223)  loss: 6.6288 (6.9223)  time: 0.0340\n",
      "Epoch: [0]  [31000/80149]  eta: 0:29:18  lr: 0.000500  loss_labels: 6.6314 (6.9214)  loss: 6.5421 (6.9214)  time: 0.0333\n",
      "Epoch: [0]  [31100/80149]  eta: 0:29:14  lr: 0.000500  loss_labels: 6.5395 (6.9202)  loss: 6.5255 (6.9202)  time: 0.0333\n",
      "Epoch: [0]  [31200/80149]  eta: 0:29:10  lr: 0.000500  loss_labels: 6.5463 (6.9192)  loss: 6.5344 (6.9192)  time: 0.0338\n",
      "Epoch: [0]  [31300/80149]  eta: 0:29:07  lr: 0.000500  loss_labels: 6.6473 (6.9181)  loss: 6.7053 (6.9181)  time: 0.0349\n",
      "Epoch: [0]  [31400/80149]  eta: 0:29:05  lr: 0.000500  loss_labels: 6.6589 (6.9173)  loss: 6.7123 (6.9173)  time: 0.0365\n",
      "Epoch: [0]  [31500/80149]  eta: 0:29:01  lr: 0.000500  loss_labels: 6.6487 (6.9165)  loss: 6.7175 (6.9165)  time: 0.0363\n",
      "Epoch: [0]  [31600/80149]  eta: 0:28:58  lr: 0.000500  loss_labels: 6.5701 (6.9153)  loss: 6.6507 (6.9153)  time: 0.0364\n",
      "Epoch: [0]  [31700/80149]  eta: 0:28:54  lr: 0.000500  loss_labels: 6.5493 (6.9143)  loss: 6.5074 (6.9143)  time: 0.0363\n",
      "Epoch: [0]  [31800/80149]  eta: 0:28:52  lr: 0.000500  loss_labels: 6.5955 (6.9133)  loss: 6.6691 (6.9133)  time: 0.0776\n",
      "Epoch: [0]  [31900/80149]  eta: 0:28:48  lr: 0.000500  loss_labels: 6.6392 (6.9124)  loss: 6.5658 (6.9124)  time: 0.0338\n",
      "Epoch: [0]  [32000/80149]  eta: 0:28:44  lr: 0.000500  loss_labels: 6.5839 (6.9114)  loss: 6.4156 (6.9114)  time: 0.0340\n",
      "Epoch: [0]  [32100/80149]  eta: 0:28:41  lr: 0.000500  loss_labels: 6.5635 (6.9103)  loss: 6.6518 (6.9103)  time: 0.0384\n",
      "Epoch: [0]  [32200/80149]  eta: 0:28:37  lr: 0.000500  loss_labels: 6.6493 (6.9094)  loss: 6.7598 (6.9094)  time: 0.0364\n",
      "Epoch: [0]  [32300/80149]  eta: 0:28:34  lr: 0.000500  loss_labels: 6.6040 (6.9084)  loss: 6.6591 (6.9084)  time: 0.0345\n",
      "Epoch: [0]  [32400/80149]  eta: 0:28:30  lr: 0.000500  loss_labels: 6.5367 (6.9073)  loss: 6.5069 (6.9073)  time: 0.0344\n",
      "Epoch: [0]  [32500/80149]  eta: 0:28:27  lr: 0.000500  loss_labels: 6.6507 (6.9065)  loss: 6.6935 (6.9065)  time: 0.0792\n",
      "Epoch: [0]  [32600/80149]  eta: 0:28:24  lr: 0.000500  loss_labels: 6.5602 (6.9054)  loss: 6.5952 (6.9054)  time: 0.0352\n",
      "Epoch: [0]  [32700/80149]  eta: 0:28:20  lr: 0.000500  loss_labels: 6.5615 (6.9043)  loss: 6.3987 (6.9043)  time: 0.0380\n",
      "Epoch: [0]  [32800/80149]  eta: 0:28:16  lr: 0.000500  loss_labels: 6.5634 (6.9033)  loss: 6.6292 (6.9033)  time: 0.0351\n",
      "Epoch: [0]  [32900/80149]  eta: 0:28:13  lr: 0.000500  loss_labels: 6.5697 (6.9023)  loss: 6.5659 (6.9023)  time: 0.0363\n",
      "Epoch: [0]  [33000/80149]  eta: 0:28:09  lr: 0.000500  loss_labels: 6.5292 (6.9011)  loss: 6.4806 (6.9011)  time: 0.0356\n",
      "Epoch: [0]  [33100/80149]  eta: 0:28:05  lr: 0.000500  loss_labels: 6.5092 (6.9001)  loss: 6.6783 (6.9001)  time: 0.0337\n",
      "Epoch: [0]  [33200/80149]  eta: 0:28:02  lr: 0.000500  loss_labels: 6.5512 (6.8990)  loss: 6.5299 (6.8990)  time: 0.0356\n",
      "Epoch: [0]  [33300/80149]  eta: 0:27:58  lr: 0.000500  loss_labels: 6.5608 (6.8979)  loss: 6.5018 (6.8979)  time: 0.0344\n",
      "Epoch: [0]  [33400/80149]  eta: 0:27:54  lr: 0.000500  loss_labels: 6.5481 (6.8969)  loss: 6.5083 (6.8969)  time: 0.0374\n",
      "Epoch: [0]  [33500/80149]  eta: 0:27:51  lr: 0.000500  loss_labels: 6.5431 (6.8958)  loss: 6.5945 (6.8958)  time: 0.0359\n",
      "Epoch: [0]  [33600/80149]  eta: 0:27:47  lr: 0.000500  loss_labels: 6.5308 (6.8948)  loss: 6.6476 (6.8948)  time: 0.0361\n",
      "Epoch: [0]  [33700/80149]  eta: 0:27:44  lr: 0.000500  loss_labels: 6.5289 (6.8938)  loss: 6.5559 (6.8938)  time: 0.0348\n",
      "Epoch: [0]  [33800/80149]  eta: 0:27:40  lr: 0.000500  loss_labels: 6.4786 (6.8927)  loss: 6.4239 (6.8927)  time: 0.0357\n",
      "Epoch: [0]  [33900/80149]  eta: 0:27:36  lr: 0.000500  loss_labels: 6.5394 (6.8917)  loss: 6.5466 (6.8917)  time: 0.0356\n",
      "Epoch: [0]  [34000/80149]  eta: 0:27:34  lr: 0.000500  loss_labels: 6.4681 (6.8904)  loss: 6.3145 (6.8904)  time: 0.0744\n",
      "Epoch: [0]  [34100/80149]  eta: 0:27:30  lr: 0.000500  loss_labels: 6.5069 (6.8893)  loss: 6.3238 (6.8893)  time: 0.0362\n",
      "Epoch: [0]  [34200/80149]  eta: 0:27:27  lr: 0.000500  loss_labels: 6.4668 (6.8881)  loss: 6.4694 (6.8881)  time: 0.0395\n",
      "Epoch: [0]  [34300/80149]  eta: 0:27:24  lr: 0.000500  loss_labels: 6.4684 (6.8869)  loss: 6.4127 (6.8869)  time: 0.0363\n",
      "Epoch: [0]  [34400/80149]  eta: 0:27:20  lr: 0.000500  loss_labels: 6.4550 (6.8857)  loss: 6.4884 (6.8857)  time: 0.0358\n",
      "Epoch: [0]  [34500/80149]  eta: 0:27:16  lr: 0.000500  loss_labels: 6.5713 (6.8847)  loss: 6.5258 (6.8847)  time: 0.0366\n",
      "Epoch: [0]  [34600/80149]  eta: 0:27:13  lr: 0.000500  loss_labels: 6.4589 (6.8834)  loss: 6.4589 (6.8834)  time: 0.0366\n",
      "Epoch: [0]  [34700/80149]  eta: 0:27:10  lr: 0.000500  loss_labels: 6.5082 (6.8823)  loss: 6.3361 (6.8823)  time: 0.0358\n",
      "Epoch: [0]  [34800/80149]  eta: 0:27:06  lr: 0.000500  loss_labels: 6.4899 (6.8812)  loss: 6.4888 (6.8812)  time: 0.0363\n",
      "Epoch: [0]  [34900/80149]  eta: 0:27:03  lr: 0.000500  loss_labels: 6.4386 (6.8800)  loss: 6.3648 (6.8800)  time: 0.0347\n",
      "Epoch: [0]  [35000/80149]  eta: 0:27:00  lr: 0.000500  loss_labels: 6.5003 (6.8789)  loss: 6.4940 (6.8789)  time: 0.0351\n",
      "Epoch: [0]  [35100/80149]  eta: 0:26:57  lr: 0.000500  loss_labels: 6.3775 (6.8776)  loss: 6.3726 (6.8776)  time: 0.0338\n",
      "Epoch: [0]  [35200/80149]  eta: 0:26:53  lr: 0.000500  loss_labels: 6.5279 (6.8766)  loss: 6.5029 (6.8766)  time: 0.0346\n",
      "Epoch: [0]  [35300/80149]  eta: 0:26:50  lr: 0.000500  loss_labels: 6.3839 (6.8752)  loss: 6.4323 (6.8752)  time: 0.0337\n",
      "Epoch: [0]  [35400/80149]  eta: 0:26:46  lr: 0.000500  loss_labels: 6.4314 (6.8741)  loss: 6.3059 (6.8741)  time: 0.0344\n",
      "Epoch: [0]  [35500/80149]  eta: 0:26:43  lr: 0.000500  loss_labels: 6.4285 (6.8727)  loss: 6.5222 (6.8727)  time: 0.0370\n",
      "Epoch: [0]  [35600/80149]  eta: 0:26:41  lr: 0.000500  loss_labels: 6.4406 (6.8716)  loss: 6.4116 (6.8716)  time: 0.0389\n",
      "Epoch: [0]  [35700/80149]  eta: 0:26:37  lr: 0.000500  loss_labels: 6.4034 (6.8703)  loss: 6.3091 (6.8703)  time: 0.0361\n",
      "Epoch: [0]  [35800/80149]  eta: 0:26:34  lr: 0.000500  loss_labels: 6.4324 (6.8691)  loss: 6.4708 (6.8691)  time: 0.0355\n",
      "Epoch: [0]  [35900/80149]  eta: 0:26:30  lr: 0.000500  loss_labels: 6.4792 (6.8679)  loss: 6.3761 (6.8679)  time: 0.0352\n",
      "Epoch: [0]  [36000/80149]  eta: 0:26:26  lr: 0.000500  loss_labels: 6.3887 (6.8667)  loss: 6.2763 (6.8667)  time: 0.0339\n",
      "Epoch: [0]  [36100/80149]  eta: 0:26:23  lr: 0.000500  loss_labels: 6.3404 (6.8654)  loss: 6.2345 (6.8654)  time: 0.0340\n",
      "Epoch: [0]  [36200/80149]  eta: 0:26:19  lr: 0.000500  loss_labels: 6.4348 (6.8641)  loss: 6.4696 (6.8641)  time: 0.0337\n",
      "Epoch: [0]  [36300/80149]  eta: 0:26:16  lr: 0.000500  loss_labels: 6.4001 (6.8629)  loss: 6.3148 (6.8629)  time: 0.0351\n",
      "Epoch: [0]  [36400/80149]  eta: 0:26:13  lr: 0.000500  loss_labels: 6.3753 (6.8616)  loss: 6.3328 (6.8616)  time: 0.0361\n",
      "Epoch: [0]  [36500/80149]  eta: 0:26:10  lr: 0.000500  loss_labels: 6.4187 (6.8604)  loss: 6.2619 (6.8604)  time: 0.0362\n",
      "Epoch: [0]  [36600/80149]  eta: 0:26:07  lr: 0.000500  loss_labels: 6.4009 (6.8591)  loss: 6.2407 (6.8591)  time: 0.0817\n",
      "Epoch: [0]  [36700/80149]  eta: 0:26:04  lr: 0.000500  loss_labels: 6.4146 (6.8579)  loss: 6.5067 (6.8579)  time: 0.0361\n",
      "Epoch: [0]  [36800/80149]  eta: 0:26:00  lr: 0.000500  loss_labels: 6.3324 (6.8566)  loss: 6.1414 (6.8566)  time: 0.0361\n",
      "Epoch: [0]  [36900/80149]  eta: 0:25:56  lr: 0.000500  loss_labels: 6.3203 (6.8553)  loss: 6.2310 (6.8553)  time: 0.0358\n",
      "Epoch: [0]  [37000/80149]  eta: 0:25:53  lr: 0.000500  loss_labels: 6.2734 (6.8538)  loss: 6.2046 (6.8538)  time: 0.0344\n",
      "Epoch: [0]  [37100/80149]  eta: 0:25:49  lr: 0.000500  loss_labels: 6.3393 (6.8525)  loss: 6.0715 (6.8525)  time: 0.0368\n",
      "Epoch: [0]  [37200/80149]  eta: 0:25:46  lr: 0.000500  loss_labels: 6.3144 (6.8511)  loss: 6.1186 (6.8511)  time: 0.0365\n",
      "Epoch: [0]  [37300/80149]  eta: 0:25:42  lr: 0.000500  loss_labels: 6.3301 (6.8497)  loss: 6.3301 (6.8497)  time: 0.0361\n",
      "Epoch: [0]  [37400/80149]  eta: 0:25:39  lr: 0.000500  loss_labels: 6.3420 (6.8485)  loss: 6.3817 (6.8485)  time: 0.0350\n",
      "Epoch: [0]  [37500/80149]  eta: 0:25:35  lr: 0.000500  loss_labels: 6.2790 (6.8471)  loss: 6.3647 (6.8471)  time: 0.0368\n",
      "Epoch: [0]  [37600/80149]  eta: 0:25:32  lr: 0.000500  loss_labels: 6.3189 (6.8456)  loss: 6.3196 (6.8456)  time: 0.0338\n",
      "Epoch: [0]  [37700/80149]  eta: 0:25:28  lr: 0.000500  loss_labels: 6.2883 (6.8442)  loss: 6.1134 (6.8442)  time: 0.0337\n",
      "Epoch: [0]  [37800/80149]  eta: 0:25:25  lr: 0.000500  loss_labels: 6.3150 (6.8429)  loss: 6.2370 (6.8429)  time: 0.0352\n",
      "Epoch: [0]  [37900/80149]  eta: 0:25:21  lr: 0.000500  loss_labels: 6.3048 (6.8414)  loss: 6.3010 (6.8414)  time: 0.0366\n",
      "Epoch: [0]  [38000/80149]  eta: 0:25:18  lr: 0.000500  loss_labels: 6.1560 (6.8397)  loss: 6.1370 (6.8397)  time: 0.0372\n",
      "Epoch: [0]  [38100/80149]  eta: 0:25:15  lr: 0.000500  loss_labels: 6.3000 (6.8383)  loss: 6.1765 (6.8383)  time: 0.0385\n",
      "Epoch: [0]  [38200/80149]  eta: 0:25:11  lr: 0.000500  loss_labels: 6.2449 (6.8369)  loss: 6.3439 (6.8369)  time: 0.0362\n",
      "Epoch: [0]  [38300/80149]  eta: 0:25:07  lr: 0.000500  loss_labels: 6.2031 (6.8354)  loss: 6.2359 (6.8354)  time: 0.0343\n",
      "Epoch: [0]  [38400/80149]  eta: 0:25:05  lr: 0.000500  loss_labels: 6.3084 (6.8340)  loss: 6.2603 (6.8340)  time: 0.0353\n",
      "Epoch: [0]  [38500/80149]  eta: 0:25:01  lr: 0.000500  loss_labels: 6.2514 (6.8325)  loss: 6.3239 (6.8325)  time: 0.0363\n",
      "Epoch: [0]  [38600/80149]  eta: 0:24:57  lr: 0.000500  loss_labels: 6.2700 (6.8312)  loss: 6.3720 (6.8312)  time: 0.0346\n",
      "Epoch: [0]  [38700/80149]  eta: 0:24:54  lr: 0.000500  loss_labels: 6.1938 (6.8297)  loss: 6.1981 (6.8297)  time: 0.0343\n",
      "Epoch: [0]  [38800/80149]  eta: 0:24:50  lr: 0.000500  loss_labels: 6.2562 (6.8282)  loss: 6.2231 (6.8282)  time: 0.0423\n",
      "Epoch: [0]  [38900/80149]  eta: 0:24:47  lr: 0.000500  loss_labels: 6.3051 (6.8269)  loss: 6.2614 (6.8269)  time: 0.0368\n",
      "Epoch: [0]  [39000/80149]  eta: 0:24:43  lr: 0.000500  loss_labels: 6.2607 (6.8255)  loss: 6.2813 (6.8255)  time: 0.0357\n",
      "Epoch: [0]  [39100/80149]  eta: 0:24:39  lr: 0.000500  loss_labels: 6.2857 (6.8241)  loss: 6.2054 (6.8241)  time: 0.0344\n",
      "Epoch: [0]  [39200/80149]  eta: 0:24:36  lr: 0.000500  loss_labels: 6.2454 (6.8228)  loss: 6.1347 (6.8228)  time: 0.0403\n",
      "Epoch: [0]  [39300/80149]  eta: 0:24:32  lr: 0.000500  loss_labels: 6.2549 (6.8214)  loss: 6.2174 (6.8214)  time: 0.0350\n",
      "Epoch: [0]  [39400/80149]  eta: 0:24:29  lr: 0.000500  loss_labels: 6.2955 (6.8200)  loss: 6.3169 (6.8200)  time: 0.0354\n",
      "Epoch: [0]  [39500/80149]  eta: 0:24:26  lr: 0.000500  loss_labels: 6.1988 (6.8184)  loss: 6.1881 (6.8184)  time: 0.0768\n",
      "Epoch: [0]  [39600/80149]  eta: 0:24:22  lr: 0.000500  loss_labels: 6.2927 (6.8169)  loss: 6.1357 (6.8169)  time: 0.0345\n",
      "Epoch: [0]  [39700/80149]  eta: 0:24:18  lr: 0.000500  loss_labels: 6.2155 (6.8155)  loss: 6.0405 (6.8155)  time: 0.0338\n",
      "Epoch: [0]  [39800/80149]  eta: 0:24:15  lr: 0.000500  loss_labels: 6.2246 (6.8140)  loss: 6.2246 (6.8140)  time: 0.0365\n",
      "Epoch: [0]  [39900/80149]  eta: 0:24:11  lr: 0.000500  loss_labels: 6.2180 (6.8126)  loss: 6.2098 (6.8126)  time: 0.0354\n",
      "Epoch: [0]  [40000/80149]  eta: 0:24:08  lr: 0.000500  loss_labels: 6.2072 (6.8112)  loss: 6.0998 (6.8112)  time: 0.0809\n",
      "Epoch: [0]  [40100/80149]  eta: 0:24:05  lr: 0.000500  loss_labels: 6.2360 (6.8098)  loss: 6.1508 (6.8098)  time: 0.0354\n",
      "Epoch: [0]  [40200/80149]  eta: 0:24:01  lr: 0.000500  loss_labels: 6.1921 (6.8083)  loss: 6.4477 (6.8083)  time: 0.0350\n",
      "Epoch: [0]  [40300/80149]  eta: 0:23:58  lr: 0.000500  loss_labels: 6.2905 (6.8070)  loss: 6.3393 (6.8070)  time: 0.0346\n",
      "Epoch: [0]  [40400/80149]  eta: 0:23:55  lr: 0.000500  loss_labels: 6.2507 (6.8057)  loss: 6.2350 (6.8057)  time: 0.0363\n",
      "Epoch: [0]  [40500/80149]  eta: 0:23:51  lr: 0.000500  loss_labels: 6.2629 (6.8043)  loss: 6.2446 (6.8043)  time: 0.0346\n",
      "Epoch: [0]  [40600/80149]  eta: 0:23:48  lr: 0.000500  loss_labels: 6.2041 (6.8029)  loss: 6.1755 (6.8029)  time: 0.0364\n",
      "Epoch: [0]  [40700/80149]  eta: 0:23:45  lr: 0.000500  loss_labels: 6.1920 (6.8015)  loss: 6.1796 (6.8015)  time: 0.0370\n",
      "Epoch: [0]  [40800/80149]  eta: 0:23:41  lr: 0.000500  loss_labels: 6.1981 (6.8001)  loss: 6.2951 (6.8001)  time: 0.0363\n",
      "Epoch: [0]  [40900/80149]  eta: 0:23:38  lr: 0.000500  loss_labels: 6.1067 (6.7984)  loss: 6.1182 (6.7984)  time: 0.0365\n",
      "Epoch: [0]  [41000/80149]  eta: 0:23:34  lr: 0.000500  loss_labels: 6.2231 (6.7970)  loss: 6.0820 (6.7970)  time: 0.0362\n",
      "Epoch: [0]  [41100/80149]  eta: 0:23:31  lr: 0.000500  loss_labels: 6.1262 (6.7954)  loss: 6.0406 (6.7954)  time: 0.0363\n",
      "Epoch: [0]  [41200/80149]  eta: 0:23:27  lr: 0.000500  loss_labels: 6.1828 (6.7939)  loss: 6.3418 (6.7939)  time: 0.0356\n",
      "Epoch: [0]  [41300/80149]  eta: 0:23:24  lr: 0.000500  loss_labels: 6.2037 (6.7925)  loss: 6.0064 (6.7925)  time: 0.0363\n",
      "Epoch: [0]  [41400/80149]  eta: 0:23:20  lr: 0.000500  loss_labels: 6.0916 (6.7909)  loss: 6.0377 (6.7909)  time: 0.0385\n",
      "Epoch: [0]  [41500/80149]  eta: 0:23:17  lr: 0.000500  loss_labels: 6.2312 (6.7895)  loss: 6.1449 (6.7895)  time: 0.0363\n",
      "Epoch: [0]  [41600/80149]  eta: 0:23:14  lr: 0.000500  loss_labels: 6.0978 (6.7879)  loss: 6.1619 (6.7879)  time: 0.0802\n",
      "Epoch: [0]  [41700/80149]  eta: 0:23:10  lr: 0.000500  loss_labels: 6.1290 (6.7863)  loss: 6.0071 (6.7863)  time: 0.0361\n",
      "Epoch: [0]  [41800/80149]  eta: 0:23:07  lr: 0.000500  loss_labels: 6.1098 (6.7848)  loss: 5.9072 (6.7848)  time: 0.0365\n",
      "Epoch: [0]  [41900/80149]  eta: 0:23:03  lr: 0.000500  loss_labels: 6.0443 (6.7832)  loss: 6.0443 (6.7832)  time: 0.0361\n",
      "Epoch: [0]  [42000/80149]  eta: 0:22:59  lr: 0.000500  loss_labels: 6.0929 (6.7816)  loss: 6.0615 (6.7816)  time: 0.0360\n",
      "Epoch: [0]  [42100/80149]  eta: 0:22:56  lr: 0.000500  loss_labels: 5.9755 (6.7797)  loss: 6.0157 (6.7797)  time: 0.0361\n",
      "Epoch: [0]  [42200/80149]  eta: 0:22:53  lr: 0.000500  loss_labels: 6.0680 (6.7781)  loss: 6.0382 (6.7781)  time: 0.0364\n",
      "Epoch: [0]  [42300/80149]  eta: 0:22:49  lr: 0.000500  loss_labels: 6.1247 (6.7766)  loss: 6.0881 (6.7766)  time: 0.0362\n",
      "Epoch: [0]  [42400/80149]  eta: 0:22:46  lr: 0.000500  loss_labels: 6.0560 (6.7750)  loss: 6.1563 (6.7750)  time: 0.0361\n",
      "Epoch: [0]  [42500/80149]  eta: 0:22:42  lr: 0.000500  loss_labels: 6.0776 (6.7734)  loss: 5.9865 (6.7734)  time: 0.0360\n",
      "Epoch: [0]  [42600/80149]  eta: 0:22:38  lr: 0.000500  loss_labels: 6.0765 (6.7719)  loss: 5.9310 (6.7719)  time: 0.0357\n",
      "Epoch: [0]  [42700/80149]  eta: 0:22:35  lr: 0.000500  loss_labels: 6.0429 (6.7703)  loss: 5.8428 (6.7703)  time: 0.0350\n",
      "Epoch: [0]  [42800/80149]  eta: 0:22:31  lr: 0.000500  loss_labels: 6.0669 (6.7687)  loss: 6.0501 (6.7687)  time: 0.0348\n",
      "Epoch: [0]  [42900/80149]  eta: 0:22:28  lr: 0.000500  loss_labels: 6.1394 (6.7672)  loss: 6.1045 (6.7672)  time: 0.0344\n",
      "Epoch: [0]  [43000/80149]  eta: 0:22:24  lr: 0.000500  loss_labels: 6.0569 (6.7655)  loss: 6.1080 (6.7655)  time: 0.0374\n",
      "Epoch: [0]  [43100/80149]  eta: 0:22:21  lr: 0.000500  loss_labels: 6.0989 (6.7640)  loss: 6.0437 (6.7640)  time: 0.0339\n",
      "Epoch: [0]  [43200/80149]  eta: 0:22:17  lr: 0.000500  loss_labels: 6.0853 (6.7624)  loss: 6.0655 (6.7624)  time: 0.0353\n",
      "Epoch: [0]  [43300/80149]  eta: 0:22:14  lr: 0.000500  loss_labels: 6.0035 (6.7608)  loss: 5.9473 (6.7608)  time: 0.0357\n",
      "Epoch: [0]  [43400/80149]  eta: 0:22:11  lr: 0.000500  loss_labels: 6.0663 (6.7592)  loss: 6.3181 (6.7592)  time: 0.0354\n",
      "Epoch: [0]  [43500/80149]  eta: 0:22:07  lr: 0.000500  loss_labels: 6.0088 (6.7577)  loss: 5.9733 (6.7577)  time: 0.0341\n",
      "Epoch: [0]  [43600/80149]  eta: 0:22:03  lr: 0.000500  loss_labels: 6.0603 (6.7562)  loss: 6.0704 (6.7562)  time: 0.0351\n",
      "Epoch: [0]  [43700/80149]  eta: 0:22:01  lr: 0.000500  loss_labels: 6.0116 (6.7546)  loss: 6.1601 (6.7546)  time: 0.0345\n",
      "Epoch: [0]  [43800/80149]  eta: 0:21:57  lr: 0.000500  loss_labels: 6.1157 (6.7531)  loss: 6.0897 (6.7531)  time: 0.0368\n",
      "Epoch: [0]  [43900/80149]  eta: 0:21:54  lr: 0.000500  loss_labels: 6.0411 (6.7516)  loss: 6.0201 (6.7516)  time: 0.0784\n",
      "Epoch: [0]  [44000/80149]  eta: 0:21:50  lr: 0.000500  loss_labels: 5.9690 (6.7499)  loss: 5.9119 (6.7499)  time: 0.0347\n",
      "Epoch: [0]  [44100/80149]  eta: 0:21:46  lr: 0.000500  loss_labels: 6.0612 (6.7485)  loss: 6.0422 (6.7485)  time: 0.0339\n",
      "Epoch: [0]  [44200/80149]  eta: 0:21:43  lr: 0.000500  loss_labels: 5.9772 (6.7467)  loss: 5.9772 (6.7467)  time: 0.0353\n",
      "Epoch: [0]  [44300/80149]  eta: 0:21:40  lr: 0.000500  loss_labels: 5.9818 (6.7450)  loss: 5.8263 (6.7450)  time: 0.0342\n",
      "Epoch: [0]  [44400/80149]  eta: 0:21:36  lr: 0.000500  loss_labels: 6.0150 (6.7434)  loss: 5.9692 (6.7434)  time: 0.0361\n",
      "Epoch: [0]  [44500/80149]  eta: 0:21:32  lr: 0.000500  loss_labels: 5.9423 (6.7418)  loss: 5.8656 (6.7418)  time: 0.0347\n",
      "Epoch: [0]  [44600/80149]  eta: 0:21:29  lr: 0.000500  loss_labels: 6.0034 (6.7400)  loss: 6.0200 (6.7400)  time: 0.0348\n",
      "Epoch: [0]  [44700/80149]  eta: 0:21:25  lr: 0.000500  loss_labels: 5.9871 (6.7384)  loss: 5.9002 (6.7384)  time: 0.0368\n",
      "Epoch: [0]  [44800/80149]  eta: 0:21:22  lr: 0.000500  loss_labels: 5.9024 (6.7367)  loss: 5.8996 (6.7367)  time: 0.0360\n",
      "Epoch: [0]  [44900/80149]  eta: 0:21:19  lr: 0.000500  loss_labels: 5.9395 (6.7350)  loss: 5.8168 (6.7350)  time: 0.0360\n",
      "Epoch: [0]  [45000/80149]  eta: 0:21:15  lr: 0.000500  loss_labels: 6.0024 (6.7335)  loss: 5.9413 (6.7335)  time: 0.0361\n",
      "Epoch: [0]  [45100/80149]  eta: 0:21:11  lr: 0.000500  loss_labels: 5.9749 (6.7319)  loss: 5.9670 (6.7319)  time: 0.0377\n",
      "Epoch: [0]  [45200/80149]  eta: 0:21:08  lr: 0.000500  loss_labels: 6.0037 (6.7303)  loss: 6.0608 (6.7303)  time: 0.0345\n",
      "Epoch: [0]  [45300/80149]  eta: 0:21:05  lr: 0.000500  loss_labels: 6.0256 (6.7286)  loss: 6.0478 (6.7286)  time: 0.0362\n",
      "Epoch: [0]  [45400/80149]  eta: 0:21:01  lr: 0.000500  loss_labels: 5.9485 (6.7271)  loss: 6.0313 (6.7271)  time: 0.0348\n",
      "Epoch: [0]  [45500/80149]  eta: 0:20:58  lr: 0.000500  loss_labels: 5.9859 (6.7255)  loss: 6.0018 (6.7255)  time: 0.0358\n",
      "Epoch: [0]  [45600/80149]  eta: 0:20:54  lr: 0.000500  loss_labels: 6.0331 (6.7240)  loss: 6.0074 (6.7240)  time: 0.0366\n",
      "Epoch: [0]  [45700/80149]  eta: 0:20:51  lr: 0.000500  loss_labels: 5.9945 (6.7224)  loss: 5.9586 (6.7224)  time: 0.0338\n",
      "Epoch: [0]  [45800/80149]  eta: 0:20:47  lr: 0.000500  loss_labels: 6.0295 (6.7209)  loss: 6.0506 (6.7209)  time: 0.0338\n",
      "Epoch: [0]  [45900/80149]  eta: 0:20:43  lr: 0.000500  loss_labels: 6.0221 (6.7194)  loss: 5.8878 (6.7194)  time: 0.0345\n",
      "Epoch: [0]  [46000/80149]  eta: 0:20:39  lr: 0.000500  loss_labels: 6.0332 (6.7178)  loss: 6.0374 (6.7178)  time: 0.0344\n",
      "Epoch: [0]  [46100/80149]  eta: 0:20:36  lr: 0.000500  loss_labels: 5.8772 (6.7161)  loss: 5.7850 (6.7161)  time: 0.0344\n",
      "Epoch: [0]  [46200/80149]  eta: 0:20:32  lr: 0.000500  loss_labels: 6.0439 (6.7145)  loss: 5.9924 (6.7145)  time: 0.0380\n",
      "Epoch: [0]  [46300/80149]  eta: 0:20:29  lr: 0.000500  loss_labels: 5.8783 (6.7128)  loss: 6.0403 (6.7128)  time: 0.0349\n",
      "Epoch: [0]  [46400/80149]  eta: 0:20:25  lr: 0.000500  loss_labels: 6.0299 (6.7113)  loss: 6.1080 (6.7113)  time: 0.0344\n",
      "Epoch: [0]  [46500/80149]  eta: 0:20:22  lr: 0.000500  loss_labels: 5.9990 (6.7097)  loss: 5.9788 (6.7097)  time: 0.0342\n",
      "Epoch: [0]  [46600/80149]  eta: 0:20:18  lr: 0.000500  loss_labels: 5.8919 (6.7079)  loss: 5.9082 (6.7079)  time: 0.0398\n",
      "Epoch: [0]  [46700/80149]  eta: 0:20:15  lr: 0.000500  loss_labels: 6.0779 (6.7065)  loss: 5.9158 (6.7065)  time: 0.0351\n",
      "Epoch: [0]  [46800/80149]  eta: 0:20:11  lr: 0.000500  loss_labels: 5.9290 (6.7049)  loss: 5.8527 (6.7049)  time: 0.0360\n",
      "Epoch: [0]  [46900/80149]  eta: 0:20:08  lr: 0.000500  loss_labels: 6.0586 (6.7035)  loss: 5.8997 (6.7035)  time: 0.0800\n",
      "Epoch: [0]  [47000/80149]  eta: 0:20:04  lr: 0.000500  loss_labels: 5.9523 (6.7019)  loss: 5.8958 (6.7019)  time: 0.0357\n",
      "Epoch: [0]  [47100/80149]  eta: 0:20:01  lr: 0.000500  loss_labels: 5.9164 (6.7003)  loss: 6.0153 (6.7003)  time: 0.0339\n",
      "Epoch: [0]  [47200/80149]  eta: 0:19:57  lr: 0.000500  loss_labels: 5.8835 (6.6987)  loss: 5.7508 (6.6987)  time: 0.0336\n",
      "Epoch: [0]  [47300/80149]  eta: 0:19:53  lr: 0.000500  loss_labels: 5.9288 (6.6971)  loss: 5.9514 (6.6971)  time: 0.0342\n",
      "Epoch: [0]  [47400/80149]  eta: 0:19:50  lr: 0.000500  loss_labels: 6.0083 (6.6956)  loss: 5.7804 (6.6956)  time: 0.0692\n",
      "Epoch: [0]  [47500/80149]  eta: 0:19:46  lr: 0.000500  loss_labels: 5.9512 (6.6940)  loss: 5.9092 (6.6940)  time: 0.0346\n",
      "Epoch: [0]  [47600/80149]  eta: 0:19:42  lr: 0.000500  loss_labels: 5.9514 (6.6924)  loss: 6.1367 (6.6924)  time: 0.0351\n",
      "Epoch: [0]  [47700/80149]  eta: 0:19:39  lr: 0.000500  loss_labels: 5.9108 (6.6907)  loss: 5.7771 (6.6907)  time: 0.0339\n",
      "Epoch: [0]  [47800/80149]  eta: 0:19:35  lr: 0.000500  loss_labels: 5.7939 (6.6890)  loss: 5.7597 (6.6890)  time: 0.0383\n",
      "Epoch: [0]  [47900/80149]  eta: 0:19:31  lr: 0.000500  loss_labels: 5.9013 (6.6874)  loss: 5.8435 (6.6874)  time: 0.0367\n",
      "Epoch: [0]  [48000/80149]  eta: 0:19:28  lr: 0.000500  loss_labels: 5.9540 (6.6859)  loss: 5.9684 (6.6859)  time: 0.0369\n",
      "Epoch: [0]  [48100/80149]  eta: 0:19:24  lr: 0.000500  loss_labels: 5.9850 (6.6844)  loss: 5.8872 (6.6844)  time: 0.0342\n",
      "Epoch: [0]  [48200/80149]  eta: 0:19:20  lr: 0.000500  loss_labels: 5.8340 (6.6827)  loss: 5.8979 (6.6827)  time: 0.0347\n",
      "Epoch: [0]  [48300/80149]  eta: 0:19:17  lr: 0.000500  loss_labels: 5.8872 (6.6811)  loss: 5.8890 (6.6811)  time: 0.0359\n",
      "Epoch: [0]  [48400/80149]  eta: 0:19:13  lr: 0.000500  loss_labels: 5.9591 (6.6796)  loss: 6.0201 (6.6796)  time: 0.0350\n",
      "Epoch: [0]  [48500/80149]  eta: 0:19:10  lr: 0.000500  loss_labels: 5.8776 (6.6779)  loss: 5.8490 (6.6779)  time: 0.0360\n",
      "Epoch: [0]  [48600/80149]  eta: 0:19:06  lr: 0.000500  loss_labels: 5.8718 (6.6764)  loss: 6.0655 (6.6764)  time: 0.0364\n",
      "Epoch: [0]  [48700/80149]  eta: 0:19:02  lr: 0.000500  loss_labels: 5.9088 (6.6748)  loss: 5.8716 (6.6748)  time: 0.0361\n",
      "Epoch: [0]  [48800/80149]  eta: 0:18:59  lr: 0.000500  loss_labels: 5.9105 (6.6733)  loss: 5.8391 (6.6733)  time: 0.0360\n",
      "Epoch: [0]  [48900/80149]  eta: 0:18:56  lr: 0.000500  loss_labels: 5.8151 (6.6716)  loss: 5.7731 (6.6716)  time: 0.0364\n",
      "Epoch: [0]  [49000/80149]  eta: 0:18:52  lr: 0.000500  loss_labels: 5.8604 (6.6700)  loss: 5.8917 (6.6700)  time: 0.0360\n",
      "Epoch: [0]  [49100/80149]  eta: 0:18:48  lr: 0.000500  loss_labels: 5.8810 (6.6684)  loss: 5.7744 (6.6684)  time: 0.0355\n",
      "Epoch: [0]  [49200/80149]  eta: 0:18:45  lr: 0.000500  loss_labels: 5.8119 (6.6667)  loss: 5.9450 (6.6667)  time: 0.0359\n",
      "Epoch: [0]  [49300/80149]  eta: 0:18:41  lr: 0.000500  loss_labels: 5.8267 (6.6651)  loss: 5.8385 (6.6651)  time: 0.0359\n",
      "Epoch: [0]  [49400/80149]  eta: 0:18:37  lr: 0.000500  loss_labels: 5.8300 (6.6634)  loss: 5.7872 (6.6634)  time: 0.0354\n",
      "Epoch: [0]  [49500/80149]  eta: 0:18:34  lr: 0.000500  loss_labels: 5.8595 (6.6618)  loss: 5.7968 (6.6618)  time: 0.0361\n",
      "Epoch: [0]  [49600/80149]  eta: 0:18:30  lr: 0.000500  loss_labels: 5.8838 (6.6602)  loss: 5.7870 (6.6602)  time: 0.0357\n",
      "Epoch: [0]  [49700/80149]  eta: 0:18:26  lr: 0.000500  loss_labels: 5.8166 (6.6586)  loss: 5.7421 (6.6586)  time: 0.0369\n",
      "Epoch: [0]  [49800/80149]  eta: 0:18:23  lr: 0.000500  loss_labels: 5.9441 (6.6571)  loss: 5.8015 (6.6571)  time: 0.0354\n",
      "Epoch: [0]  [49900/80149]  eta: 0:18:19  lr: 0.000500  loss_labels: 5.8406 (6.6555)  loss: 5.8398 (6.6555)  time: 0.0340\n",
      "Epoch: [0]  [50000/80149]  eta: 0:18:16  lr: 0.000500  loss_labels: 5.8452 (6.6539)  loss: 5.9130 (6.6539)  time: 0.0365\n",
      "Epoch: [0]  [50100/80149]  eta: 0:18:12  lr: 0.000500  loss_labels: 5.7905 (6.6522)  loss: 5.9383 (6.6522)  time: 0.0358\n",
      "Epoch: [0]  [50200/80149]  eta: 0:18:09  lr: 0.000500  loss_labels: 5.8857 (6.6507)  loss: 5.8857 (6.6507)  time: 0.0357\n",
      "Epoch: [0]  [50300/80149]  eta: 0:18:05  lr: 0.000500  loss_labels: 5.7937 (6.6491)  loss: 5.8082 (6.6491)  time: 0.0364\n",
      "Epoch: [0]  [50400/80149]  eta: 0:18:02  lr: 0.000500  loss_labels: 5.9148 (6.6476)  loss: 6.0390 (6.6476)  time: 0.0364\n",
      "Epoch: [0]  [50500/80149]  eta: 0:17:59  lr: 0.000500  loss_labels: 5.8353 (6.6461)  loss: 5.8308 (6.6461)  time: 0.0766\n",
      "Epoch: [0]  [50600/80149]  eta: 0:17:55  lr: 0.000500  loss_labels: 5.8252 (6.6444)  loss: 5.8828 (6.6444)  time: 0.0356\n",
      "Epoch: [0]  [50700/80149]  eta: 0:17:51  lr: 0.000500  loss_labels: 5.6967 (6.6427)  loss: 5.7227 (6.6427)  time: 0.0384\n",
      "Epoch: [0]  [50800/80149]  eta: 0:17:48  lr: 0.000500  loss_labels: 5.7907 (6.6412)  loss: 5.6247 (6.6412)  time: 0.0361\n",
      "Epoch: [0]  [50900/80149]  eta: 0:17:44  lr: 0.000500  loss_labels: 5.9090 (6.6397)  loss: 5.9143 (6.6397)  time: 0.0360\n",
      "Epoch: [0]  [51000/80149]  eta: 0:17:40  lr: 0.000500  loss_labels: 5.8261 (6.6382)  loss: 5.8643 (6.6382)  time: 0.0354\n",
      "Epoch: [0]  [51100/80149]  eta: 0:17:37  lr: 0.000500  loss_labels: 5.7624 (6.6364)  loss: 5.7793 (6.6364)  time: 0.0346\n",
      "Epoch: [0]  [51200/80149]  eta: 0:17:33  lr: 0.000500  loss_labels: 5.8066 (6.6348)  loss: 5.9392 (6.6348)  time: 0.0345\n",
      "Epoch: [0]  [51300/80149]  eta: 0:17:29  lr: 0.000500  loss_labels: 5.6900 (6.6331)  loss: 5.4947 (6.6331)  time: 0.0344\n",
      "Epoch: [0]  [51400/80149]  eta: 0:17:25  lr: 0.000500  loss_labels: 5.8045 (6.6315)  loss: 5.7826 (6.6315)  time: 0.0336\n",
      "Epoch: [0]  [51500/80149]  eta: 0:17:22  lr: 0.000500  loss_labels: 5.7466 (6.6298)  loss: 5.7198 (6.6298)  time: 0.0337\n",
      "Epoch: [0]  [51600/80149]  eta: 0:17:18  lr: 0.000500  loss_labels: 5.7457 (6.6282)  loss: 5.6504 (6.6282)  time: 0.0355\n",
      "Epoch: [0]  [51700/80149]  eta: 0:17:14  lr: 0.000500  loss_labels: 5.7244 (6.6265)  loss: 5.7596 (6.6265)  time: 0.0347\n",
      "Epoch: [0]  [51800/80149]  eta: 0:17:10  lr: 0.000500  loss_labels: 5.8177 (6.6250)  loss: 5.8202 (6.6250)  time: 0.0357\n",
      "Epoch: [0]  [51900/80149]  eta: 0:17:07  lr: 0.000500  loss_labels: 5.9094 (6.6235)  loss: 5.7045 (6.6235)  time: 0.0343\n",
      "Epoch: [0]  [52000/80149]  eta: 0:17:03  lr: 0.000500  loss_labels: 5.8246 (6.6219)  loss: 5.8434 (6.6219)  time: 0.0354\n",
      "Epoch: [0]  [52100/80149]  eta: 0:16:59  lr: 0.000500  loss_labels: 5.8812 (6.6205)  loss: 5.9411 (6.6205)  time: 0.0354\n",
      "Epoch: [0]  [52200/80149]  eta: 0:16:56  lr: 0.000500  loss_labels: 5.7811 (6.6190)  loss: 5.7811 (6.6190)  time: 0.0349\n",
      "Epoch: [0]  [52300/80149]  eta: 0:16:52  lr: 0.000500  loss_labels: 5.8284 (6.6175)  loss: 5.8150 (6.6175)  time: 0.0357\n",
      "Epoch: [0]  [52400/80149]  eta: 0:16:48  lr: 0.000500  loss_labels: 5.8191 (6.6159)  loss: 5.8906 (6.6159)  time: 0.0356\n",
      "Epoch: [0]  [52500/80149]  eta: 0:16:45  lr: 0.000500  loss_labels: 5.7850 (6.6143)  loss: 5.7304 (6.6143)  time: 0.0361\n",
      "Epoch: [0]  [52600/80149]  eta: 0:16:41  lr: 0.000500  loss_labels: 5.8278 (6.6128)  loss: 5.8261 (6.6128)  time: 0.0348\n",
      "Epoch: [0]  [52700/80149]  eta: 0:16:37  lr: 0.000500  loss_labels: 5.7545 (6.6112)  loss: 5.7387 (6.6112)  time: 0.0366\n",
      "Epoch: [0]  [52800/80149]  eta: 0:16:34  lr: 0.000500  loss_labels: 5.7524 (6.6096)  loss: 5.5810 (6.6096)  time: 0.0354\n",
      "Epoch: [0]  [52900/80149]  eta: 0:16:30  lr: 0.000500  loss_labels: 5.7934 (6.6081)  loss: 5.7880 (6.6081)  time: 0.0387\n",
      "Epoch: [0]  [53000/80149]  eta: 0:16:26  lr: 0.000500  loss_labels: 5.7293 (6.6065)  loss: 5.6990 (6.6065)  time: 0.0360\n",
      "Epoch: [0]  [53100/80149]  eta: 0:16:23  lr: 0.000500  loss_labels: 5.6747 (6.6048)  loss: 5.7738 (6.6048)  time: 0.0347\n",
      "Epoch: [0]  [53200/80149]  eta: 0:16:19  lr: 0.000500  loss_labels: 5.6765 (6.6031)  loss: 5.6743 (6.6031)  time: 0.0364\n",
      "Epoch: [0]  [53300/80149]  eta: 0:16:15  lr: 0.000500  loss_labels: 5.7987 (6.6016)  loss: 5.8241 (6.6016)  time: 0.0338\n",
      "Epoch: [0]  [53400/80149]  eta: 0:16:12  lr: 0.000500  loss_labels: 5.7253 (6.5999)  loss: 5.6089 (6.5999)  time: 0.0331\n",
      "Epoch: [0]  [53500/80149]  eta: 0:16:08  lr: 0.000500  loss_labels: 5.7768 (6.5983)  loss: 5.8428 (6.5983)  time: 0.0336\n",
      "Epoch: [0]  [53600/80149]  eta: 0:16:05  lr: 0.000500  loss_labels: 5.7482 (6.5967)  loss: 5.8282 (6.5967)  time: 0.0792\n",
      "Epoch: [0]  [53700/80149]  eta: 0:16:01  lr: 0.000500  loss_labels: 5.7861 (6.5952)  loss: 5.7877 (6.5952)  time: 0.0398\n",
      "Epoch: [0]  [53800/80149]  eta: 0:15:57  lr: 0.000500  loss_labels: 5.7422 (6.5937)  loss: 5.8926 (6.5937)  time: 0.0339\n",
      "Epoch: [0]  [53900/80149]  eta: 0:15:53  lr: 0.000500  loss_labels: 5.7330 (6.5920)  loss: 5.6151 (6.5920)  time: 0.0377\n",
      "Epoch: [0]  [54000/80149]  eta: 0:15:50  lr: 0.000500  loss_labels: 5.7427 (6.5904)  loss: 5.6971 (6.5904)  time: 0.0349\n",
      "Epoch: [0]  [54100/80149]  eta: 0:15:46  lr: 0.000500  loss_labels: 5.7352 (6.5889)  loss: 5.6813 (6.5889)  time: 0.0713\n",
      "Epoch: [0]  [54200/80149]  eta: 0:15:43  lr: 0.000500  loss_labels: 5.7288 (6.5873)  loss: 5.7772 (6.5873)  time: 0.0360\n",
      "Epoch: [0]  [54300/80149]  eta: 0:15:39  lr: 0.000500  loss_labels: 5.6966 (6.5858)  loss: 5.5973 (6.5858)  time: 0.0352\n",
      "Epoch: [0]  [54400/80149]  eta: 0:15:36  lr: 0.000500  loss_labels: 5.6182 (6.5841)  loss: 5.6768 (6.5841)  time: 0.0770\n",
      "Epoch: [0]  [54500/80149]  eta: 0:15:32  lr: 0.000500  loss_labels: 5.7666 (6.5827)  loss: 5.6948 (6.5827)  time: 0.0341\n",
      "Epoch: [0]  [54600/80149]  eta: 0:15:28  lr: 0.000500  loss_labels: 5.6096 (6.5810)  loss: 5.4901 (6.5810)  time: 0.0361\n",
      "Epoch: [0]  [54700/80149]  eta: 0:15:25  lr: 0.000500  loss_labels: 5.6806 (6.5794)  loss: 5.5652 (6.5794)  time: 0.0346\n",
      "Epoch: [0]  [54800/80149]  eta: 0:15:21  lr: 0.000500  loss_labels: 5.6930 (6.5778)  loss: 5.8342 (6.5778)  time: 0.0345\n",
      "Epoch: [0]  [54900/80149]  eta: 0:15:17  lr: 0.000500  loss_labels: 5.6479 (6.5763)  loss: 5.7486 (6.5763)  time: 0.0344\n",
      "Epoch: [0]  [55000/80149]  eta: 0:15:14  lr: 0.000500  loss_labels: 5.7005 (6.5747)  loss: 5.8201 (6.5747)  time: 0.0350\n",
      "Epoch: [0]  [55100/80149]  eta: 0:15:10  lr: 0.000500  loss_labels: 5.6666 (6.5732)  loss: 5.7172 (6.5732)  time: 0.0352\n",
      "Epoch: [0]  [55200/80149]  eta: 0:15:06  lr: 0.000500  loss_labels: 5.6693 (6.5716)  loss: 5.7794 (6.5716)  time: 0.0351\n",
      "Epoch: [0]  [55300/80149]  eta: 0:15:03  lr: 0.000500  loss_labels: 5.7427 (6.5702)  loss: 5.6920 (6.5702)  time: 0.0336\n",
      "Epoch: [0]  [55400/80149]  eta: 0:14:59  lr: 0.000500  loss_labels: 5.6716 (6.5686)  loss: 5.5500 (6.5686)  time: 0.0357\n",
      "Epoch: [0]  [55500/80149]  eta: 0:14:55  lr: 0.000500  loss_labels: 5.6586 (6.5670)  loss: 5.8259 (6.5670)  time: 0.0333\n",
      "Epoch: [0]  [55600/80149]  eta: 0:14:52  lr: 0.000500  loss_labels: 5.6364 (6.5654)  loss: 5.6198 (6.5654)  time: 0.0331\n",
      "Epoch: [0]  [55700/80149]  eta: 0:14:48  lr: 0.000500  loss_labels: 5.7089 (6.5638)  loss: 5.6595 (6.5638)  time: 0.0350\n",
      "Epoch: [0]  [55800/80149]  eta: 0:14:44  lr: 0.000500  loss_labels: 5.6298 (6.5622)  loss: 5.6449 (6.5622)  time: 0.0342\n",
      "Epoch: [0]  [55900/80149]  eta: 0:14:41  lr: 0.000500  loss_labels: 5.7500 (6.5607)  loss: 5.6411 (6.5607)  time: 0.0348\n",
      "Epoch: [0]  [56000/80149]  eta: 0:14:37  lr: 0.000500  loss_labels: 5.6918 (6.5592)  loss: 5.9246 (6.5592)  time: 0.0368\n",
      "Epoch: [0]  [56100/80149]  eta: 0:14:33  lr: 0.000500  loss_labels: 5.6339 (6.5576)  loss: 5.5332 (6.5576)  time: 0.0331\n",
      "Epoch: [0]  [56200/80149]  eta: 0:14:30  lr: 0.000500  loss_labels: 5.6647 (6.5560)  loss: 5.6750 (6.5560)  time: 0.0343\n",
      "Epoch: [0]  [56300/80149]  eta: 0:14:26  lr: 0.000500  loss_labels: 5.5992 (6.5544)  loss: 5.5585 (6.5544)  time: 0.0333\n",
      "Epoch: [0]  [56400/80149]  eta: 0:14:22  lr: 0.000500  loss_labels: 5.6262 (6.5528)  loss: 5.5495 (6.5528)  time: 0.0328\n",
      "Epoch: [0]  [56500/80149]  eta: 0:14:18  lr: 0.000500  loss_labels: 5.6720 (6.5513)  loss: 5.4639 (6.5513)  time: 0.0330\n",
      "Epoch: [0]  [56600/80149]  eta: 0:14:14  lr: 0.000500  loss_labels: 5.6710 (6.5498)  loss: 5.6683 (6.5498)  time: 0.0328\n",
      "Epoch: [0]  [56700/80149]  eta: 0:14:11  lr: 0.000500  loss_labels: 5.7637 (6.5483)  loss: 5.8147 (6.5483)  time: 0.0327\n",
      "Epoch: [0]  [56800/80149]  eta: 0:14:07  lr: 0.000500  loss_labels: 5.6502 (6.5467)  loss: 5.5863 (6.5467)  time: 0.0325\n",
      "Epoch: [0]  [56900/80149]  eta: 0:14:03  lr: 0.000500  loss_labels: 5.6269 (6.5452)  loss: 5.6269 (6.5452)  time: 0.0330\n",
      "Epoch: [0]  [57000/80149]  eta: 0:13:59  lr: 0.000500  loss_labels: 5.6666 (6.5437)  loss: 5.6545 (6.5437)  time: 0.0329\n",
      "Epoch: [0]  [57100/80149]  eta: 0:13:56  lr: 0.000500  loss_labels: 5.6447 (6.5421)  loss: 5.5955 (6.5421)  time: 0.0328\n",
      "Epoch: [0]  [57200/80149]  eta: 0:13:52  lr: 0.000500  loss_labels: 5.6117 (6.5404)  loss: 5.6117 (6.5404)  time: 0.0329\n",
      "Epoch: [0]  [57300/80149]  eta: 0:13:48  lr: 0.000500  loss_labels: 5.6054 (6.5389)  loss: 5.5574 (6.5389)  time: 0.0328\n",
      "Epoch: [0]  [57400/80149]  eta: 0:13:44  lr: 0.000500  loss_labels: 5.6710 (6.5374)  loss: 5.6155 (6.5374)  time: 0.0327\n",
      "Epoch: [0]  [57500/80149]  eta: 0:13:41  lr: 0.000500  loss_labels: 5.6099 (6.5358)  loss: 5.4714 (6.5358)  time: 0.0327\n",
      "Epoch: [0]  [57600/80149]  eta: 0:13:37  lr: 0.000500  loss_labels: 5.6699 (6.5343)  loss: 5.4651 (6.5343)  time: 0.0331\n",
      "Epoch: [0]  [57700/80149]  eta: 0:13:33  lr: 0.000500  loss_labels: 5.6272 (6.5328)  loss: 5.6967 (6.5328)  time: 0.0330\n",
      "Epoch: [0]  [57800/80149]  eta: 0:13:29  lr: 0.000500  loss_labels: 5.5956 (6.5312)  loss: 5.6588 (6.5312)  time: 0.0330\n",
      "Epoch: [0]  [57900/80149]  eta: 0:13:26  lr: 0.000500  loss_labels: 5.6210 (6.5297)  loss: 5.7010 (6.5297)  time: 0.0327\n",
      "Epoch: [0]  [58000/80149]  eta: 0:13:22  lr: 0.000500  loss_labels: 5.6643 (6.5282)  loss: 5.5921 (6.5282)  time: 0.0328\n",
      "Epoch: [0]  [58100/80149]  eta: 0:13:18  lr: 0.000500  loss_labels: 5.5598 (6.5266)  loss: 5.4896 (6.5266)  time: 0.0329\n",
      "Epoch: [0]  [58200/80149]  eta: 0:13:14  lr: 0.000500  loss_labels: 5.5530 (6.5251)  loss: 5.5036 (6.5251)  time: 0.0331\n",
      "Epoch: [0]  [58300/80149]  eta: 0:13:11  lr: 0.000500  loss_labels: 5.5970 (6.5235)  loss: 5.4792 (6.5235)  time: 0.0329\n",
      "Epoch: [0]  [58400/80149]  eta: 0:13:07  lr: 0.000500  loss_labels: 5.5282 (6.5219)  loss: 5.5742 (6.5219)  time: 0.0336\n",
      "Epoch: [0]  [58500/80149]  eta: 0:13:03  lr: 0.000500  loss_labels: 5.5820 (6.5203)  loss: 5.4921 (6.5203)  time: 0.0336\n",
      "Epoch: [0]  [58600/80149]  eta: 0:12:59  lr: 0.000500  loss_labels: 5.5768 (6.5188)  loss: 5.5981 (6.5188)  time: 0.0334\n",
      "Epoch: [0]  [58700/80149]  eta: 0:12:56  lr: 0.000500  loss_labels: 5.5990 (6.5173)  loss: 5.7576 (6.5173)  time: 0.0342\n",
      "Epoch: [0]  [58800/80149]  eta: 0:12:52  lr: 0.000500  loss_labels: 5.6037 (6.5157)  loss: 5.6787 (6.5157)  time: 0.0341\n",
      "Epoch: [0]  [58900/80149]  eta: 0:12:48  lr: 0.000500  loss_labels: 5.5547 (6.5142)  loss: 5.4635 (6.5142)  time: 0.0339\n",
      "Epoch: [0]  [59000/80149]  eta: 0:12:45  lr: 0.000500  loss_labels: 5.6715 (6.5127)  loss: 5.6106 (6.5127)  time: 0.0342\n",
      "Epoch: [0]  [59100/80149]  eta: 0:12:41  lr: 0.000500  loss_labels: 5.5441 (6.5112)  loss: 5.7362 (6.5112)  time: 0.0395\n",
      "Epoch: [0]  [59200/80149]  eta: 0:12:37  lr: 0.000500  loss_labels: 5.5168 (6.5096)  loss: 5.5030 (6.5096)  time: 0.0344\n",
      "Epoch: [0]  [59300/80149]  eta: 0:12:34  lr: 0.000500  loss_labels: 5.5799 (6.5081)  loss: 5.7384 (6.5081)  time: 0.0346\n",
      "Epoch: [0]  [59400/80149]  eta: 0:12:30  lr: 0.000500  loss_labels: 5.6207 (6.5066)  loss: 5.5396 (6.5066)  time: 0.0341\n",
      "Epoch: [0]  [59500/80149]  eta: 0:12:27  lr: 0.000500  loss_labels: 5.5675 (6.5051)  loss: 5.4865 (6.5051)  time: 0.0351\n",
      "Epoch: [0]  [59600/80149]  eta: 0:12:23  lr: 0.000500  loss_labels: 5.6126 (6.5037)  loss: 5.5823 (6.5037)  time: 0.0336\n",
      "Epoch: [0]  [59700/80149]  eta: 0:12:19  lr: 0.000500  loss_labels: 5.5732 (6.5021)  loss: 5.5622 (6.5021)  time: 0.0334\n",
      "Epoch: [0]  [59800/80149]  eta: 0:12:16  lr: 0.000500  loss_labels: 5.6255 (6.5007)  loss: 5.5619 (6.5007)  time: 0.0361\n",
      "Epoch: [0]  [59900/80149]  eta: 0:12:12  lr: 0.000500  loss_labels: 5.6269 (6.4992)  loss: 5.5993 (6.4992)  time: 0.0335\n",
      "Epoch: [0]  [60000/80149]  eta: 0:12:08  lr: 0.000500  loss_labels: 5.5621 (6.4976)  loss: 5.5839 (6.4976)  time: 0.0332\n",
      "Epoch: [0]  [60100/80149]  eta: 0:12:05  lr: 0.000500  loss_labels: 5.6182 (6.4961)  loss: 5.6409 (6.4961)  time: 0.0345\n",
      "Epoch: [0]  [60200/80149]  eta: 0:12:01  lr: 0.000500  loss_labels: 5.5182 (6.4946)  loss: 5.4152 (6.4946)  time: 0.0350\n",
      "Epoch: [0]  [60300/80149]  eta: 0:11:57  lr: 0.000500  loss_labels: 5.5513 (6.4931)  loss: 5.6424 (6.4931)  time: 0.0359\n",
      "Epoch: [0]  [60400/80149]  eta: 0:11:54  lr: 0.000500  loss_labels: 5.6236 (6.4916)  loss: 5.5472 (6.4916)  time: 0.0345\n",
      "Epoch: [0]  [60500/80149]  eta: 0:11:50  lr: 0.000500  loss_labels: 5.5382 (6.4902)  loss: 5.4957 (6.4902)  time: 0.0349\n",
      "Epoch: [0]  [60600/80149]  eta: 0:11:47  lr: 0.000500  loss_labels: 5.5515 (6.4887)  loss: 5.5292 (6.4887)  time: 0.0344\n",
      "Epoch: [0]  [60700/80149]  eta: 0:11:43  lr: 0.000500  loss_labels: 5.5429 (6.4871)  loss: 5.5453 (6.4871)  time: 0.0345\n",
      "Epoch: [0]  [60800/80149]  eta: 0:11:39  lr: 0.000500  loss_labels: 5.6144 (6.4857)  loss: 5.5838 (6.4857)  time: 0.0347\n",
      "Epoch: [0]  [60900/80149]  eta: 0:11:36  lr: 0.000500  loss_labels: 5.5382 (6.4842)  loss: 5.6384 (6.4842)  time: 0.0351\n",
      "Epoch: [0]  [61000/80149]  eta: 0:11:32  lr: 0.000500  loss_labels: 5.4842 (6.4827)  loss: 5.5174 (6.4827)  time: 0.0352\n",
      "Epoch: [0]  [61100/80149]  eta: 0:11:28  lr: 0.000500  loss_labels: 5.6065 (6.4812)  loss: 5.6348 (6.4812)  time: 0.0338\n",
      "Epoch: [0]  [61200/80149]  eta: 0:11:25  lr: 0.000500  loss_labels: 5.5206 (6.4797)  loss: 5.5431 (6.4797)  time: 0.0343\n",
      "Epoch: [0]  [61300/80149]  eta: 0:11:21  lr: 0.000500  loss_labels: 5.5505 (6.4782)  loss: 5.5309 (6.4782)  time: 0.0355\n",
      "Epoch: [0]  [61400/80149]  eta: 0:11:17  lr: 0.000500  loss_labels: 5.5872 (6.4767)  loss: 5.8870 (6.4767)  time: 0.0339\n",
      "Epoch: [0]  [61500/80149]  eta: 0:11:14  lr: 0.000500  loss_labels: 5.5202 (6.4752)  loss: 5.4439 (6.4752)  time: 0.0354\n",
      "Epoch: [0]  [61600/80149]  eta: 0:11:10  lr: 0.000500  loss_labels: 5.4754 (6.4737)  loss: 5.3299 (6.4737)  time: 0.0353\n",
      "Epoch: [0]  [61700/80149]  eta: 0:11:07  lr: 0.000500  loss_labels: 5.6505 (6.4722)  loss: 5.6273 (6.4722)  time: 0.0354\n",
      "Epoch: [0]  [61800/80149]  eta: 0:11:03  lr: 0.000500  loss_labels: 5.4959 (6.4707)  loss: 5.4959 (6.4707)  time: 0.0355\n",
      "Epoch: [0]  [61900/80149]  eta: 0:10:59  lr: 0.000500  loss_labels: 5.4704 (6.4691)  loss: 5.3990 (6.4691)  time: 0.0345\n",
      "Epoch: [0]  [62000/80149]  eta: 0:10:56  lr: 0.000500  loss_labels: 5.4410 (6.4676)  loss: 5.4796 (6.4676)  time: 0.0349\n",
      "Epoch: [0]  [62100/80149]  eta: 0:10:52  lr: 0.000500  loss_labels: 5.4545 (6.4660)  loss: 5.3608 (6.4660)  time: 0.0363\n",
      "Epoch: [0]  [62200/80149]  eta: 0:10:49  lr: 0.000500  loss_labels: 5.4169 (6.4644)  loss: 5.4246 (6.4644)  time: 0.0344\n",
      "Epoch: [0]  [62300/80149]  eta: 0:10:45  lr: 0.000500  loss_labels: 5.5070 (6.4628)  loss: 5.4374 (6.4628)  time: 0.0334\n",
      "Epoch: [0]  [62400/80149]  eta: 0:10:41  lr: 0.000500  loss_labels: 5.5667 (6.4614)  loss: 5.6370 (6.4614)  time: 0.0333\n",
      "Epoch: [0]  [62500/80149]  eta: 0:10:38  lr: 0.000500  loss_labels: 5.5884 (6.4599)  loss: 5.3576 (6.4599)  time: 0.0342\n",
      "Epoch: [0]  [62600/80149]  eta: 0:10:34  lr: 0.000500  loss_labels: 5.4755 (6.4585)  loss: 5.6137 (6.4585)  time: 0.0342\n",
      "Epoch: [0]  [62700/80149]  eta: 0:10:30  lr: 0.000500  loss_labels: 5.4227 (6.4568)  loss: 5.3152 (6.4568)  time: 0.0333\n",
      "Epoch: [0]  [62800/80149]  eta: 0:10:27  lr: 0.000500  loss_labels: 5.5446 (6.4553)  loss: 5.6788 (6.4553)  time: 0.0354\n",
      "Epoch: [0]  [62900/80149]  eta: 0:10:23  lr: 0.000500  loss_labels: 5.5220 (6.4539)  loss: 5.5988 (6.4539)  time: 0.0374\n",
      "Epoch: [0]  [63000/80149]  eta: 0:10:19  lr: 0.000500  loss_labels: 5.4936 (6.4524)  loss: 5.4595 (6.4524)  time: 0.0333\n",
      "Epoch: [0]  [63100/80149]  eta: 0:10:16  lr: 0.000500  loss_labels: 5.4922 (6.4509)  loss: 5.3073 (6.4509)  time: 0.0360\n",
      "Epoch: [0]  [63200/80149]  eta: 0:10:12  lr: 0.000500  loss_labels: 5.4854 (6.4494)  loss: 5.6194 (6.4494)  time: 0.0339\n",
      "Epoch: [0]  [63300/80149]  eta: 0:10:09  lr: 0.000500  loss_labels: 5.4892 (6.4479)  loss: 5.4892 (6.4479)  time: 0.0339\n",
      "Epoch: [0]  [63400/80149]  eta: 0:10:05  lr: 0.000500  loss_labels: 5.4559 (6.4464)  loss: 5.5322 (6.4464)  time: 0.0339\n",
      "Epoch: [0]  [63500/80149]  eta: 0:10:01  lr: 0.000500  loss_labels: 5.4736 (6.4449)  loss: 5.4736 (6.4449)  time: 0.0340\n",
      "Epoch: [0]  [63600/80149]  eta: 0:09:58  lr: 0.000500  loss_labels: 5.5773 (6.4435)  loss: 5.5508 (6.4435)  time: 0.0339\n",
      "Epoch: [0]  [63700/80149]  eta: 0:09:54  lr: 0.000500  loss_labels: 5.4343 (6.4419)  loss: 5.2669 (6.4419)  time: 0.0339\n",
      "Epoch: [0]  [63800/80149]  eta: 0:09:50  lr: 0.000500  loss_labels: 5.4527 (6.4404)  loss: 5.2844 (6.4404)  time: 0.0345\n",
      "Epoch: [0]  [63900/80149]  eta: 0:09:47  lr: 0.000500  loss_labels: 5.5297 (6.4390)  loss: 5.3593 (6.4390)  time: 0.0340\n",
      "Epoch: [0]  [64000/80149]  eta: 0:09:43  lr: 0.000500  loss_labels: 5.4258 (6.4374)  loss: 5.5394 (6.4374)  time: 0.0756\n",
      "Epoch: [0]  [64100/80149]  eta: 0:09:39  lr: 0.000500  loss_labels: 5.4184 (6.4358)  loss: 5.6404 (6.4358)  time: 0.0339\n",
      "Epoch: [0]  [64200/80149]  eta: 0:09:36  lr: 0.000500  loss_labels: 5.4806 (6.4344)  loss: 5.4946 (6.4344)  time: 0.0342\n",
      "Epoch: [0]  [64300/80149]  eta: 0:09:32  lr: 0.000500  loss_labels: 5.3592 (6.4327)  loss: 5.2131 (6.4327)  time: 0.0333\n",
      "Epoch: [0]  [64400/80149]  eta: 0:09:28  lr: 0.000500  loss_labels: 5.5139 (6.4314)  loss: 5.5949 (6.4314)  time: 0.0332\n",
      "Epoch: [0]  [64500/80149]  eta: 0:09:25  lr: 0.000500  loss_labels: 5.4268 (6.4298)  loss: 5.4804 (6.4298)  time: 0.0347\n",
      "Epoch: [0]  [64600/80149]  eta: 0:09:21  lr: 0.000500  loss_labels: 5.4279 (6.4283)  loss: 5.4858 (6.4283)  time: 0.0340\n",
      "Epoch: [0]  [64700/80149]  eta: 0:09:17  lr: 0.000500  loss_labels: 5.4496 (6.4268)  loss: 5.7439 (6.4268)  time: 0.0354\n",
      "Epoch: [0]  [64800/80149]  eta: 0:09:14  lr: 0.000500  loss_labels: 5.4301 (6.4253)  loss: 5.3041 (6.4253)  time: 0.0337\n",
      "Epoch: [0]  [64900/80149]  eta: 0:09:10  lr: 0.000500  loss_labels: 5.3929 (6.4238)  loss: 5.3541 (6.4238)  time: 0.0337\n",
      "Epoch: [0]  [65000/80149]  eta: 0:09:06  lr: 0.000500  loss_labels: 5.4369 (6.4223)  loss: 5.3983 (6.4223)  time: 0.0350\n",
      "Epoch: [0]  [65100/80149]  eta: 0:09:03  lr: 0.000500  loss_labels: 5.5282 (6.4210)  loss: 5.5315 (6.4210)  time: 0.0357\n",
      "Epoch: [0]  [65200/80149]  eta: 0:08:59  lr: 0.000500  loss_labels: 5.4541 (6.4195)  loss: 5.4381 (6.4195)  time: 0.0348\n",
      "Epoch: [0]  [65300/80149]  eta: 0:08:56  lr: 0.000500  loss_labels: 5.3979 (6.4181)  loss: 5.4715 (6.4181)  time: 0.0339\n",
      "Epoch: [0]  [65400/80149]  eta: 0:08:52  lr: 0.000500  loss_labels: 5.3909 (6.4165)  loss: 5.3345 (6.4165)  time: 0.0367\n",
      "Epoch: [0]  [65500/80149]  eta: 0:08:48  lr: 0.000500  loss_labels: 5.3944 (6.4149)  loss: 5.5628 (6.4149)  time: 0.0345\n",
      "Epoch: [0]  [65600/80149]  eta: 0:08:45  lr: 0.000500  loss_labels: 5.3847 (6.4134)  loss: 5.4449 (6.4134)  time: 0.0339\n",
      "Epoch: [0]  [65700/80149]  eta: 0:08:41  lr: 0.000500  loss_labels: 5.4523 (6.4120)  loss: 5.4029 (6.4120)  time: 0.0340\n",
      "Epoch: [0]  [65800/80149]  eta: 0:08:37  lr: 0.000500  loss_labels: 5.4040 (6.4106)  loss: 5.3226 (6.4106)  time: 0.0346\n",
      "Epoch: [0]  [65900/80149]  eta: 0:08:34  lr: 0.000500  loss_labels: 5.4498 (6.4092)  loss: 5.3915 (6.4092)  time: 0.0349\n",
      "Epoch: [0]  [66000/80149]  eta: 0:08:30  lr: 0.000500  loss_labels: 5.3707 (6.4077)  loss: 5.3111 (6.4077)  time: 0.0354\n",
      "Epoch: [0]  [66100/80149]  eta: 0:08:26  lr: 0.000500  loss_labels: 5.3737 (6.4062)  loss: 5.3232 (6.4062)  time: 0.0346\n",
      "Epoch: [0]  [66200/80149]  eta: 0:08:23  lr: 0.000500  loss_labels: 5.5187 (6.4049)  loss: 5.6104 (6.4049)  time: 0.0349\n",
      "Epoch: [0]  [66300/80149]  eta: 0:08:19  lr: 0.000500  loss_labels: 5.4219 (6.4034)  loss: 5.2387 (6.4034)  time: 0.0345\n",
      "Epoch: [0]  [66400/80149]  eta: 0:08:16  lr: 0.000500  loss_labels: 5.3483 (6.4018)  loss: 5.2510 (6.4018)  time: 0.0342\n",
      "Epoch: [0]  [66500/80149]  eta: 0:08:12  lr: 0.000500  loss_labels: 5.3820 (6.4004)  loss: 5.4458 (6.4004)  time: 0.0340\n",
      "Epoch: [0]  [66600/80149]  eta: 0:08:08  lr: 0.000500  loss_labels: 5.3894 (6.3989)  loss: 5.5696 (6.3989)  time: 0.0346\n",
      "Epoch: [0]  [66700/80149]  eta: 0:08:05  lr: 0.000500  loss_labels: 5.3786 (6.3974)  loss: 5.3618 (6.3974)  time: 0.0337\n",
      "Epoch: [0]  [66800/80149]  eta: 0:08:01  lr: 0.000500  loss_labels: 5.2816 (6.3959)  loss: 5.5131 (6.3959)  time: 0.0335\n",
      "Epoch: [0]  [66900/80149]  eta: 0:07:57  lr: 0.000500  loss_labels: 5.3592 (6.3944)  loss: 5.2697 (6.3944)  time: 0.0342\n",
      "Epoch: [0]  [67000/80149]  eta: 0:07:54  lr: 0.000500  loss_labels: 5.3712 (6.3928)  loss: 5.5250 (6.3928)  time: 0.0352\n",
      "Epoch: [0]  [67100/80149]  eta: 0:07:50  lr: 0.000500  loss_labels: 5.3195 (6.3913)  loss: 5.4839 (6.3913)  time: 0.0361\n",
      "Epoch: [0]  [67200/80149]  eta: 0:07:46  lr: 0.000500  loss_labels: 5.4350 (6.3899)  loss: 5.4563 (6.3899)  time: 0.0337\n",
      "Epoch: [0]  [67300/80149]  eta: 0:07:43  lr: 0.000500  loss_labels: 5.3371 (6.3884)  loss: 5.3814 (6.3884)  time: 0.0350\n",
      "Epoch: [0]  [67400/80149]  eta: 0:07:39  lr: 0.000500  loss_labels: 5.4315 (6.3870)  loss: 5.5365 (6.3870)  time: 0.0346\n",
      "Epoch: [0]  [67500/80149]  eta: 0:07:36  lr: 0.000500  loss_labels: 5.3604 (6.3855)  loss: 5.2260 (6.3855)  time: 0.0344\n",
      "Epoch: [0]  [67600/80149]  eta: 0:07:32  lr: 0.000500  loss_labels: 5.2739 (6.3840)  loss: 5.2595 (6.3840)  time: 0.0368\n",
      "Epoch: [0]  [67700/80149]  eta: 0:07:28  lr: 0.000500  loss_labels: 5.3896 (6.3825)  loss: 5.3811 (6.3825)  time: 0.0337\n",
      "Epoch: [0]  [67800/80149]  eta: 0:07:25  lr: 0.000500  loss_labels: 5.3842 (6.3811)  loss: 5.4647 (6.3811)  time: 0.0354\n",
      "Epoch: [0]  [67900/80149]  eta: 0:07:21  lr: 0.000500  loss_labels: 5.3721 (6.3797)  loss: 5.3875 (6.3797)  time: 0.0360\n",
      "Epoch: [0]  [68000/80149]  eta: 0:07:18  lr: 0.000500  loss_labels: 5.4521 (6.3783)  loss: 5.4376 (6.3783)  time: 0.0366\n",
      "Epoch: [0]  [68100/80149]  eta: 0:07:14  lr: 0.000500  loss_labels: 5.3348 (6.3768)  loss: 5.4203 (6.3768)  time: 0.0364\n",
      "Epoch: [0]  [68200/80149]  eta: 0:07:10  lr: 0.000500  loss_labels: 5.3001 (6.3753)  loss: 4.9256 (6.3753)  time: 0.0352\n",
      "Epoch: [0]  [68300/80149]  eta: 0:07:07  lr: 0.000500  loss_labels: 5.3478 (6.3738)  loss: 5.2273 (6.3738)  time: 0.0363\n",
      "Epoch: [0]  [68400/80149]  eta: 0:07:03  lr: 0.000500  loss_labels: 5.2683 (6.3722)  loss: 5.1551 (6.3722)  time: 0.0365\n",
      "Epoch: [0]  [68500/80149]  eta: 0:07:00  lr: 0.000500  loss_labels: 5.2398 (6.3706)  loss: 5.3666 (6.3706)  time: 0.0365\n",
      "Epoch: [0]  [68600/80149]  eta: 0:06:56  lr: 0.000500  loss_labels: 5.4076 (6.3692)  loss: 5.3370 (6.3692)  time: 0.0358\n",
      "Epoch: [0]  [68700/80149]  eta: 0:06:52  lr: 0.000500  loss_labels: 5.3757 (6.3678)  loss: 5.3218 (6.3678)  time: 0.0364\n",
      "Epoch: [0]  [68800/80149]  eta: 0:06:49  lr: 0.000500  loss_labels: 5.2815 (6.3663)  loss: 5.1865 (6.3663)  time: 0.0374\n",
      "Epoch: [0]  [68900/80149]  eta: 0:06:45  lr: 0.000500  loss_labels: 5.2502 (6.3648)  loss: 5.3614 (6.3648)  time: 0.0372\n",
      "Epoch: [0]  [69000/80149]  eta: 0:06:42  lr: 0.000500  loss_labels: 5.3534 (6.3633)  loss: 5.2425 (6.3633)  time: 0.0362\n",
      "Epoch: [0]  [69100/80149]  eta: 0:06:38  lr: 0.000500  loss_labels: 5.2970 (6.3619)  loss: 5.1178 (6.3619)  time: 0.0341\n",
      "Epoch: [0]  [69200/80149]  eta: 0:06:34  lr: 0.000500  loss_labels: 5.1745 (6.3603)  loss: 5.1412 (6.3603)  time: 0.0352\n",
      "Epoch: [0]  [69300/80149]  eta: 0:06:31  lr: 0.000500  loss_labels: 5.3052 (6.3588)  loss: 5.5054 (6.3588)  time: 0.0365\n",
      "Epoch: [0]  [69400/80149]  eta: 0:06:27  lr: 0.000500  loss_labels: 5.3403 (6.3573)  loss: 5.4697 (6.3573)  time: 0.0363\n",
      "Epoch: [0]  [69500/80149]  eta: 0:06:24  lr: 0.000500  loss_labels: 5.3642 (6.3559)  loss: 5.3642 (6.3559)  time: 0.0360\n",
      "Epoch: [0]  [69600/80149]  eta: 0:06:20  lr: 0.000500  loss_labels: 5.3194 (6.3544)  loss: 5.1967 (6.3544)  time: 0.0343\n",
      "Epoch: [0]  [69700/80149]  eta: 0:06:16  lr: 0.000500  loss_labels: 5.3077 (6.3530)  loss: 5.3323 (6.3530)  time: 0.0344\n",
      "Epoch: [0]  [69800/80149]  eta: 0:06:13  lr: 0.000500  loss_labels: 5.3130 (6.3515)  loss: 5.3596 (6.3515)  time: 0.0343\n",
      "Epoch: [0]  [69900/80149]  eta: 0:06:09  lr: 0.000500  loss_labels: 5.4509 (6.3502)  loss: 5.3649 (6.3502)  time: 0.0353\n",
      "Epoch: [0]  [70000/80149]  eta: 0:06:05  lr: 0.000500  loss_labels: 5.3237 (6.3487)  loss: 5.2184 (6.3487)  time: 0.0355\n",
      "Epoch: [0]  [70100/80149]  eta: 0:06:02  lr: 0.000500  loss_labels: 5.3512 (6.3473)  loss: 5.2535 (6.3473)  time: 0.0355\n",
      "Epoch: [0]  [70200/80149]  eta: 0:05:58  lr: 0.000500  loss_labels: 5.3077 (6.3458)  loss: 5.1794 (6.3458)  time: 0.0359\n",
      "Epoch: [0]  [70300/80149]  eta: 0:05:55  lr: 0.000500  loss_labels: 5.3232 (6.3444)  loss: 5.4255 (6.3444)  time: 0.0360\n",
      "Epoch: [0]  [70400/80149]  eta: 0:05:51  lr: 0.000500  loss_labels: 5.2720 (6.3428)  loss: 5.2952 (6.3428)  time: 0.0366\n",
      "Epoch: [0]  [70500/80149]  eta: 0:05:47  lr: 0.000500  loss_labels: 5.3284 (6.3414)  loss: 5.3193 (6.3414)  time: 0.0367\n",
      "Epoch: [0]  [70600/80149]  eta: 0:05:44  lr: 0.000500  loss_labels: 5.2962 (6.3400)  loss: 5.4584 (6.3400)  time: 0.0375\n",
      "Epoch: [0]  [70700/80149]  eta: 0:05:40  lr: 0.000500  loss_labels: 5.3541 (6.3386)  loss: 5.4604 (6.3386)  time: 0.0361\n",
      "Epoch: [0]  [70800/80149]  eta: 0:05:37  lr: 0.000500  loss_labels: 5.3343 (6.3372)  loss: 5.5708 (6.3372)  time: 0.0371\n",
      "Epoch: [0]  [70900/80149]  eta: 0:05:33  lr: 0.000500  loss_labels: 5.2570 (6.3357)  loss: 5.1422 (6.3357)  time: 0.0370\n",
      "Epoch: [0]  [71000/80149]  eta: 0:05:29  lr: 0.000500  loss_labels: 5.2799 (6.3342)  loss: 5.3733 (6.3342)  time: 0.0340\n",
      "Epoch: [0]  [71100/80149]  eta: 0:05:26  lr: 0.000500  loss_labels: 5.3863 (6.3329)  loss: 5.4340 (6.3329)  time: 0.0334\n",
      "Epoch: [0]  [71200/80149]  eta: 0:05:22  lr: 0.000500  loss_labels: 5.2936 (6.3315)  loss: 5.2688 (6.3315)  time: 0.0356\n",
      "Epoch: [0]  [71300/80149]  eta: 0:05:19  lr: 0.000500  loss_labels: 5.1767 (6.3299)  loss: 5.2297 (6.3299)  time: 0.0351\n",
      "Epoch: [0]  [71400/80149]  eta: 0:05:15  lr: 0.000500  loss_labels: 5.3284 (6.3285)  loss: 5.3385 (6.3285)  time: 0.0362\n",
      "Epoch: [0]  [71500/80149]  eta: 0:05:11  lr: 0.000500  loss_labels: 5.1975 (6.3270)  loss: 5.1939 (6.3270)  time: 0.0343\n",
      "Epoch: [0]  [71600/80149]  eta: 0:05:08  lr: 0.000500  loss_labels: 5.3120 (6.3255)  loss: 5.1881 (6.3255)  time: 0.0345\n",
      "Epoch: [0]  [71700/80149]  eta: 0:05:04  lr: 0.000500  loss_labels: 5.2947 (6.3242)  loss: 5.1813 (6.3242)  time: 0.0349\n",
      "Epoch: [0]  [71800/80149]  eta: 0:05:00  lr: 0.000500  loss_labels: 5.3767 (6.3228)  loss: 5.5023 (6.3228)  time: 0.0350\n",
      "Epoch: [0]  [71900/80149]  eta: 0:04:57  lr: 0.000500  loss_labels: 5.1812 (6.3213)  loss: 5.1258 (6.3213)  time: 0.0340\n",
      "Epoch: [0]  [72000/80149]  eta: 0:04:53  lr: 0.000500  loss_labels: 5.1819 (6.3198)  loss: 5.0650 (6.3198)  time: 0.0345\n",
      "Epoch: [0]  [72100/80149]  eta: 0:04:50  lr: 0.000500  loss_labels: 5.2890 (6.3184)  loss: 5.1734 (6.3184)  time: 0.0346\n",
      "Epoch: [0]  [72200/80149]  eta: 0:04:46  lr: 0.000500  loss_labels: 5.1901 (6.3168)  loss: 4.9559 (6.3168)  time: 0.0343\n",
      "Epoch: [0]  [72300/80149]  eta: 0:04:42  lr: 0.000500  loss_labels: 5.3559 (6.3155)  loss: 5.4827 (6.3155)  time: 0.0350\n",
      "Epoch: [0]  [72400/80149]  eta: 0:04:39  lr: 0.000500  loss_labels: 5.2466 (6.3141)  loss: 5.2555 (6.3141)  time: 0.0347\n",
      "Epoch: [0]  [72500/80149]  eta: 0:04:35  lr: 0.000500  loss_labels: 5.2717 (6.3128)  loss: 5.2752 (6.3128)  time: 0.0342\n",
      "Epoch: [0]  [72600/80149]  eta: 0:04:31  lr: 0.000500  loss_labels: 5.2888 (6.3114)  loss: 5.1408 (6.3114)  time: 0.0343\n",
      "Epoch: [0]  [72700/80149]  eta: 0:04:28  lr: 0.000500  loss_labels: 5.2482 (6.3100)  loss: 5.2944 (6.3100)  time: 0.0354\n",
      "Epoch: [0]  [72800/80149]  eta: 0:04:24  lr: 0.000500  loss_labels: 5.3207 (6.3087)  loss: 5.2897 (6.3087)  time: 0.0344\n",
      "Epoch: [0]  [72900/80149]  eta: 0:04:21  lr: 0.000500  loss_labels: 5.2604 (6.3072)  loss: 5.0054 (6.3072)  time: 0.0344\n",
      "Epoch: [0]  [73000/80149]  eta: 0:04:17  lr: 0.000500  loss_labels: 5.1899 (6.3057)  loss: 4.9188 (6.3057)  time: 0.0342\n",
      "Epoch: [0]  [73100/80149]  eta: 0:04:13  lr: 0.000500  loss_labels: 5.2300 (6.3043)  loss: 5.1909 (6.3043)  time: 0.0343\n",
      "Epoch: [0]  [73200/80149]  eta: 0:04:10  lr: 0.000500  loss_labels: 5.2453 (6.3028)  loss: 5.1887 (6.3028)  time: 0.0345\n",
      "Epoch: [0]  [73300/80149]  eta: 0:04:06  lr: 0.000500  loss_labels: 5.1982 (6.3014)  loss: 4.9656 (6.3014)  time: 0.0345\n",
      "Epoch: [0]  [73400/80149]  eta: 0:04:03  lr: 0.000500  loss_labels: 5.2321 (6.3000)  loss: 5.1509 (6.3000)  time: 0.0344\n",
      "Epoch: [0]  [73500/80149]  eta: 0:03:59  lr: 0.000500  loss_labels: 5.1976 (6.2984)  loss: 5.2227 (6.2984)  time: 0.0345\n",
      "Epoch: [0]  [73600/80149]  eta: 0:03:55  lr: 0.000500  loss_labels: 5.2470 (6.2971)  loss: 5.2742 (6.2971)  time: 0.0342\n",
      "Epoch: [0]  [73700/80149]  eta: 0:03:52  lr: 0.000500  loss_labels: 5.2121 (6.2956)  loss: 5.2185 (6.2956)  time: 0.0347\n",
      "Epoch: [0]  [73800/80149]  eta: 0:03:48  lr: 0.000500  loss_labels: 5.2630 (6.2942)  loss: 4.9786 (6.2942)  time: 0.0352\n",
      "Epoch: [0]  [73900/80149]  eta: 0:03:45  lr: 0.000500  loss_labels: 5.1973 (6.2927)  loss: 5.1149 (6.2927)  time: 0.0351\n",
      "Epoch: [0]  [74000/80149]  eta: 0:03:41  lr: 0.000500  loss_labels: 5.2222 (6.2913)  loss: 5.3915 (6.2913)  time: 0.0365\n",
      "Epoch: [0]  [74100/80149]  eta: 0:03:37  lr: 0.000500  loss_labels: 5.2525 (6.2899)  loss: 5.2470 (6.2899)  time: 0.0346\n",
      "Epoch: [0]  [74200/80149]  eta: 0:03:34  lr: 0.000500  loss_labels: 5.2910 (6.2885)  loss: 5.3073 (6.2885)  time: 0.0352\n",
      "Epoch: [0]  [74300/80149]  eta: 0:03:30  lr: 0.000500  loss_labels: 5.1842 (6.2870)  loss: 5.0230 (6.2870)  time: 0.0348\n",
      "Epoch: [0]  [74400/80149]  eta: 0:03:26  lr: 0.000500  loss_labels: 5.2358 (6.2856)  loss: 5.2611 (6.2856)  time: 0.0350\n",
      "Epoch: [0]  [74500/80149]  eta: 0:03:23  lr: 0.000500  loss_labels: 5.2759 (6.2843)  loss: 5.1487 (6.2843)  time: 0.0352\n",
      "Epoch: [0]  [74600/80149]  eta: 0:03:19  lr: 0.000500  loss_labels: 5.1628 (6.2828)  loss: 5.0186 (6.2828)  time: 0.0352\n",
      "Epoch: [0]  [74700/80149]  eta: 0:03:16  lr: 0.000500  loss_labels: 5.2380 (6.2814)  loss: 5.3223 (6.2814)  time: 0.0351\n",
      "Epoch: [0]  [74800/80149]  eta: 0:03:12  lr: 0.000500  loss_labels: 5.1524 (6.2800)  loss: 5.1524 (6.2800)  time: 0.0352\n",
      "Epoch: [0]  [74900/80149]  eta: 0:03:08  lr: 0.000500  loss_labels: 5.1610 (6.2786)  loss: 5.3066 (6.2786)  time: 0.0386\n",
      "Epoch: [0]  [75000/80149]  eta: 0:03:05  lr: 0.000500  loss_labels: 5.2450 (6.2773)  loss: 5.3309 (6.2773)  time: 0.0357\n",
      "Epoch: [0]  [75100/80149]  eta: 0:03:01  lr: 0.000500  loss_labels: 5.2106 (6.2758)  loss: 5.2803 (6.2758)  time: 0.0357\n",
      "Epoch: [0]  [75200/80149]  eta: 0:02:58  lr: 0.000500  loss_labels: 5.1958 (6.2744)  loss: 5.1613 (6.2744)  time: 0.0354\n",
      "Epoch: [0]  [75300/80149]  eta: 0:02:54  lr: 0.000500  loss_labels: 5.0673 (6.2729)  loss: 5.1760 (6.2729)  time: 0.0355\n",
      "Epoch: [0]  [75400/80149]  eta: 0:02:51  lr: 0.000500  loss_labels: 5.2504 (6.2715)  loss: 5.3340 (6.2715)  time: 0.0355\n",
      "Epoch: [0]  [75500/80149]  eta: 0:02:47  lr: 0.000500  loss_labels: 5.0591 (6.2700)  loss: 5.0151 (6.2700)  time: 0.0354\n",
      "Epoch: [0]  [75600/80149]  eta: 0:02:43  lr: 0.000500  loss_labels: 5.1063 (6.2686)  loss: 5.3146 (6.2686)  time: 0.0349\n",
      "Epoch: [0]  [75700/80149]  eta: 0:02:40  lr: 0.000500  loss_labels: 5.1909 (6.2672)  loss: 5.1132 (6.2672)  time: 0.0359\n",
      "Epoch: [0]  [75800/80149]  eta: 0:02:36  lr: 0.000500  loss_labels: 5.0875 (6.2657)  loss: 5.1338 (6.2657)  time: 0.0352\n",
      "Epoch: [0]  [75900/80149]  eta: 0:02:32  lr: 0.000500  loss_labels: 5.2046 (6.2643)  loss: 5.2188 (6.2643)  time: 0.0350\n",
      "Epoch: [0]  [76000/80149]  eta: 0:02:29  lr: 0.000500  loss_labels: 5.1486 (6.2629)  loss: 5.1399 (6.2629)  time: 0.0331\n",
      "Epoch: [0]  [76100/80149]  eta: 0:02:25  lr: 0.000500  loss_labels: 5.1230 (6.2614)  loss: 4.9495 (6.2614)  time: 0.0331\n",
      "Epoch: [0]  [76200/80149]  eta: 0:02:22  lr: 0.000500  loss_labels: 5.1291 (6.2600)  loss: 5.1936 (6.2600)  time: 0.0345\n",
      "Epoch: [0]  [76300/80149]  eta: 0:02:18  lr: 0.000500  loss_labels: 5.2009 (6.2587)  loss: 5.1522 (6.2587)  time: 0.0376\n",
      "Epoch: [0]  [76400/80149]  eta: 0:02:14  lr: 0.000500  loss_labels: 5.1977 (6.2574)  loss: 5.1965 (6.2574)  time: 0.0386\n",
      "Epoch: [0]  [76500/80149]  eta: 0:02:11  lr: 0.000500  loss_labels: 5.1959 (6.2560)  loss: 5.0336 (6.2560)  time: 0.0348\n",
      "Epoch: [0]  [76600/80149]  eta: 0:02:07  lr: 0.000500  loss_labels: 5.1457 (6.2546)  loss: 5.1214 (6.2546)  time: 0.0360\n",
      "Epoch: [0]  [76700/80149]  eta: 0:02:04  lr: 0.000500  loss_labels: 5.1213 (6.2531)  loss: 5.1082 (6.2531)  time: 0.0347\n",
      "Epoch: [0]  [76800/80149]  eta: 0:02:00  lr: 0.000500  loss_labels: 5.2818 (6.2518)  loss: 5.0583 (6.2518)  time: 0.0354\n",
      "Epoch: [0]  [76900/80149]  eta: 0:01:56  lr: 0.000500  loss_labels: 5.1200 (6.2504)  loss: 5.3240 (6.2504)  time: 0.0349\n",
      "Epoch: [0]  [77000/80149]  eta: 0:01:53  lr: 0.000500  loss_labels: 5.1104 (6.2490)  loss: 5.1551 (6.2490)  time: 0.0349\n",
      "Epoch: [0]  [77100/80149]  eta: 0:01:49  lr: 0.000500  loss_labels: 5.2063 (6.2476)  loss: 5.1644 (6.2476)  time: 0.0347\n",
      "Epoch: [0]  [77200/80149]  eta: 0:01:46  lr: 0.000500  loss_labels: 5.1107 (6.2462)  loss: 5.0662 (6.2462)  time: 0.0341\n",
      "Epoch: [0]  [77300/80149]  eta: 0:01:42  lr: 0.000500  loss_labels: 5.2153 (6.2449)  loss: 5.3002 (6.2449)  time: 0.0332\n",
      "Epoch: [0]  [77400/80149]  eta: 0:01:38  lr: 0.000500  loss_labels: 5.1605 (6.2435)  loss: 5.0572 (6.2435)  time: 0.0354\n",
      "Epoch: [0]  [77500/80149]  eta: 0:01:35  lr: 0.000500  loss_labels: 5.1774 (6.2421)  loss: 5.1297 (6.2421)  time: 0.0340\n",
      "Epoch: [0]  [77600/80149]  eta: 0:01:31  lr: 0.000500  loss_labels: 5.0897 (6.2406)  loss: 4.9721 (6.2406)  time: 0.0338\n",
      "Epoch: [0]  [77700/80149]  eta: 0:01:28  lr: 0.000500  loss_labels: 5.1006 (6.2392)  loss: 5.1226 (6.2392)  time: 0.0349\n",
      "Epoch: [0]  [77800/80149]  eta: 0:01:24  lr: 0.000500  loss_labels: 5.1550 (6.2378)  loss: 4.9922 (6.2378)  time: 0.0333\n",
      "Epoch: [0]  [77900/80149]  eta: 0:01:20  lr: 0.000500  loss_labels: 5.1232 (6.2363)  loss: 5.0807 (6.2363)  time: 0.0331\n",
      "Epoch: [0]  [78000/80149]  eta: 0:01:17  lr: 0.000500  loss_labels: 5.1836 (6.2349)  loss: 5.2316 (6.2349)  time: 0.0330\n",
      "Epoch: [0]  [78100/80149]  eta: 0:01:13  lr: 0.000500  loss_labels: 5.0796 (6.2335)  loss: 4.9529 (6.2335)  time: 0.0329\n",
      "Epoch: [0]  [78200/80149]  eta: 0:01:10  lr: 0.000500  loss_labels: 5.0589 (6.2321)  loss: 4.9925 (6.2321)  time: 0.0334\n",
      "Epoch: [0]  [78300/80149]  eta: 0:01:06  lr: 0.000500  loss_labels: 5.1172 (6.2307)  loss: 5.0941 (6.2307)  time: 0.0358\n",
      "Epoch: [0]  [78400/80149]  eta: 0:01:02  lr: 0.000500  loss_labels: 5.0389 (6.2293)  loss: 5.0601 (6.2293)  time: 0.0336\n",
      "Epoch: [0]  [78500/80149]  eta: 0:00:59  lr: 0.000500  loss_labels: 5.1395 (6.2278)  loss: 5.1902 (6.2278)  time: 0.0337\n",
      "Epoch: [0]  [78600/80149]  eta: 0:00:55  lr: 0.000500  loss_labels: 5.1778 (6.2265)  loss: 5.2450 (6.2265)  time: 0.0342\n",
      "Epoch: [0]  [78700/80149]  eta: 0:00:52  lr: 0.000500  loss_labels: 5.1097 (6.2252)  loss: 5.1097 (6.2252)  time: 0.0340\n",
      "Epoch: [0]  [78800/80149]  eta: 0:00:48  lr: 0.000500  loss_labels: 5.0874 (6.2238)  loss: 5.0955 (6.2238)  time: 0.0337\n",
      "Epoch: [0]  [78900/80149]  eta: 0:00:44  lr: 0.000500  loss_labels: 5.1075 (6.2224)  loss: 5.0028 (6.2224)  time: 0.0340\n",
      "Epoch: [0]  [79000/80149]  eta: 0:00:41  lr: 0.000500  loss_labels: 5.1490 (6.2211)  loss: 5.0843 (6.2211)  time: 0.0370\n",
      "Epoch: [0]  [79100/80149]  eta: 0:00:37  lr: 0.000500  loss_labels: 5.0437 (6.2196)  loss: 5.0437 (6.2196)  time: 0.0351\n",
      "Epoch: [0]  [79200/80149]  eta: 0:00:34  lr: 0.000500  loss_labels: 5.1479 (6.2183)  loss: 5.1894 (6.2183)  time: 0.0358\n",
      "Epoch: [0]  [79300/80149]  eta: 0:00:30  lr: 0.000500  loss_labels: 5.1193 (6.2169)  loss: 4.9413 (6.2169)  time: 0.0353\n",
      "Epoch: [0]  [79400/80149]  eta: 0:00:26  lr: 0.000500  loss_labels: 5.0638 (6.2155)  loss: 4.9330 (6.2155)  time: 0.0351\n",
      "Epoch: [0]  [79500/80149]  eta: 0:00:23  lr: 0.000500  loss_labels: 5.1281 (6.2142)  loss: 5.2700 (6.2142)  time: 0.0370\n",
      "Epoch: [0]  [79600/80149]  eta: 0:00:19  lr: 0.000500  loss_labels: 5.0697 (6.2127)  loss: 4.8971 (6.2127)  time: 0.0357\n",
      "Epoch: [0]  [79700/80149]  eta: 0:00:16  lr: 0.000500  loss_labels: 5.0203 (6.2113)  loss: 4.8593 (6.2113)  time: 0.0347\n",
      "Epoch: [0]  [79800/80149]  eta: 0:00:12  lr: 0.000500  loss_labels: 5.0259 (6.2099)  loss: 4.9973 (6.2099)  time: 0.0370\n",
      "Epoch: [0]  [79900/80149]  eta: 0:00:08  lr: 0.000500  loss_labels: 5.1338 (6.2086)  loss: 5.1527 (6.2086)  time: 0.0351\n",
      "Epoch: [0]  [80000/80149]  eta: 0:00:05  lr: 0.000500  loss_labels: 5.1343 (6.2072)  loss: 5.0453 (6.2072)  time: 0.0355\n",
      "Epoch: [0]  [80100/80149]  eta: 0:00:01  lr: 0.000500  loss_labels: 5.1777 (6.2059)  loss: 5.2539 (6.2059)  time: 0.0407\n",
      "Epoch: [0]  [80148/80149]  eta: 0:00:00  lr: 0.000500  loss_labels: 5.1697 (6.2052)  loss: 5.0278 (6.2052)  time: 0.0397\n",
      "Epoch: [0] Total time: 0:48:02 (0.0360 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 5.1697 (6.2052)  loss: 5.0278 (6.2052)\n",
      "Test:  [   0/6069]  eta: 0:27:16  loss_labels: 4.4514 (4.4514)  loss: 4.4514 (4.4514)  time: 0.2697\n",
      "Traceback (most recent call last):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/BLT_models/main.py\", line 311, in <module>\n",
      "    main(0, 1, args)\n",
      "  File \"/engram/nklab/hossein/recurrent_models/BLT_models/main.py\", line 268, in main\n",
      "    val_ = evaluate(model_ddp, criterion, val_loader, args)\n",
      "  File \"/engram/nklab/hossein/recurrent_models/BLT_models/engine.py\", line 79, in evaluate\n",
      "    outputs = model(imgs)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/engram/nklab/hossein/recurrent_models/BLT_models/models/blt.py\", line 233, in forward\n",
      "    input = getattr(self, f'conv_{i}_{block}')(outputs[f'{i}'])\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 44.31 GiB total capacity; 42.22 GiB already allocated; 12.31 MiB free; 43.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python main.py --distributed 0 --save_model 1 --pool 'blur' --dataset 'imagenet_vggface2' --num_layers 6 --model 'blt_bl' --epochs 100 --lr .0005 --loss_choice 'weighted' --loss_gamma 0 --recurrent_steps 10 --lr_drop 100 --port '12379' --run 'maxpool' --batch_size 16 --port '12370'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/BLT_models/models/blt.py:230: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if conn_input is not 0:\n",
      "| distributed init (rank 0): env://\n",
      "| distributed init (rank 1): env://\n",
      "cuda\n",
      "100%|██████████████████████████████████████| 3890/3890 [00:06<00:00, 618.47it/s]\n",
      "100%|██████████████████████████████████████| 3890/3890 [00:06<00:00, 618.47it/s]\n",
      "100%|█████████████████████████████████████| 3890/3890 [00:02<00:00, 1602.47it/s]\n",
      "100%|█████████████████████████████████████| 3890/3890 [00:02<00:00, 1577.15it/s]\n",
      "56 28 2\n",
      "28 14 2\n",
      "14 7 2\n",
      "7 1 7\n",
      "1 1 1\n",
      "Number of model parameters: 12473650\n",
      "blt(\n",
      "  (conv_input): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (maxpool_input): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (non_lin_0): ReLU(inplace=True)\n",
      "  (norm_0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (output_0): Identity()\n",
      "  (conv_0_1): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (non_lin_1): ReLU(inplace=True)\n",
      "  (norm_1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (output_1): Identity()\n",
      "  (conv_1_2): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (non_lin_2): ReLU(inplace=True)\n",
      "  (norm_2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (output_2): Identity()\n",
      "  (conv_2_3): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (non_lin_3): ReLU(inplace=True)\n",
      "  (norm_3): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (output_3): Identity()\n",
      "  (conv_3_4): Sequential(\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "    (Unsqueeze): Unsqueeze()\n",
      "  )\n",
      "  (non_lin_4): ReLU(inplace=True)\n",
      "  (norm_4): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "  (output_4): Identity()\n",
      "  (conv_4_5): Sequential(\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (Unsqueeze): Unsqueeze()\n",
      "  )\n",
      "  (non_lin_5): ReLU(inplace=True)\n",
      "  (norm_5): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (output_5): Identity()\n",
      "  (read_out): Sequential(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=512, out_features=3890, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['conv_input.weight', 'conv_input.bias', 'norm_0.weight', 'norm_0.bias', 'conv_0_1.conv.weight', 'conv_0_1.conv.bias', 'norm_1.weight', 'norm_1.bias', 'conv_1_2.conv.weight', 'conv_1_2.conv.bias', 'norm_2.weight', 'norm_2.bias', 'conv_2_3.conv.weight', 'conv_2_3.conv.bias', 'norm_3.weight', 'norm_3.bias', 'conv_3_4.linear.weight', 'conv_3_4.linear.bias', 'norm_4.weight', 'norm_4.bias', 'conv_4_5.linear.weight', 'conv_4_5.linear.bias', 'norm_5.weight', 'norm_5.bias', 'read_out.linear.weight', 'read_out.linear.bias']\n",
      "Start training\n",
      "[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch: [0]  [    0/20033]  eta: 6 days, 16:37:29  lr: 0.000500  loss_labels: 8.3049 (8.3049)  loss: 8.3049 (8.3049)  time: 28.8648\n",
      "Epoch: [0]  [  100/20033]  eta: 1:44:36  lr: 0.000500  loss_labels: 8.4667 (8.4666)  loss: 8.5043 (8.4666)  time: 0.0274\n",
      "Epoch: [0]  [  200/20033]  eta: 0:57:26  lr: 0.000500  loss_labels: 8.4730 (8.4641)  loss: 8.4216 (8.4641)  time: 0.0510\n",
      "Epoch: [0]  [  300/20033]  eta: 0:41:02  lr: 0.000500  loss_labels: 8.3695 (8.4314)  loss: 8.3384 (8.4314)  time: 0.0240\n",
      "Epoch: [0]  [  400/20033]  eta: 0:32:47  lr: 0.000500  loss_labels: 8.2867 (8.3981)  loss: 8.2714 (8.3981)  time: 0.0248\n",
      "Epoch: [0]  [  500/20033]  eta: 0:28:17  lr: 0.000500  loss_labels: 8.2744 (8.3732)  loss: 8.2597 (8.3732)  time: 0.0266\n",
      "Epoch: [0]  [  600/20033]  eta: 0:25:16  lr: 0.000500  loss_labels: 8.2532 (8.3536)  loss: 8.2350 (8.3536)  time: 0.0257\n",
      "Epoch: [0]  [  700/20033]  eta: 0:22:45  lr: 0.000500  loss_labels: 8.2603 (8.3398)  loss: 8.2507 (8.3398)  time: 0.0274\n",
      "Epoch: [0]  [  800/20033]  eta: 0:20:49  lr: 0.000500  loss_labels: 8.2508 (8.3290)  loss: 8.2647 (8.3290)  time: 0.0242\n",
      "Epoch: [0]  [  900/20033]  eta: 0:19:41  lr: 0.000500  loss_labels: 8.2474 (8.3201)  loss: 8.2403 (8.3201)  time: 0.0253\n",
      "Epoch: [0]  [ 1000/20033]  eta: 0:18:51  lr: 0.000500  loss_labels: 8.2493 (8.3133)  loss: 8.2321 (8.3133)  time: 0.0902\n",
      "Epoch: [0]  [ 1100/20033]  eta: 0:17:47  lr: 0.000500  loss_labels: 8.2462 (8.3074)  loss: 8.2441 (8.3074)  time: 0.0249\n",
      "Epoch: [0]  [ 1200/20033]  eta: 0:16:53  lr: 0.000500  loss_labels: 8.2454 (8.3022)  loss: 8.2383 (8.3022)  time: 0.0279\n",
      "Epoch: [0]  [ 1300/20033]  eta: 0:16:27  lr: 0.000500  loss_labels: 8.2452 (8.2979)  loss: 8.2452 (8.2979)  time: 0.0248\n",
      "Epoch: [0]  [ 1400/20033]  eta: 0:15:44  lr: 0.000500  loss_labels: 8.2309 (8.2934)  loss: 8.2294 (8.2934)  time: 0.0262\n",
      "Epoch: [0]  [ 1500/20033]  eta: 0:15:14  lr: 0.000500  loss_labels: 8.2174 (8.2883)  loss: 8.2251 (8.2883)  time: 0.0275\n",
      "Epoch: [0]  [ 1600/20033]  eta: 0:14:50  lr: 0.000500  loss_labels: 8.1866 (8.2821)  loss: 8.1572 (8.2821)  time: 0.0515\n",
      "Epoch: [0]  [ 1700/20033]  eta: 0:14:20  lr: 0.000500  loss_labels: 8.1584 (8.2746)  loss: 8.1612 (8.2746)  time: 0.0248\n",
      "Epoch: [0]  [ 1800/20033]  eta: 0:13:53  lr: 0.000500  loss_labels: 8.1204 (8.2660)  loss: 8.1151 (8.2660)  time: 0.0266\n",
      "Epoch: [0]  [ 1900/20033]  eta: 0:13:35  lr: 0.000500  loss_labels: 8.0871 (8.2567)  loss: 8.0805 (8.2567)  time: 0.0254\n",
      "Epoch: [0]  [ 2000/20033]  eta: 0:13:13  lr: 0.000500  loss_labels: 8.0421 (8.2460)  loss: 8.0582 (8.2460)  time: 0.0248\n",
      "Epoch: [0]  [ 2100/20033]  eta: 0:12:59  lr: 0.000500  loss_labels: 8.0006 (8.2342)  loss: 8.0006 (8.2342)  time: 0.0229\n",
      "Epoch: [0]  [ 2200/20033]  eta: 0:12:40  lr: 0.000500  loss_labels: 7.9832 (8.2226)  loss: 7.9678 (8.2226)  time: 0.0253\n",
      "Epoch: [0]  [ 2300/20033]  eta: 0:12:24  lr: 0.000500  loss_labels: 7.9277 (8.2102)  loss: 7.9277 (8.2102)  time: 0.0344\n",
      "Epoch: [0]  [ 2400/20033]  eta: 0:12:07  lr: 0.000500  loss_labels: 7.9008 (8.1976)  loss: 7.9224 (8.1976)  time: 0.0239\n",
      "Epoch: [0]  [ 2500/20033]  eta: 0:12:00  lr: 0.000500  loss_labels: 7.8587 (8.1842)  loss: 7.8423 (8.1842)  time: 0.0234\n",
      "Epoch: [0]  [ 2600/20033]  eta: 0:11:46  lr: 0.000500  loss_labels: 7.8474 (8.1713)  loss: 7.8339 (8.1713)  time: 0.0252\n",
      "Epoch: [0]  [ 2700/20033]  eta: 0:11:36  lr: 0.000500  loss_labels: 7.8065 (8.1573)  loss: 7.8040 (8.1573)  time: 0.0263\n",
      "Epoch: [0]  [ 2800/20033]  eta: 0:11:23  lr: 0.000500  loss_labels: 7.7791 (8.1437)  loss: 7.7526 (8.1437)  time: 0.0226\n",
      "Epoch: [0]  [ 2900/20033]  eta: 0:11:10  lr: 0.000500  loss_labels: 7.7342 (8.1295)  loss: 7.7058 (8.1295)  time: 0.0246\n",
      "Epoch: [0]  [ 3000/20033]  eta: 0:11:03  lr: 0.000500  loss_labels: 7.7092 (8.1154)  loss: 7.7290 (8.1154)  time: 0.0256\n",
      "Epoch: [0]  [ 3100/20033]  eta: 0:10:52  lr: 0.000500  loss_labels: 7.6494 (8.1007)  loss: 7.6516 (8.1007)  time: 0.0265\n",
      "Epoch: [0]  [ 3200/20033]  eta: 0:10:47  lr: 0.000500  loss_labels: 7.6097 (8.0858)  loss: 7.5970 (8.0858)  time: 0.0252\n",
      "Epoch: [0]  [ 3300/20033]  eta: 0:10:37  lr: 0.000500  loss_labels: 7.5819 (8.0707)  loss: 7.5573 (8.0707)  time: 0.0272\n",
      "Epoch: [0]  [ 3400/20033]  eta: 0:10:29  lr: 0.000500  loss_labels: 7.5837 (8.0558)  loss: 7.5415 (8.0558)  time: 0.0264\n",
      "Epoch: [0]  [ 3500/20033]  eta: 0:10:19  lr: 0.000500  loss_labels: 7.4887 (8.0398)  loss: 7.4560 (8.0398)  time: 0.0273\n",
      "Epoch: [0]  [ 3600/20033]  eta: 0:10:16  lr: 0.000500  loss_labels: 7.4466 (8.0233)  loss: 7.4511 (8.0233)  time: 0.0262\n",
      "Epoch: [0]  [ 3700/20033]  eta: 0:10:09  lr: 0.000500  loss_labels: 7.3798 (8.0060)  loss: 7.3949 (8.0060)  time: 0.0485\n",
      "Epoch: [0]  [ 3800/20033]  eta: 0:10:00  lr: 0.000500  loss_labels: 7.3467 (7.9885)  loss: 7.2727 (7.9885)  time: 0.0277\n",
      "Epoch: [0]  [ 3900/20033]  eta: 0:09:53  lr: 0.000500  loss_labels: 7.2913 (7.9710)  loss: 7.2873 (7.9710)  time: 0.0258\n",
      "Epoch: [0]  [ 4000/20033]  eta: 0:09:45  lr: 0.000500  loss_labels: 7.2292 (7.9529)  loss: 7.1786 (7.9529)  time: 0.0254\n",
      "Epoch: [0]  [ 4100/20033]  eta: 0:09:39  lr: 0.000500  loss_labels: 7.1857 (7.9345)  loss: 7.1375 (7.9345)  time: 0.0261\n",
      "Epoch: [0]  [ 4200/20033]  eta: 0:09:31  lr: 0.000500  loss_labels: 7.1356 (7.9158)  loss: 7.1148 (7.9158)  time: 0.0265\n",
      "Epoch: [0]  [ 4300/20033]  eta: 0:09:26  lr: 0.000500  loss_labels: 7.1513 (7.8978)  loss: 7.1681 (7.8978)  time: 0.0262\n",
      "Epoch: [0]  [ 4400/20033]  eta: 0:09:18  lr: 0.000500  loss_labels: 7.0878 (7.8794)  loss: 7.0915 (7.8794)  time: 0.0247\n",
      "Epoch: [0]  [ 4500/20033]  eta: 0:09:13  lr: 0.000500  loss_labels: 7.0544 (7.8611)  loss: 7.0847 (7.8611)  time: 0.0253\n",
      "Epoch: [0]  [ 4600/20033]  eta: 0:09:08  lr: 0.000500  loss_labels: 7.0310 (7.8431)  loss: 7.0211 (7.8431)  time: 0.0333\n",
      "Epoch: [0]  [ 4700/20033]  eta: 0:09:06  lr: 0.000500  loss_labels: 6.9912 (7.8251)  loss: 6.8845 (7.8251)  time: 0.0359\n",
      "Epoch: [0]  [ 4800/20033]  eta: 0:09:00  lr: 0.000500  loss_labels: 6.9683 (7.8073)  loss: 6.9900 (7.8073)  time: 0.0298\n",
      "Epoch: [0]  [ 4900/20033]  eta: 0:08:57  lr: 0.000500  loss_labels: 6.9332 (7.7899)  loss: 6.9323 (7.7899)  time: 0.0242\n",
      "Epoch: [0]  [ 5000/20033]  eta: 0:08:51  lr: 0.000500  loss_labels: 6.9260 (7.7727)  loss: 6.9003 (7.7727)  time: 0.0263\n",
      "Epoch: [0]  [ 5100/20033]  eta: 0:08:45  lr: 0.000500  loss_labels: 6.9014 (7.7555)  loss: 6.8610 (7.7555)  time: 0.0296\n",
      "Epoch: [0]  [ 5200/20033]  eta: 0:08:40  lr: 0.000500  loss_labels: 6.8860 (7.7387)  loss: 6.8950 (7.7387)  time: 0.0271\n",
      "Epoch: [0]  [ 5300/20033]  eta: 0:08:34  lr: 0.000500  loss_labels: 6.8261 (7.7216)  loss: 6.8049 (7.7216)  time: 0.0274\n",
      "Epoch: [0]  [ 5400/20033]  eta: 0:08:31  lr: 0.000500  loss_labels: 6.7960 (7.7049)  loss: 6.7955 (7.7049)  time: 0.0299\n",
      "Epoch: [0]  [ 5500/20033]  eta: 0:08:28  lr: 0.000500  loss_labels: 6.7756 (7.6879)  loss: 6.6977 (7.6879)  time: 0.0297\n",
      "Epoch: [0]  [ 5600/20033]  eta: 0:08:23  lr: 0.000500  loss_labels: 6.7617 (7.6712)  loss: 6.6788 (7.6712)  time: 0.0268\n",
      "Epoch: [0]  [ 5700/20033]  eta: 0:08:18  lr: 0.000500  loss_labels: 6.7257 (7.6544)  loss: 6.7441 (7.6544)  time: 0.0288\n",
      "Epoch: [0]  [ 5800/20033]  eta: 0:08:14  lr: 0.000500  loss_labels: 6.6955 (7.6381)  loss: 6.7121 (7.6381)  time: 0.0387\n",
      "Epoch: [0]  [ 5900/20033]  eta: 0:08:11  lr: 0.000500  loss_labels: 6.6657 (7.6217)  loss: 6.7210 (7.6217)  time: 0.0293\n",
      "Epoch: [0]  [ 6000/20033]  eta: 0:08:07  lr: 0.000500  loss_labels: 6.5987 (7.6049)  loss: 6.5776 (7.6049)  time: 0.0267\n",
      "Epoch: [0]  [ 6100/20033]  eta: 0:08:03  lr: 0.000500  loss_labels: 6.5640 (7.5879)  loss: 6.5590 (7.5879)  time: 0.0261\n",
      "Epoch: [0]  [ 6200/20033]  eta: 0:08:01  lr: 0.000500  loss_labels: 6.5684 (7.5718)  loss: 6.5614 (7.5718)  time: 0.0280\n",
      "Epoch: [0]  [ 6300/20033]  eta: 0:07:58  lr: 0.000500  loss_labels: 6.5395 (7.5552)  loss: 6.4352 (7.5552)  time: 0.0829\n",
      "Epoch: [0]  [ 6400/20033]  eta: 0:07:57  lr: 0.000500  loss_labels: 6.5172 (7.5390)  loss: 6.4268 (7.5390)  time: 0.0306\n",
      "Epoch: [0]  [ 6500/20033]  eta: 0:07:53  lr: 0.000500  loss_labels: 6.4755 (7.5227)  loss: 6.4784 (7.5227)  time: 0.0318\n",
      "Epoch: [0]  [ 6600/20033]  eta: 0:07:51  lr: 0.000500  loss_labels: 6.4530 (7.5065)  loss: 6.3682 (7.5065)  time: 0.0575\n",
      "Epoch: [0]  [ 6700/20033]  eta: 0:07:46  lr: 0.000500  loss_labels: 6.4340 (7.4906)  loss: 6.4459 (7.4906)  time: 0.0266\n",
      "Epoch: [0]  [ 6800/20033]  eta: 0:07:41  lr: 0.000500  loss_labels: 6.4246 (7.4748)  loss: 6.4328 (7.4748)  time: 0.0291\n",
      "Epoch: [0]  [ 6900/20033]  eta: 0:07:38  lr: 0.000500  loss_labels: 6.3317 (7.4585)  loss: 6.3410 (7.4585)  time: 0.0297\n",
      "Epoch: [0]  [ 7000/20033]  eta: 0:07:35  lr: 0.000500  loss_labels: 6.3273 (7.4423)  loss: 6.2894 (7.4423)  time: 0.0353\n",
      "Epoch: [0]  [ 7100/20033]  eta: 0:07:31  lr: 0.000500  loss_labels: 6.2597 (7.4260)  loss: 6.2309 (7.4260)  time: 0.0263\n",
      "Epoch: [0]  [ 7200/20033]  eta: 0:07:27  lr: 0.000500  loss_labels: 6.3084 (7.4101)  loss: 6.2781 (7.4101)  time: 0.0265\n",
      "Epoch: [0]  [ 7300/20033]  eta: 0:07:23  lr: 0.000500  loss_labels: 6.2618 (7.3945)  loss: 6.3151 (7.3945)  time: 0.0383\n",
      "Epoch: [0]  [ 7400/20033]  eta: 0:07:19  lr: 0.000500  loss_labels: 6.1971 (7.3787)  loss: 6.1561 (7.3787)  time: 0.0306\n",
      "Epoch: [0]  [ 7500/20033]  eta: 0:07:16  lr: 0.000500  loss_labels: 6.2105 (7.3634)  loss: 6.2671 (7.3634)  time: 0.0255\n",
      "Epoch: [0]  [ 7600/20033]  eta: 0:07:12  lr: 0.000500  loss_labels: 6.1970 (7.3480)  loss: 6.2246 (7.3480)  time: 0.0433\n",
      "Epoch: [0]  [ 7700/20033]  eta: 0:07:08  lr: 0.000500  loss_labels: 6.1218 (7.3323)  loss: 6.0332 (7.3323)  time: 0.0271\n",
      "Epoch: [0]  [ 7800/20033]  eta: 0:07:04  lr: 0.000500  loss_labels: 6.1024 (7.3167)  loss: 6.0574 (7.3167)  time: 0.0226\n",
      "Epoch: [0]  [ 7900/20033]  eta: 0:07:01  lr: 0.000500  loss_labels: 6.0596 (7.3012)  loss: 6.0242 (7.3012)  time: 0.0396\n",
      "Epoch: [0]  [ 8000/20033]  eta: 0:06:56  lr: 0.000500  loss_labels: 6.0397 (7.2853)  loss: 6.0332 (7.2853)  time: 0.0261\n",
      "Epoch: [0]  [ 8100/20033]  eta: 0:06:53  lr: 0.000500  loss_labels: 6.0538 (7.2701)  loss: 6.0219 (7.2701)  time: 0.0297\n",
      "Epoch: [0]  [ 8200/20033]  eta: 0:06:49  lr: 0.000500  loss_labels: 6.0398 (7.2549)  loss: 5.9075 (7.2549)  time: 0.0256\n",
      "Epoch: [0]  [ 8300/20033]  eta: 0:06:45  lr: 0.000500  loss_labels: 5.9924 (7.2399)  loss: 5.9338 (7.2399)  time: 0.0269\n",
      "Epoch: [0]  [ 8400/20033]  eta: 0:06:40  lr: 0.000500  loss_labels: 5.9345 (7.2244)  loss: 5.9828 (7.2244)  time: 0.0304\n",
      "Epoch: [0]  [ 8500/20033]  eta: 0:06:35  lr: 0.000500  loss_labels: 5.9132 (7.2089)  loss: 5.9116 (7.2089)  time: 0.0224\n",
      "Epoch: [0]  [ 8600/20033]  eta: 0:06:32  lr: 0.000500  loss_labels: 5.8797 (7.1935)  loss: 5.8797 (7.1935)  time: 0.0245\n",
      "Epoch: [0]  [ 8700/20033]  eta: 0:06:27  lr: 0.000500  loss_labels: 5.8365 (7.1779)  loss: 5.7568 (7.1779)  time: 0.0272\n",
      "Epoch: [0]  [ 8800/20033]  eta: 0:06:25  lr: 0.000500  loss_labels: 5.8599 (7.1628)  loss: 5.8306 (7.1628)  time: 0.0308\n",
      "Epoch: [0]  [ 8900/20033]  eta: 0:06:21  lr: 0.000500  loss_labels: 5.7541 (7.1474)  loss: 5.7382 (7.1474)  time: 0.0444\n",
      "Epoch: [0]  [ 9000/20033]  eta: 0:06:17  lr: 0.000500  loss_labels: 5.7802 (7.1327)  loss: 5.7518 (7.1327)  time: 0.0416\n",
      "Epoch: [0]  [ 9100/20033]  eta: 0:06:13  lr: 0.000500  loss_labels: 5.8202 (7.1183)  loss: 5.7398 (7.1183)  time: 0.0276\n",
      "Epoch: [0]  [ 9200/20033]  eta: 0:06:09  lr: 0.000500  loss_labels: 5.6956 (7.1032)  loss: 5.7112 (7.1032)  time: 0.0250\n",
      "Epoch: [0]  [ 9300/20033]  eta: 0:06:07  lr: 0.000500  loss_labels: 5.6676 (7.0882)  loss: 5.6275 (7.0882)  time: 0.0268\n",
      "Epoch: [0]  [ 9400/20033]  eta: 0:06:04  lr: 0.000500  loss_labels: 5.7003 (7.0734)  loss: 5.6980 (7.0734)  time: 0.0248\n",
      "Epoch: [0]  [ 9500/20033]  eta: 0:06:06  lr: 0.000500  loss_labels: 5.6418 (7.0584)  loss: 5.6109 (7.0584)  time: 0.1100\n",
      "Epoch: [0]  [ 9600/20033]  eta: 0:06:08  lr: 0.000500  loss_labels: 5.6475 (7.0433)  loss: 5.5526 (7.0433)  time: 0.0649\n",
      "Epoch: [0]  [ 9700/20033]  eta: 0:06:05  lr: 0.000500  loss_labels: 5.6037 (7.0285)  loss: 5.5482 (7.0285)  time: 0.0661\n",
      "Epoch: [0]  [ 9800/20033]  eta: 0:06:06  lr: 0.000500  loss_labels: 5.5709 (7.0138)  loss: 5.5959 (7.0138)  time: 0.0614\n",
      "Epoch: [0]  [ 9900/20033]  eta: 0:06:06  lr: 0.000500  loss_labels: 5.5356 (6.9991)  loss: 5.5573 (6.9991)  time: 0.0800\n",
      "Epoch: [0]  [10000/20033]  eta: 0:06:06  lr: 0.000500  loss_labels: 5.4761 (6.9841)  loss: 5.3692 (6.9841)  time: 0.0522\n",
      "Epoch: [0]  [10100/20033]  eta: 0:06:05  lr: 0.000500  loss_labels: 5.4784 (6.9692)  loss: 5.4927 (6.9692)  time: 0.0734\n",
      "Epoch: [0]  [10200/20033]  eta: 0:06:01  lr: 0.000500  loss_labels: 5.4592 (6.9544)  loss: 5.4235 (6.9544)  time: 0.0248\n",
      "Epoch: [0]  [10300/20033]  eta: 0:05:57  lr: 0.000500  loss_labels: 5.4515 (6.9399)  loss: 5.4388 (6.9399)  time: 0.0246\n",
      "Epoch: [0]  [10400/20033]  eta: 0:05:52  lr: 0.000500  loss_labels: 5.4377 (6.9252)  loss: 5.4677 (6.9252)  time: 0.0270\n",
      "Epoch: [0]  [10500/20033]  eta: 0:05:47  lr: 0.000500  loss_labels: 5.4377 (6.9110)  loss: 5.4416 (6.9110)  time: 0.0243\n",
      "Epoch: [0]  [10600/20033]  eta: 0:05:43  lr: 0.000500  loss_labels: 5.3073 (6.8960)  loss: 5.3768 (6.8960)  time: 0.0243\n",
      "Epoch: [0]  [10700/20033]  eta: 0:05:39  lr: 0.000500  loss_labels: 5.3478 (6.8814)  loss: 5.2712 (6.8814)  time: 0.0230\n",
      "Epoch: [0]  [10800/20033]  eta: 0:05:34  lr: 0.000500  loss_labels: 5.3180 (6.8667)  loss: 5.3308 (6.8667)  time: 0.0271\n",
      "Epoch: [0]  [10900/20033]  eta: 0:05:30  lr: 0.000500  loss_labels: 5.2772 (6.8523)  loss: 5.2341 (6.8523)  time: 0.0266\n",
      "Epoch: [0]  [11000/20033]  eta: 0:05:25  lr: 0.000500  loss_labels: 5.2590 (6.8378)  loss: 5.3386 (6.8378)  time: 0.0285\n",
      "Epoch: [0]  [11100/20033]  eta: 0:05:21  lr: 0.000500  loss_labels: 5.2013 (6.8234)  loss: 5.2032 (6.8234)  time: 0.0271\n",
      "Epoch: [0]  [11200/20033]  eta: 0:05:17  lr: 0.000500  loss_labels: 5.2533 (6.8091)  loss: 5.2043 (6.8091)  time: 0.0272\n",
      "Epoch: [0]  [11300/20033]  eta: 0:05:13  lr: 0.000500  loss_labels: 5.1965 (6.7950)  loss: 5.1266 (6.7950)  time: 0.0281\n",
      "Epoch: [0]  [11400/20033]  eta: 0:05:08  lr: 0.000500  loss_labels: 5.2123 (6.7812)  loss: 5.1891 (6.7812)  time: 0.0272\n",
      "Epoch: [0]  [11500/20033]  eta: 0:05:05  lr: 0.000500  loss_labels: 5.0885 (6.7666)  loss: 5.0046 (6.7666)  time: 0.0278\n",
      "Epoch: [0]  [11600/20033]  eta: 0:05:00  lr: 0.000500  loss_labels: 5.0564 (6.7522)  loss: 5.0550 (6.7522)  time: 0.0251\n",
      "Epoch: [0]  [11700/20033]  eta: 0:04:56  lr: 0.000500  loss_labels: 5.1342 (6.7383)  loss: 5.0570 (6.7383)  time: 0.0264\n",
      "Epoch: [0]  [11800/20033]  eta: 0:04:52  lr: 0.000500  loss_labels: 5.1174 (6.7244)  loss: 4.9387 (6.7244)  time: 0.0244\n",
      "Epoch: [0]  [11900/20033]  eta: 0:04:49  lr: 0.000500  loss_labels: 5.0226 (6.7103)  loss: 4.9796 (6.7103)  time: 0.0257\n",
      "Epoch: [0]  [12000/20033]  eta: 0:04:44  lr: 0.000500  loss_labels: 5.0943 (6.6966)  loss: 5.0769 (6.6966)  time: 0.0247\n",
      "Epoch: [0]  [12100/20033]  eta: 0:04:41  lr: 0.000500  loss_labels: 5.0252 (6.6828)  loss: 5.0014 (6.6828)  time: 0.0480\n",
      "Epoch: [0]  [12200/20033]  eta: 0:04:37  lr: 0.000500  loss_labels: 5.0101 (6.6692)  loss: 4.9322 (6.6692)  time: 0.0250\n",
      "Epoch: [0]  [12300/20033]  eta: 0:04:33  lr: 0.000500  loss_labels: 4.9965 (6.6556)  loss: 4.9612 (6.6556)  time: 0.0266\n",
      "Epoch: [0]  [12400/20033]  eta: 0:04:29  lr: 0.000500  loss_labels: 4.9040 (6.6418)  loss: 4.9457 (6.6418)  time: 0.0240\n",
      "Epoch: [0]  [12500/20033]  eta: 0:04:25  lr: 0.000500  loss_labels: 4.9506 (6.6283)  loss: 4.9203 (6.6283)  time: 0.0219\n",
      "Epoch: [0]  [12600/20033]  eta: 0:04:21  lr: 0.000500  loss_labels: 4.8651 (6.6145)  loss: 4.8187 (6.6145)  time: 0.0243\n",
      "Epoch: [0]  [12700/20033]  eta: 0:04:17  lr: 0.000500  loss_labels: 4.8397 (6.6008)  loss: 4.9835 (6.6008)  time: 0.0268\n",
      "Epoch: [0]  [12800/20033]  eta: 0:04:13  lr: 0.000500  loss_labels: 4.8238 (6.5873)  loss: 4.8035 (6.5873)  time: 0.0228\n",
      "Epoch: [0]  [12900/20033]  eta: 0:04:09  lr: 0.000500  loss_labels: 4.8444 (6.5739)  loss: 4.7250 (6.5739)  time: 0.0353\n",
      "Epoch: [0]  [13000/20033]  eta: 0:04:05  lr: 0.000500  loss_labels: 4.8660 (6.5605)  loss: 4.7475 (6.5605)  time: 0.0216\n",
      "Epoch: [0]  [13100/20033]  eta: 0:04:01  lr: 0.000500  loss_labels: 4.7731 (6.5472)  loss: 4.7513 (6.5472)  time: 0.0247\n",
      "Epoch: [0]  [13200/20033]  eta: 0:03:58  lr: 0.000500  loss_labels: 4.7789 (6.5339)  loss: 4.6637 (6.5339)  time: 0.0551\n",
      "Epoch: [0]  [13300/20033]  eta: 0:03:54  lr: 0.000500  loss_labels: 4.7353 (6.5208)  loss: 4.6865 (6.5208)  time: 0.0249\n",
      "Epoch: [0]  [13400/20033]  eta: 0:03:50  lr: 0.000500  loss_labels: 4.6683 (6.5071)  loss: 4.6356 (6.5071)  time: 0.0230\n",
      "Epoch: [0]  [13500/20033]  eta: 0:03:46  lr: 0.000500  loss_labels: 4.7109 (6.4940)  loss: 4.7352 (6.4940)  time: 0.0250\n",
      "Epoch: [0]  [13600/20033]  eta: 0:03:43  lr: 0.000500  loss_labels: 4.7669 (6.4810)  loss: 4.7290 (6.4810)  time: 0.0773\n",
      "Epoch: [0]  [13700/20033]  eta: 0:03:39  lr: 0.000500  loss_labels: 4.7242 (6.4681)  loss: 4.6988 (6.4681)  time: 0.0260\n",
      "Epoch: [0]  [13800/20033]  eta: 0:03:35  lr: 0.000500  loss_labels: 4.7046 (6.4553)  loss: 4.7622 (6.4553)  time: 0.0247\n",
      "Epoch: [0]  [13900/20033]  eta: 0:03:31  lr: 0.000500  loss_labels: 4.6741 (6.4423)  loss: 4.6887 (6.4423)  time: 0.0254\n",
      "Epoch: [0]  [14000/20033]  eta: 0:03:28  lr: 0.000500  loss_labels: 4.6729 (6.4296)  loss: 4.6936 (6.4296)  time: 0.0217\n",
      "Epoch: [0]  [14100/20033]  eta: 0:03:24  lr: 0.000500  loss_labels: 4.6058 (6.4168)  loss: 4.5705 (6.4168)  time: 0.0214\n",
      "Epoch: [0]  [14200/20033]  eta: 0:03:20  lr: 0.000500  loss_labels: 4.6352 (6.4043)  loss: 4.6002 (6.4043)  time: 0.0230\n",
      "Epoch: [0]  [14300/20033]  eta: 0:03:16  lr: 0.000500  loss_labels: 4.6771 (6.3922)  loss: 4.6292 (6.3922)  time: 0.0232\n",
      "Epoch: [0]  [14400/20033]  eta: 0:03:13  lr: 0.000500  loss_labels: 4.6568 (6.3797)  loss: 4.6962 (6.3797)  time: 0.0255\n",
      "Epoch: [0]  [14500/20033]  eta: 0:03:09  lr: 0.000500  loss_labels: 4.5407 (6.3671)  loss: 4.5297 (6.3671)  time: 0.0260\n",
      "Epoch: [0]  [14600/20033]  eta: 0:03:05  lr: 0.000500  loss_labels: 4.6038 (6.3549)  loss: 4.6061 (6.3549)  time: 0.0249\n",
      "Epoch: [0]  [14700/20033]  eta: 0:03:02  lr: 0.000500  loss_labels: 4.5912 (6.3428)  loss: 4.4594 (6.3428)  time: 0.0285\n",
      "Epoch: [0]  [14800/20033]  eta: 0:02:58  lr: 0.000500  loss_labels: 4.4721 (6.3303)  loss: 4.4011 (6.3303)  time: 0.0266\n",
      "Epoch: [0]  [14900/20033]  eta: 0:02:55  lr: 0.000500  loss_labels: 4.5075 (6.3180)  loss: 4.5182 (6.3180)  time: 0.0225\n",
      "Epoch: [0]  [15000/20033]  eta: 0:02:51  lr: 0.000500  loss_labels: 4.4977 (6.3060)  loss: 4.6121 (6.3060)  time: 0.0291\n",
      "Epoch: [0]  [15100/20033]  eta: 0:02:47  lr: 0.000500  loss_labels: 4.5087 (6.2941)  loss: 4.3831 (6.2941)  time: 0.0252\n",
      "Epoch: [0]  [15200/20033]  eta: 0:02:44  lr: 0.000500  loss_labels: 4.3839 (6.2818)  loss: 4.3642 (6.2818)  time: 0.0246\n",
      "Epoch: [0]  [15300/20033]  eta: 0:02:40  lr: 0.000500  loss_labels: 4.3974 (6.2697)  loss: 4.4444 (6.2697)  time: 0.0244\n",
      "Epoch: [0]  [15400/20033]  eta: 0:02:37  lr: 0.000500  loss_labels: 4.4503 (6.2580)  loss: 4.3589 (6.2580)  time: 0.0299\n",
      "Epoch: [0]  [15500/20033]  eta: 0:02:33  lr: 0.000500  loss_labels: 4.4435 (6.2462)  loss: 4.3673 (6.2462)  time: 0.0241\n",
      "Epoch: [0]  [15600/20033]  eta: 0:02:30  lr: 0.000500  loss_labels: 4.3491 (6.2342)  loss: 4.4354 (6.2342)  time: 0.0260\n",
      "Epoch: [0]  [15700/20033]  eta: 0:02:26  lr: 0.000500  loss_labels: 4.3612 (6.2223)  loss: 4.2680 (6.2223)  time: 0.0236\n",
      "Epoch: [0]  [15800/20033]  eta: 0:02:23  lr: 0.000500  loss_labels: 4.3456 (6.2106)  loss: 4.3384 (6.2106)  time: 0.0289\n",
      "Epoch: [0]  [15900/20033]  eta: 0:02:19  lr: 0.000500  loss_labels: 4.4551 (6.1994)  loss: 4.4232 (6.1994)  time: 0.0240\n",
      "Epoch: [0]  [16000/20033]  eta: 0:02:16  lr: 0.000500  loss_labels: 4.3211 (6.1877)  loss: 4.3486 (6.1877)  time: 0.0273\n",
      "Epoch: [0]  [16100/20033]  eta: 0:02:13  lr: 0.000500  loss_labels: 4.2948 (6.1760)  loss: 4.2712 (6.1760)  time: 0.0263\n",
      "Epoch: [0]  [16200/20033]  eta: 0:02:09  lr: 0.000500  loss_labels: 4.2755 (6.1644)  loss: 4.2533 (6.1644)  time: 0.0282\n",
      "Epoch: [0]  [16300/20033]  eta: 0:02:06  lr: 0.000500  loss_labels: 4.3177 (6.1530)  loss: 4.3289 (6.1530)  time: 0.0264\n",
      "Epoch: [0]  [16400/20033]  eta: 0:02:02  lr: 0.000500  loss_labels: 4.2763 (6.1418)  loss: 4.3460 (6.1418)  time: 0.0340\n",
      "Epoch: [0]  [16500/20033]  eta: 0:01:59  lr: 0.000500  loss_labels: 4.2833 (6.1306)  loss: 4.2080 (6.1306)  time: 0.0255\n",
      "Epoch: [0]  [16600/20033]  eta: 0:01:55  lr: 0.000500  loss_labels: 4.2766 (6.1195)  loss: 4.2415 (6.1195)  time: 0.0279\n",
      "Epoch: [0]  [16700/20033]  eta: 0:01:52  lr: 0.000500  loss_labels: 4.1963 (6.1080)  loss: 4.1227 (6.1080)  time: 0.0269\n",
      "Epoch: [0]  [16800/20033]  eta: 0:01:48  lr: 0.000500  loss_labels: 4.1973 (6.0967)  loss: 4.2728 (6.0967)  time: 0.0244\n",
      "Epoch: [0]  [16900/20033]  eta: 0:01:45  lr: 0.000500  loss_labels: 4.2140 (6.0853)  loss: 4.2579 (6.0853)  time: 0.0272\n",
      "Epoch: [0]  [17000/20033]  eta: 0:01:41  lr: 0.000500  loss_labels: 4.2372 (6.0743)  loss: 4.3693 (6.0743)  time: 0.0234\n",
      "Epoch: [0]  [17100/20033]  eta: 0:01:38  lr: 0.000500  loss_labels: 4.1581 (6.0631)  loss: 4.2134 (6.0631)  time: 0.0442\n",
      "Epoch: [0]  [17200/20033]  eta: 0:01:34  lr: 0.000500  loss_labels: 4.1846 (6.0521)  loss: 4.1212 (6.0521)  time: 0.0255\n",
      "Epoch: [0]  [17300/20033]  eta: 0:01:31  lr: 0.000500  loss_labels: 4.1579 (6.0413)  loss: 4.1542 (6.0413)  time: 0.0286\n",
      "Epoch: [0]  [17400/20033]  eta: 0:01:28  lr: 0.000500  loss_labels: 4.1534 (6.0307)  loss: 4.0211 (6.0307)  time: 0.0242\n",
      "Epoch: [0]  [17500/20033]  eta: 0:01:24  lr: 0.000500  loss_labels: 4.1254 (6.0198)  loss: 4.1641 (6.0198)  time: 0.0222\n",
      "Epoch: [0]  [17600/20033]  eta: 0:01:21  lr: 0.000500  loss_labels: 4.1240 (6.0090)  loss: 3.9926 (6.0090)  time: 0.0277\n",
      "Epoch: [0]  [17700/20033]  eta: 0:01:17  lr: 0.000500  loss_labels: 4.0820 (5.9981)  loss: 4.0820 (5.9981)  time: 0.0245\n",
      "Epoch: [0]  [17800/20033]  eta: 0:01:14  lr: 0.000500  loss_labels: 4.0373 (5.9871)  loss: 3.9681 (5.9871)  time: 0.0254\n",
      "Epoch: [0]  [17900/20033]  eta: 0:01:10  lr: 0.000500  loss_labels: 4.1227 (5.9767)  loss: 4.1048 (5.9767)  time: 0.0230\n",
      "Epoch: [0]  [18000/20033]  eta: 0:01:07  lr: 0.000500  loss_labels: 4.0603 (5.9660)  loss: 4.0810 (5.9660)  time: 0.0250\n",
      "Epoch: [0]  [18100/20033]  eta: 0:01:04  lr: 0.000500  loss_labels: 3.9986 (5.9552)  loss: 4.0723 (5.9552)  time: 0.0281\n",
      "Epoch: [0]  [18200/20033]  eta: 0:01:00  lr: 0.000500  loss_labels: 4.0173 (5.9445)  loss: 3.8098 (5.9445)  time: 0.0255\n",
      "Epoch: [0]  [18300/20033]  eta: 0:00:57  lr: 0.000500  loss_labels: 3.9684 (5.9338)  loss: 4.1898 (5.9338)  time: 0.0262\n",
      "Epoch: [0]  [18400/20033]  eta: 0:00:54  lr: 0.000500  loss_labels: 3.9744 (5.9233)  loss: 3.9892 (5.9233)  time: 0.0251\n",
      "Epoch: [0]  [18500/20033]  eta: 0:00:50  lr: 0.000500  loss_labels: 3.9560 (5.9128)  loss: 3.8661 (5.9128)  time: 0.0245\n",
      "Epoch: [0]  [18600/20033]  eta: 0:00:47  lr: 0.000500  loss_labels: 3.9993 (5.9024)  loss: 3.9054 (5.9024)  time: 0.0360\n",
      "Epoch: [0]  [18700/20033]  eta: 0:00:44  lr: 0.000500  loss_labels: 3.8708 (5.8919)  loss: 3.8168 (5.8919)  time: 0.0255\n",
      "Epoch: [0]  [18800/20033]  eta: 0:00:40  lr: 0.000500  loss_labels: 3.9285 (5.8815)  loss: 3.6959 (5.8815)  time: 0.0245\n",
      "Epoch: [0]  [18900/20033]  eta: 0:00:37  lr: 0.000500  loss_labels: 3.9119 (5.8711)  loss: 3.9193 (5.8711)  time: 0.0289\n",
      "Epoch: [0]  [19000/20033]  eta: 0:00:34  lr: 0.000500  loss_labels: 3.8738 (5.8607)  loss: 3.8063 (5.8607)  time: 0.0259\n",
      "Epoch: [0]  [19100/20033]  eta: 0:00:30  lr: 0.000500  loss_labels: 3.8791 (5.8504)  loss: 3.9042 (5.8504)  time: 0.0235\n",
      "Epoch: [0]  [19200/20033]  eta: 0:00:27  lr: 0.000500  loss_labels: 3.8589 (5.8403)  loss: 3.8937 (5.8403)  time: 0.0249\n",
      "Epoch: [0]  [19300/20033]  eta: 0:00:24  lr: 0.000500  loss_labels: 3.9163 (5.8303)  loss: 3.9163 (5.8303)  time: 0.0244\n",
      "Epoch: [0]  [19400/20033]  eta: 0:00:20  lr: 0.000500  loss_labels: 3.8288 (5.8201)  loss: 3.7817 (5.8201)  time: 0.0293\n",
      "Epoch: [0]  [19500/20033]  eta: 0:00:17  lr: 0.000500  loss_labels: 3.8861 (5.8100)  loss: 3.8998 (5.8100)  time: 0.0265\n",
      "Epoch: [0]  [19600/20033]  eta: 0:00:14  lr: 0.000500  loss_labels: 3.8569 (5.8001)  loss: 3.8839 (5.8001)  time: 0.0228\n",
      "Epoch: [0]  [19700/20033]  eta: 0:00:10  lr: 0.000500  loss_labels: 3.8164 (5.7901)  loss: 3.6954 (5.7901)  time: 0.0258\n",
      "Epoch: [0]  [19800/20033]  eta: 0:00:07  lr: 0.000500  loss_labels: 3.8367 (5.7802)  loss: 4.0280 (5.7802)  time: 0.0257\n",
      "Epoch: [0]  [19900/20033]  eta: 0:00:04  lr: 0.000500  loss_labels: 3.7244 (5.7700)  loss: 3.8007 (5.7700)  time: 0.0242\n",
      "Epoch: [0]  [20000/20033]  eta: 0:00:01  lr: 0.000500  loss_labels: 3.8570 (5.7603)  loss: 3.8400 (5.7603)  time: 0.0257\n",
      "Epoch: [0]  [20032/20033]  eta: 0:00:00  lr: 0.000500  loss_labels: 3.8400 (5.7573)  loss: 3.8187 (5.7573)  time: 0.0561\n",
      "Epoch: [0] Total time: 0:10:57 (0.0328 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 3.8400 (5.7573)  loss: 3.8187 (5.7573)\n",
      "Test:  [   0/2257]  eta: 14:23:24  loss_labels: 4.9762 (4.9762)  loss: 4.9762 (4.9762)  time: 22.9529\n",
      "Test:  [ 100/2257]  eta: 0:09:12  loss_labels: 4.3226 (4.2962)  loss: 3.7932 (4.2962)  time: 0.0311\n",
      "Test:  [ 200/2257]  eta: 0:04:52  loss_labels: 4.3390 (4.3549)  loss: 4.2643 (4.3549)  time: 0.0279\n",
      "Test:  [ 300/2257]  eta: 0:03:26  loss_labels: 4.3748 (4.4016)  loss: 3.8297 (4.4016)  time: 0.0260\n",
      "Test:  [ 400/2257]  eta: 0:02:44  loss_labels: 4.4281 (4.4064)  loss: 3.5035 (4.4064)  time: 0.0225\n",
      "Test:  [ 500/2257]  eta: 0:02:13  loss_labels: 4.2567 (4.3783)  loss: 4.2641 (4.3783)  time: 0.0258\n",
      "Test:  [ 600/2257]  eta: 0:01:51  loss_labels: 4.3504 (4.3747)  loss: 4.2068 (4.3747)  time: 0.0262\n",
      "Test:  [ 700/2257]  eta: 0:01:35  loss_labels: 4.0861 (4.3401)  loss: 4.0861 (4.3401)  time: 0.0241\n",
      "Test:  [ 800/2257]  eta: 0:01:23  loss_labels: 4.5538 (4.3638)  loss: 4.7661 (4.3638)  time: 0.0247\n",
      "Test:  [ 900/2257]  eta: 0:01:14  loss_labels: 4.0826 (4.3340)  loss: 3.7875 (4.3340)  time: 0.0616\n",
      "Test:  [1000/2257]  eta: 0:01:05  loss_labels: 4.1900 (4.3215)  loss: 4.2762 (4.3215)  time: 0.0254\n",
      "Test:  [1100/2257]  eta: 0:00:57  loss_labels: 4.4190 (4.3220)  loss: 4.0312 (4.3220)  time: 0.0257\n",
      "Test:  [1200/2257]  eta: 0:00:50  loss_labels: 4.1557 (4.3135)  loss: 4.3424 (4.3135)  time: 0.0245\n",
      "Test:  [1300/2257]  eta: 0:00:44  loss_labels: 4.1427 (4.3040)  loss: 4.3673 (4.3040)  time: 0.0255\n",
      "Test:  [1400/2257]  eta: 0:00:39  loss_labels: 4.6351 (4.3262)  loss: 4.6803 (4.3262)  time: 0.0633\n",
      "Test:  [1500/2257]  eta: 0:00:33  loss_labels: 4.3443 (4.3302)  loss: 4.5852 (4.3302)  time: 0.0248\n",
      "Test:  [1600/2257]  eta: 0:00:28  loss_labels: 4.2331 (4.3323)  loss: 4.5187 (4.3323)  time: 0.0252\n",
      "Test:  [1700/2257]  eta: 0:00:23  loss_labels: 3.9997 (4.3143)  loss: 4.7105 (4.3143)  time: 0.0264\n",
      "Test:  [1800/2257]  eta: 0:00:18  loss_labels: 4.0008 (4.3087)  loss: 4.0659 (4.3087)  time: 0.0258\n",
      "Test:  [1900/2257]  eta: 0:00:14  loss_labels: 3.9778 (4.2936)  loss: 4.0604 (4.2936)  time: 0.0233\n",
      "Test:  [2000/2257]  eta: 0:00:10  loss_labels: 4.2919 (4.3001)  loss: 4.2226 (4.3001)  time: 0.0261\n",
      "Test:  [2100/2257]  eta: 0:00:06  loss_labels: 4.3197 (4.2972)  loss: 3.9265 (4.2972)  time: 0.0266\n",
      "Test:  [2200/2257]  eta: 0:00:02  loss_labels: 4.3849 (4.2985)  loss: 4.8077 (4.2985)  time: 0.0376\n",
      "Test:  [2256/2257]  eta: 0:00:00  loss_labels: 4.2921 (4.2983)  loss: 4.1492 (4.2983)  time: 0.0393\n",
      "Test: Total time: 0:01:29 (0.0395 s / it)\n",
      "Averaged stats: loss_labels: 4.2921 (4.2983)  loss: 4.1492 (4.2983)\n",
      "acc: 0.21281781792640686\n",
      "top 1 and top 5 accuracies {'top1': 0.2128178142137041, 'top5': 0.4121752617293525, 'loss': tensor(0.1343, device='cuda:0')}\n",
      "Epoch: [1]  [    0/20033]  eta: 9 days, 21:48:52  lr: 0.000500  loss_labels: 3.8955 (3.8955)  loss: 3.8955 (3.8955)  time: 42.7361\n",
      "Epoch: [1]  [  100/20033]  eta: 2:31:10  lr: 0.000500  loss_labels: 3.7906 (3.8072)  loss: 3.7699 (3.8072)  time: 0.0283\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/BLT_models/main.py\", line 304, in <module>\n",
      "    mp.spawn(main, args=(args.world_size, args), nprocs=args.world_size)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 281, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 237, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 117, in join\n",
      "    ready = multiprocessing.connection.wait(\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# --wandb_p 'vggface2' --wandb_r 'bl_6_run5' \n",
    "!python main.py --distributed 1 --save_model 1 --dataset 'imagenet_vggface2' --num_layers 6 --model 'blt_b_top2linear' --epochs 100 --lr .0005 --loss_choice 'weighted' --loss_gamma 0.2 --recurrent_steps 10 --batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --save_model 0 --dataset 'imagenet' --wandb_p 'imagenet' --wandb_r 'blt_bl'  --model 'blt_bl' --epochs 100 --lr .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:3: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/tmp/ipykernel_71418/692844870.py:3: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  print(a is not 0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.zeros(3,3)\n",
    "print(a is not 0)\n",
    "\n",
    "_top2linear\n",
    "--epochs 100 --lr .0005 --loss_choice 'weighted' --loss_gamma 0 --recurrent_steps 10 --lr_drop 100 --port '12370' --run 35 --batch_size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3996127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'], returncode=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "script = f'''#!/bin/sh\n",
    "#\n",
    "#\n",
    "#SBATCH --account=nklab\n",
    "#SBATCH --job-name=blnd  # The job name.\n",
    "#SBATCH --gres=gpu:4\n",
    "\n",
    "#SBATCH --cpus-per-task=20\n",
    "\n",
    "ml load anaconda3-2023.07\n",
    "\n",
    "cd /engram/nklab/hossein/recurrent_models/BLT_models/\n",
    "\n",
    "conda activate diff\n",
    "\n",
    "python main.py --distributed 1 --save_model 0 --dataset 'imagenet' --num_layers 6 --model 'blt_bl' --wandb_p 'imagenet' --wandb_r 'blt_bl' --epochs 100 --lr .0005 --loss_choice 'weighted' --loss_gamma 0 --recurrent_steps 10 --lr_drop 100 --port '12390' --run 'blt_bl_nodrop' --pool 'max' --batch_size 32\n",
    "\n",
    "'''\n",
    "\n",
    "bash_script_path = \"/engram/nklab/hossein/batch_scripts/imagenet_nb.sh\"\n",
    "os.chdir('/engram/nklab/hossein/batch_scripts/')\n",
    "\n",
    "with open(bash_script_path, \"w+\") as bash_script_file:\n",
    "    bash_script_file.write(script)\n",
    "\n",
    "subprocess.run(['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python main.py --save_model 1 --dataset 'vggface2' --wandb_p 'vggface2' --wandb_r 'b_small_0.0005'  --model 'blt_b' --num_layers 6 --epochs 100 --lr .0005 --lr_drop 10 --port '12391' --run 22\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGGFace2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3730795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "script = f'''#!/bin/sh\n",
    "#\n",
    "#\n",
    "#SBATCH --account=nklab\n",
    "#SBATCH --job-name=blt_vgg  # The job name.\n",
    "#SBATCH --gres=gpu:3\n",
    "#SBATCH --nodelist=ax26\n",
    "#SBATCH --cpus-per-task=18\n",
    "\n",
    "ml load anaconda3-2019.03\n",
    "\n",
    "cd /engram/nklab/hossein/recurrent_models/BLT_models/\n",
    "\n",
    "conda activate py39\n",
    "\n",
    "python main.py --distributed 1 --save_model 1 --dataset 'vggface2' --wandb_p 'vggface2' --wandb_r 'bl_blur_img_g' --num_layers 6 --model 'blt_bl' --epochs 100 --lr .0005 --loss_choice 'weighted' --loss_gamma 0 --recurrent_steps 10 --lr_drop 100 --port '12381' --run 'blur_norm_img_g' --pool 'blur' --batch_size 128\n",
    "\n",
    "'''\n",
    "\n",
    "bash_script_path = \"/engram/nklab/hossein/batch_scripts/imagenet_nb.sh\"\n",
    "os.chdir('/engram/nklab/hossein/batch_scripts/')\n",
    "\n",
    "with open(bash_script_path, \"w+\") as bash_script_file:\n",
    "    bash_script_file.write(script)\n",
    "\n",
    "subprocess.run(['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "x.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], device='cuda:1')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(size=(2, 4), device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1245,  1.1608, -0.1005, -1.2000],\n",
       "        [ 0.9481, -0.5120, -0.6132, -0.4649]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1245,  1.1608, -0.1005, -1.2000],\n",
       "        [ 0.9481, -0.5120, -0.6132, -0.4649]], device='cuda:1')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.to('cuda:1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env (diff)",
   "language": "python",
   "name": "diff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
