{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d7970ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/BLT_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/BLT_models')\n",
    "!pwd\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "from matplotlib.patches import Rectangle\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from models.cornet import get_cornet_model\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "#import rsatoolbox\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# from analyze_representations import sample_vggface2, calc_rdms, plot_maps, compare_rdms,\\\n",
    "#   plot_recurrent_rdms, reduce_dim, plot_dim_reduction_one, sample_FEI_dataset, \\\n",
    "#   plot_rdm_mds, load_model_path, load_pretrained_models, extract_features, kasper_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b576af01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 110, 110])\n",
      "torch.Size([4, 288, 12100])\n",
      "torch.Size([4, 1, 110, 110])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "A = torch.ones(4, 32, 224, 224)\n",
    "conv = nn.Conv2d(32, 1, kernel_size=3, dilation=2, stride=2,padding=0, bias=False)\n",
    "conv.weight = torch.nn.Parameter(torch.ones_like(conv.weight))\n",
    "out = conv(A)\n",
    "print(out.shape)\n",
    "\n",
    "windows = torch.nn.functional.unfold(A, kernel_size=3, dilation=2, stride=2, padding=0)\n",
    "print(windows.shape)\n",
    "windows = windows.reshape(4, 1, 32*9, out.shape[-2], out.shape[-1])\n",
    "windows_reduced = torch.sum(windows, axis=2)\n",
    "print(windows_reduced.shape)\n",
    "print(torch.allclose(out, windows_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bba99ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 288, 110, 110])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1dee6100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27, 784])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Sample input tensor\n",
    "input_tensor = torch.randn(1, 3, 28, 28)\n",
    "\n",
    "# Unfold operation\n",
    "unfold = nn.Unfold(kernel_size=(3, 3), padding=1, stride=1)\n",
    "unfolded_tensor = unfold(input_tensor)\n",
    "unfolded_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d963753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convolution layer\n",
    "conv_layer = nn.Conv2d(in_channels=27, out_channels=64, kernel_size=1)\n",
    "output_tensor = conv_layer(unfolded_tensor)\n",
    "\n",
    "# Reshape the output\n",
    "output_tensor = output_tensor.view(1, 64, 28, 28)\n",
    "\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d14855",
   "metadata": {},
   "source": [
    "Fold, unfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "22597d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "kernel_size=(2, 2)\n",
    "stride=1\n",
    "unfold = nn.Unfold(kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "\n",
    "conv_1 = nn.Conv2d(3, 8, kernel_size, stride)\n",
    "w = conv_1.weight\n",
    "print(w.shape)\n",
    "\n",
    "inp = torch.randn(1, 2, 3, 2)\n",
    "inp_unf = unfold(inp) #.transpose(1, 2)\n",
    "print(inp_unf.shape)\n",
    "\n",
    "out_unf = inp_unf.matmul(w.view(w.size(0), -1).t()).transpose(1, 2)\n",
    "out_unf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6bbbafad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8428,  0.4029],\n",
       "          [ 0.3548,  0.3547],\n",
       "          [-0.6932, -0.3738]],\n",
       "\n",
       "         [[-0.7694,  2.0244],\n",
       "          [ 0.0813, -1.8770],\n",
       "          [ 1.7204,  2.2201]]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(1, 2, 3, 2)\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "948d6398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8428,  0.4029],\n",
       "        [ 0.3548,  0.3547],\n",
       "        [-0.6932, -0.3738]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4fbe3860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8428,  0.3548],\n",
       "         [ 0.4029,  0.3547],\n",
       "         [ 0.3548, -0.6932],\n",
       "         [ 0.3547, -0.3738],\n",
       "         [-0.7694,  0.0813],\n",
       "         [ 2.0244, -1.8770],\n",
       "         [ 0.0813,  1.7204],\n",
       "         [-1.8770,  2.2201]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_unf = unfold(inp) #.transpose(1, 2)\n",
    "print(inp_unf.shape)\n",
    "inp_unf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5a68d767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8428,  0.3548],\n",
       "          [ 0.4029,  0.3547],\n",
       "          [ 0.3548, -0.6932],\n",
       "          [ 0.3547, -0.3738]],\n",
       "\n",
       "         [[-0.7694,  0.0813],\n",
       "          [ 2.0244, -1.8770],\n",
       "          [ 0.0813,  1.7204],\n",
       "          [-1.8770,  2.2201]]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_unf.reshape(1, 2, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "da6da9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8428,  0.4029],\n",
       "          [ 0.3548,  0.3547],\n",
       "          [-0.6932, -0.3738]],\n",
       "\n",
       "         [[-0.7694,  2.0244],\n",
       "          [ 0.0813, -1.8770],\n",
       "          [ 1.7204,  2.2201]]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5722755d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_unf = inp_unf.matmul(w.view(w.size(0), -1).t()).transpose(1, 2)\n",
    "out_unf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d52c09f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 2, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.conv2d(inp, w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "964d251b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 8])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.view(w.size(0), -1).t().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "82b5a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "def position(H, W, is_cuda=True):\n",
    "    if is_cuda:\n",
    "        loc_w = torch.linspace(-1.0, 1.0, W).cuda().unsqueeze(0).repeat(H, 1)\n",
    "        loc_h = torch.linspace(-1.0, 1.0, H).cuda().unsqueeze(1).repeat(1, W)\n",
    "    else:\n",
    "        loc_w = torch.linspace(-1.0, 1.0, W).unsqueeze(0).repeat(H, 1)\n",
    "        loc_h = torch.linspace(-1.0, 1.0, H).unsqueeze(1).repeat(1, W)\n",
    "    loc = torch.cat([loc_w.unsqueeze(0), loc_h.unsqueeze(0)], 0).unsqueeze(0)\n",
    "    return loc\n",
    "\n",
    "\n",
    "def stride(x, stride):\n",
    "    b, c, h, w = x.shape\n",
    "    return x[:, :, ::stride, ::stride]\n",
    "\n",
    "def init_rate_half(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0.5)\n",
    "\n",
    "def init_rate_0(tensor):\n",
    "    if tensor is not None:\n",
    "        tensor.data.fill_(0.)\n",
    "\n",
    "\n",
    "class ACmix(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_att=7, head=4, kernel_conv=3, stride=1, dilation=1):\n",
    "        super(ACmix, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.head = head\n",
    "        self.kernel_att = kernel_att\n",
    "        self.kernel_conv = kernel_conv\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.rate1 = torch.nn.Parameter(torch.Tensor(1))\n",
    "        self.rate2 = torch.nn.Parameter(torch.Tensor(1))\n",
    "        self.head_dim = self.out_planes // self.head\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1)\n",
    "        self.conv3 = nn.Conv2d(in_planes, out_planes, kernel_size=1)\n",
    "        self.conv_p = nn.Conv2d(2, self.head_dim, kernel_size=1)\n",
    "\n",
    "        self.padding_att = (self.dilation * (self.kernel_att - 1) + 1) // 2\n",
    "        self.pad_att = torch.nn.ReflectionPad2d(self.padding_att)\n",
    "        self.unfold = nn.Unfold(kernel_size=self.kernel_att, padding=0, stride=self.stride)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        self.fc = nn.Conv2d(3*self.head, self.kernel_conv * self.kernel_conv, kernel_size=1, bias=False)\n",
    "        self.dep_conv = nn.Conv2d(self.kernel_conv * self.kernel_conv * self.head_dim, out_planes, kernel_size=self.kernel_conv, bias=True, groups=self.head_dim, padding=1, stride=stride)\n",
    "\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        init_rate_half(self.rate1)\n",
    "        init_rate_half(self.rate2)\n",
    "        kernel = torch.zeros(self.kernel_conv * self.kernel_conv, self.kernel_conv, self.kernel_conv)\n",
    "        for i in range(self.kernel_conv * self.kernel_conv):\n",
    "            kernel[i, i//self.kernel_conv, i%self.kernel_conv] = 1.\n",
    "        kernel = kernel.squeeze(0).repeat(self.out_planes, 1, 1, 1)\n",
    "        self.dep_conv.weight = nn.Parameter(data=kernel, requires_grad=True)\n",
    "        self.dep_conv.bias = init_rate_0(self.dep_conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = self.conv1(x), self.conv2(x), self.conv3(x)\n",
    "        scaling = float(self.head_dim) ** -0.5\n",
    "        b, c, h, w = q.shape\n",
    "        h_out, w_out = h//self.stride, w//self.stride\n",
    "\n",
    "\n",
    "# ### att\n",
    "        # ## positional encoding\n",
    "        pe = self.conv_p(position(h, w, x.is_cuda))\n",
    "\n",
    "        q_att = q.view(b*self.head, self.head_dim, h, w) * scaling\n",
    "        k_att = k.view(b*self.head, self.head_dim, h, w)\n",
    "        v_att = v.view(b*self.head, self.head_dim, h, w)\n",
    "\n",
    "        if self.stride > 1:\n",
    "            q_att = stride(q_att, self.stride)\n",
    "            q_pe = stride(pe, self.stride)\n",
    "        else:\n",
    "            q_pe = pe\n",
    "\n",
    "        unfold_k = self.unfold(self.pad_att(k_att)).view(b*self.head, self.head_dim, self.kernel_att*self.kernel_att, h_out, w_out) # b*head, head_dim, k_att^2, h_out, w_out\n",
    "        unfold_rpe = self.unfold(self.pad_att(pe)).view(1, self.head_dim, self.kernel_att*self.kernel_att, h_out, w_out) # 1, head_dim, k_att^2, h_out, w_out\n",
    "        \n",
    "        att = (q_att.unsqueeze(2)*(unfold_k + q_pe.unsqueeze(2) - unfold_rpe)).sum(1) # (b*head, head_dim, 1, h_out, w_out) * (b*head, head_dim, k_att^2, h_out, w_out) -> (b*head, k_att^2, h_out, w_out)\n",
    "        att = self.softmax(att)\n",
    "\n",
    "        out_att = self.unfold(self.pad_att(v_att)).view(b*self.head, self.head_dim, self.kernel_att*self.kernel_att, h_out, w_out)\n",
    "        print('v_att', out_att.shape)\n",
    "        print(att.unsqueeze(1).shape)\n",
    "        out_att = (att.unsqueeze(1) * out_att).sum(2).view(b, self.out_planes, h_out, w_out)\n",
    "        print('out_att', out_att.shape)\n",
    "\n",
    "## conv\n",
    "        f_all = self.fc(torch.cat([q.view(b, self.head, self.head_dim, h*w), k.view(b, self.head, self.head_dim, h*w), v.view(b, self.head, self.head_dim, h*w)], 1))\n",
    "        f_conv = f_all.permute(0, 2, 1, 3).reshape(x.shape[0], -1, x.shape[-2], x.shape[-1])\n",
    "        \n",
    "        out_conv = self.dep_conv(f_conv)\n",
    "\n",
    "        return self.rate1 * out_att + self.rate2 * out_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "847f823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_conv = ACmix(64, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1595159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_att torch.Size([4, 8, 49, 14, 14])\n",
      "torch.Size([4, 1, 49, 14, 14])\n",
      "out_att torch.Size([1, 32, 14, 14])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 14, 14])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.randn(1, 64, 14, 14)\n",
    "\n",
    "mix_conv(input_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7cbca66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 7, 7])\n",
      "torch.Size([1, 3136, 196])\n"
     ]
    }
   ],
   "source": [
    "in_channels = 64\n",
    "out_channels = 32\n",
    "kernel_size=7 #(7, 7)\n",
    "stride=1\n",
    "unfold = nn.Unfold(kernel_size=kernel_size, stride=stride, padding=3)\n",
    "\n",
    "\n",
    "conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=3)\n",
    "w = conv_1.weight\n",
    "print(w.shape)\n",
    "\n",
    "inp_unf = unfold(input_tensor) #.transpose(1, 2)\n",
    "print(inp_unf.shape)\n",
    "\n",
    "inp_unf = inp_unf.transpose(1, 2) \n",
    "w = w.view(w.size(0), -1).t()\n",
    "\n",
    "# out_unf = inp_unf.matmul(w.view(w.size(0), -1).t()).transpose(1, 2)\n",
    "# out_unf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "77554097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 196, 3136]), torch.Size([3136, 32]))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_unf.shape, w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "04f33fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 3136, 32])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_unf = inp_unf.unsqueeze(-1) * w\n",
    "out_unf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "30bbf160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 49, 14, 14])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_unf = out_unf.permute(0, 3, 2, 1)\n",
    "out_unf = out_unf.reshape(1*4, 8, 64, 49, 14, 14).sum(2)\n",
    "out_unf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6f7a8c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_unf = out_unf.sum(2).transpose(1, 2).reshape(1, 32, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f74d05cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7221, grad_fn=<MaxBackward1>) tensor(-2.0759, grad_fn=<MinBackward1>)\n",
      "tensor(0.0165, grad_fn=<MaxBackward1>) tensor(-0.0174, grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "diff = conv_1(input_tensor) - conv_unf\n",
    "print(torch.max(conv_unf), torch.min(conv_unf))\n",
    "print(torch.max(diff), torch.min(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "47eed194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3136/49"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env ({diff_test2})",
   "language": "python",
   "name": "diff_test2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
